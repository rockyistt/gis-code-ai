"""
æ¨¡å‹åŠ è½½å’Œæ¨ç†æ¨¡å— - æ”¯æŒä»Google DriveåŠ è½½å¾®è°ƒçš„CodeLlamaæ¨¡å‹
"""

import os
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from typing import Dict, Optional, Tuple


class GISCodeGenerator:
    """GISä»£ç ç”Ÿæˆå™¨ - ä½¿ç”¨CodeLlama + LoRAå¾®è°ƒæ¨¡å‹"""
    
    def __init__(
        self,
        model_path: str,
        base_model: str = "codellama/CodeLlama-7b-Instruct-hf",
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        use_fp16: bool = True
    ):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            model_path: LoRAæ¨¡å‹è·¯å¾„ (åŒ…å«adapter_config.jsonçš„ç›®å½•)
            base_model: åŸºç¡€æ¨¡å‹åç§°
            device: è®¡ç®—è®¾å¤‡ ("cuda" æˆ– "cpu")
            use_fp16: æ˜¯å¦ä½¿ç”¨FP16
        """
        self.device = device
        self.use_fp16 = use_fp16
        
        print(f"ğŸ”§ è®¾ç½®æ¨ç†ç¯å¢ƒ...")
        print(f"  è®¾å¤‡: {device}")
        print(f"  FP16: {use_fp16}")
        
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        model_path = Path(model_path)
        if not model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        if not (model_path / "adapter_config.json").exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°adapter_config.json: {model_path}")
        
        print(f"\nğŸ“– åŠ è½½Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            str(model_path),
            padding_side="right"
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        print(f"âœ… TokenizeråŠ è½½å®Œæˆ (vocab_size={len(self.tokenizer)})")
        
        print(f"\nğŸ¤– åŠ è½½åŸºç¡€æ¨¡å‹: {base_model}")
        dtype = torch.float16 if use_fp16 else torch.float32
        
        base_model_obj = AutoModelForCausalLM.from_pretrained(
            base_model,
            torch_dtype=dtype,
            device_map="auto",
            low_cpu_mem_usage=True,
        )
        
        base_model_obj.config.use_cache = False
        print(f"âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ")
        
        print(f"\nğŸ”§ åŠ è½½LoRAæƒé‡...")
        self.model = PeftModel.from_pretrained(
            base_model_obj,
            str(model_path),
            torch_dtype=dtype,
            device_map="auto",
        )
        
        self.model.eval()
        print(f"âœ… LoRAæƒé‡åŠ è½½å®Œæˆ")
        
        # æ‰“å°å‚æ•°ä¿¡æ¯
        trainable = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in self.model.parameters())
        print(f"\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°: {total:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable:,} ({100*trainable/total:.2f}%)")
        
        print(f"\nâœ… æ¨¡å‹åˆå§‹åŒ–å®Œæˆï¼")
    
    def generate(
        self,
        instruction: str,
        context: str = "",
        max_new_tokens: int = 512,
        temperature: float = 0.7,
        top_p: float = 0.9,
        num_return_sequences: int = 1,
    ) -> Dict[str, str]:
        """
        ç”ŸæˆGISä»£ç 
        
        Args:
            instruction: ç”¨æˆ·æŒ‡ä»¤
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ (å¯é€‰)
            max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•° (0-1)
            top_p: nucleusé‡‡æ ·å‚æ•°
            num_return_sequences: ç”Ÿæˆåºåˆ—æ•°
        
        Returns:
            å­—å…¸ï¼ŒåŒ…å«ç”Ÿæˆçš„ä»£ç 
        """
        
        # æ„å»ºPrompt
        if context:
            prompt = f"""You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.

Instruction: {instruction}
Context: {context}

JSON Code:
"""
        else:
            prompt = f"""You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.

Instruction: {instruction}

JSON Code:
"""
        
        # Tokenize
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        # ç”Ÿæˆ
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_new_tokens,
                temperature=temperature,
                do_sample=True,
                top_p=top_p,
                num_return_sequences=num_return_sequences,
                pad_token_id=self.tokenizer.pad_token_id,
            )
        
        # è§£ç 
        responses = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            # æå–JSONéƒ¨åˆ†
            code = text.split("JSON Code:")[-1].strip()
            responses.append(code)
        
        if num_return_sequences == 1:
            return {
                "instruction": instruction,
                "context": context,
                "generated_code": responses[0]
            }
        else:
            return {
                "instruction": instruction,
                "context": context,
                "generated_codes": responses
            }


def load_model_from_drive(
    drive_path: str = "/content/drive/MyDrive/gis-models/codellama-gis-lora",
    **kwargs
) -> GISCodeGenerator:
    """
    ä»Google DriveåŠ è½½æ¨¡å‹ (Colabç¯å¢ƒ)
    
    Args:
        drive_path: Google Driveä¸­çš„æ¨¡å‹è·¯å¾„
        **kwargs: ä¼ é€’ç»™GISCodeGeneratorçš„å…¶ä»–å‚æ•°
    
    Returns:
        åˆå§‹åŒ–çš„GISCodeGeneratorå¯¹è±¡
    """
    
    if not os.path.exists(drive_path):
        raise FileNotFoundError(
            f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {drive_path}\n"
            f"è¯·ç¡®ä¿å·²æŒ‚è½½Google Driveå¹¶ä¸”æ¨¡å‹å·²ä¿å­˜åˆ°è¯¥è·¯å¾„"
        )
    
    return GISCodeGenerator(drive_path, **kwargs)


def load_model_from_local(
    local_path: str,
    **kwargs
) -> GISCodeGenerator:
    """
    ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹
    
    Args:
        local_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
        **kwargs: ä¼ é€’ç»™GISCodeGeneratorçš„å…¶ä»–å‚æ•°
    
    Returns:
        åˆå§‹åŒ–çš„GISCodeGeneratorå¯¹è±¡
    """
    
    return GISCodeGenerator(local_path, **kwargs)


if __name__ == "__main__":
    # æµ‹è¯•åŠ è½½æ¨¡å‹ (éœ€è¦åœ¨æœ‰GPUçš„ç¯å¢ƒä¸­è¿è¡Œ)
    import sys
    
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
    else:
        model_path = "/content/drive/MyDrive/gis-models/codellama-gis-lora"
    
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    generator = load_model_from_drive(model_path)
    
    # ç®€å•æµ‹è¯•
    test_instruction = "Create a new MS cable object"
    test_context = "Application: PowerGrid | Database: ND | Steps: 3"
    
    print("\n" + "="*70)
    print("ğŸ§ª æµ‹è¯•æ¨ç†...")
    print("="*70)
    result = generator.generate(test_instruction, test_context)
    print(f"\nğŸ“ æŒ‡ä»¤: {result['instruction']}")
    print(f"ğŸ“ ä¸Šä¸‹æ–‡: {result['context']}")
    print(f"\nğŸ’» ç”Ÿæˆçš„ä»£ç :\n{result['generated_code'][:500]}...")
