"""
æ¨¡å‹è¯„ä¼°è„šæœ¬ - åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°CodeLlamaå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½
"""

import json
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
from tqdm import tqdm
import re

# å‡è®¾å·²æœ‰load_modelæ¨¡å—
try:
    from src.inference.load_model import GISCodeGenerator
except ImportError:
    print("âš ï¸ æ— æ³•å¯¼å…¥load_modelï¼Œè¯·æ£€æŸ¥è·¯å¾„")


class WorkflowEvaluator:
    """å·¥ä½œæµç”Ÿæˆè¯„ä¼°å™¨"""
    
    @staticmethod
    def is_valid_json(text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆJSON"""
        try:
            json.loads(text)
            return True
        except:
            return False
    
    @staticmethod
    def extract_json(text: str) -> Dict:
        """ä»æ–‡æœ¬ä¸­æå–JSON"""
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except:
            # å°è¯•æ‰¾åˆ°ç¬¬ä¸€ä¸ª{å’Œæœ€åä¸€ä¸ª}
            start = text.find('{')
            end = text.rfind('}')
            if start != -1 and end != -1:
                try:
                    return json.loads(text[start:end+1])
                except:
                    pass
            return None
    
    @staticmethod
    def structure_match(generated: Dict, reference: Dict) -> float:
        """
        è®¡ç®—ç»“æ„åŒ¹é…åº¦ (0-1)
        æ£€æŸ¥å¿…è¦å­—æ®µæ˜¯å¦å­˜åœ¨
        """
        if not isinstance(generated, dict):
            return 0.0
        
        required_fields = ["workflow"]
        if not all(field in generated for field in required_fields):
            return 0.0
        
        workflow = generated.get("workflow", {})
        ref_workflow = reference.get("workflow", {})
        
        # æ£€æŸ¥steps
        gen_steps = workflow.get("steps", [])
        ref_steps = ref_workflow.get("steps", [])
        
        if len(gen_steps) == 0:
            return 0.0
        
        # æ£€æŸ¥æ¯ä¸ªstepçš„å¿…è¦å­—æ®µ
        required_step_fields = ["module", "method", "object", "database"]
        valid_steps = 0
        
        for step in gen_steps:
            if all(field in step for field in required_step_fields):
                valid_steps += 1
        
        structure_score = valid_steps / len(gen_steps) if gen_steps else 0.0
        
        # æ­¥éª¤æ•°æ¥è¿‘åº¦
        if ref_steps:
            length_ratio = min(len(gen_steps), len(ref_steps)) / max(len(gen_steps), len(ref_steps))
            structure_score = 0.7 * structure_score + 0.3 * length_ratio
        
        return structure_score
    
    @staticmethod
    def semantic_similarity(text1: str, text2: str) -> float:
        """
        ç®€å•çš„è¯­ä¹‰ç›¸ä¼¼åº¦ (åŸºäºå…³é”®è¯åŒ¹é…)
        æ›´å¥½çš„æ–¹æ³•æ˜¯ä½¿ç”¨Sentence-BERT
        """
        # æå–å…³é”®è¯
        def extract_keywords(text):
            # ç§»é™¤æ ‡ç‚¹å’Œç‰¹æ®Šå­—ç¬¦
            text = re.sub(r'[^a-zA-Z0-9\s]', ' ', text.lower())
            words = set(text.split())
            # è¿‡æ»¤çŸ­è¯
            return {w for w in words if len(w) > 2}
        
        kw1 = extract_keywords(text1)
        kw2 = extract_keywords(text2)
        
        if not kw1 or not kw2:
            return 0.0
        
        intersection = len(kw1 & kw2)
        union = len(kw1 | kw2)
        
        return intersection / union if union > 0 else 0.0
    
    @staticmethod
    def evaluate_sample(
        instruction: str,
        generated_output: str,
        reference_output: str,
    ) -> Dict[str, float]:
        """
        è¯„ä¼°å•ä¸ªæ ·æœ¬
        
        Returns:
            åŒ…å«å¤šä¸ªæŒ‡æ ‡çš„å­—å…¸
        """
        metrics = {}
        
        # 1. JSONæœ‰æ•ˆæ€§
        generated_json = WorkflowEvaluator.extract_json(generated_output)
        reference_json = WorkflowEvaluator.extract_json(reference_output)
        
        metrics["json_valid"] = 1.0 if generated_json else 0.0
        
        # 2. ç»“æ„åŒ¹é…åº¦
        if generated_json and reference_json:
            metrics["structure_match"] = WorkflowEvaluator.structure_match(
                generated_json, reference_json
            )
        else:
            metrics["structure_match"] = 0.0
        
        # 3. è¯­ä¹‰ç›¸ä¼¼åº¦
        metrics["semantic_similarity"] = WorkflowEvaluator.semantic_similarity(
            generated_output, reference_output
        )
        
        # 4. æ­¥éª¤æ•°å¯¹æ¯”
        if generated_json and reference_json:
            gen_steps = len(generated_json.get("workflow", {}).get("steps", []))
            ref_steps = len(reference_json.get("workflow", {}).get("steps", []))
            
            if ref_steps > 0:
                metrics["step_count_ratio"] = min(gen_steps, ref_steps) / ref_steps
            else:
                metrics["step_count_ratio"] = 1.0 if gen_steps == 0 else 0.0
        else:
            metrics["step_count_ratio"] = 0.0
        
        return metrics


class ModelEvaluator:
    """æ¨¡å‹è¯„ä¼°æ¡†æ¶"""
    
    def __init__(self, model: GISCodeGenerator):
        self.model = model
        self.evaluator = WorkflowEvaluator()
    
    def evaluate_on_dataset(
        self,
        test_data: List[Dict],
        num_samples: int = None,
        output_file: str = None,
    ) -> Dict[str, Any]:
        """
        åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
        
        Args:
            test_data: æµ‹è¯•æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å«instruction, input, output
            num_samples: è¯„ä¼°æ ·æœ¬æ•° (None=å…¨éƒ¨)
            output_file: ä¿å­˜è¯¦ç»†ç»“æœçš„æ–‡ä»¶
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        
        if num_samples is None:
            num_samples = len(test_data)
        
        num_samples = min(num_samples, len(test_data))
        
        print(f"\n{'='*70}")
        print(f"ğŸ§ª å¼€å§‹è¯„ä¼°æ¨¡å‹")
        print(f"{'='*70}")
        print(f"ğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {num_samples}")
        
        all_metrics = []
        detailed_results = []
        
        for i, sample in enumerate(tqdm(test_data[:num_samples], desc="è¯„ä¼°è¿›åº¦")):
            instruction = sample.get("instruction", "")
            context = sample.get("input", "")
            reference_output = sample.get("output", "")
            
            try:
                # ç”Ÿæˆä»£ç 
                result = self.model.generate(instruction, context)
                generated_output = result.get("generated_code", "")
                
                # è¯„ä¼°
                metrics = self.evaluator.evaluate_sample(
                    instruction, generated_output, reference_output
                )
                
                all_metrics.append(metrics)
                
                # è®°å½•è¯¦ç»†ç»“æœ
                detailed_results.append({
                    "sample_id": i,
                    "instruction": instruction,
                    "context": context,
                    "generated_output": generated_output,
                    "reference_output": reference_output,
                    "metrics": metrics
                })
                
            except Exception as e:
                print(f"âŒ æ ·æœ¬{i}è¯„ä¼°å¤±è´¥: {str(e)}")
                all_metrics.append({
                    "json_valid": 0.0,
                    "structure_match": 0.0,
                    "semantic_similarity": 0.0,
                    "step_count_ratio": 0.0,
                    "error": str(e)
                })
        
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        summary = {}
        if all_metrics:
            for key in ["json_valid", "structure_match", "semantic_similarity", "step_count_ratio"]:
                values = [m.get(key, 0.0) for m in all_metrics if "error" not in m]
                if values:
                    summary[key] = {
                        "mean": np.mean(values),
                        "std": np.std(values),
                        "min": np.min(values),
                        "max": np.max(values),
                    }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ† (åŠ æƒå¹³å‡)
        if all_metrics:
            json_valid_scores = [m.get("json_valid", 0) for m in all_metrics if "error" not in m]
            structure_scores = [m.get("structure_match", 0) for m in all_metrics if "error" not in m]
            semantic_scores = [m.get("semantic_similarity", 0) for m in all_metrics if "error" not in m]
            
            if json_valid_scores:
                overall_score = (
                    0.3 * np.mean(json_valid_scores) +  # JSONæœ‰æ•ˆæ€§æƒé‡30%
                    0.5 * np.mean(structure_scores) +   # ç»“æ„åŒ¹é…æƒé‡50%
                    0.2 * np.mean(semantic_scores)      # è¯­ä¹‰ç›¸ä¼¼æƒé‡20%
                )
                summary["overall_score"] = overall_score
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        if output_file:
            output_path = Path(output_file)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump({
                    "summary": summary,
                    "detailed_results": detailed_results
                }, f, indent=2, ensure_ascii=False)
            
            print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")
        
        return {
            "summary": summary,
            "num_samples": num_samples,
            "detailed_results": detailed_results
        }
    
    def print_summary(self, results: Dict):
        """æ‰“å°è¯„ä¼°æ‘˜è¦"""
        
        summary = results["summary"]
        num_samples = results["num_samples"]
        
        print(f"\n{'='*70}")
        print(f"ğŸ“Š è¯„ä¼°ç»“æœæ‘˜è¦ (æ ·æœ¬æ•°: {num_samples})")
        print(f"{'='*70}")
        
        # JSONæœ‰æ•ˆæ€§
        if "json_valid" in summary:
            stat = summary["json_valid"]
            print(f"\nâœ… JSONæœ‰æ•ˆæ€§:")
            print(f"  å¹³å‡: {stat['mean']:.2%}")
            print(f"  èŒƒå›´: [{stat['min']:.2%}, {stat['max']:.2%}]")
        
        # ç»“æ„åŒ¹é…åº¦
        if "structure_match" in summary:
            stat = summary["structure_match"]
            print(f"\nğŸ—ï¸ ç»“æ„åŒ¹é…åº¦:")
            print(f"  å¹³å‡: {stat['mean']:.2%}")
            print(f"  æ ‡å‡†å·®: {stat['std']:.2%}")
            print(f"  èŒƒå›´: [{stat['min']:.2%}, {stat['max']:.2%}]")
        
        # è¯­ä¹‰ç›¸ä¼¼åº¦
        if "semantic_similarity" in summary:
            stat = summary["semantic_similarity"]
            print(f"\nğŸ“  è¯­ä¹‰ç›¸ä¼¼åº¦:")
            print(f"  å¹³å‡: {stat['mean']:.2%}")
            print(f"  æ ‡å‡†å·®: {stat['std']:.2%}")
            print(f"  èŒƒå›´: [{stat['min']:.2%}, {stat['max']:.2%}]")
        
        # æ­¥éª¤æ•°å¯¹æ¯”
        if "step_count_ratio" in summary:
            stat = summary["step_count_ratio"]
            print(f"\nğŸ“ æ­¥éª¤æ•°å¯¹æ¯”:")
            print(f"  å¹³å‡æ¯”: {stat['mean']:.2%}")
            print(f"  èŒƒå›´: [{stat['min']:.2%}, {stat['max']:.2%}]")
        
        # ç»¼åˆè¯„åˆ†
        if "overall_score" in summary:
            score = summary["overall_score"]
            print(f"\nğŸ¯ ç»¼åˆè¯„åˆ†:")
            print(f"  {score:.2%}")
            
            if score > 0.8:
                print(f"  ç­‰çº§: â­â­â­â­â­ ä¼˜ç§€")
            elif score > 0.6:
                print(f"  ç­‰çº§: â­â­â­â­ è‰¯å¥½")
            elif score > 0.4:
                print(f"  ç­‰çº§: â­â­â­ ä¸­ç­‰")
            else:
                print(f"  ç­‰çº§: â­â­ éœ€è¦æ”¹è¿›")
        
        print(f"\n{'='*70}\n")


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    import sys
    
    model_path = sys.argv[1] if len(sys.argv) > 1 else "/content/drive/MyDrive/gis-models/codellama-gis-lora"
    test_data_path = sys.argv[2] if len(sys.argv) > 2 else "data/training/training_data_val.json"
    
    print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_path}")
    generator = GISCodeGenerator(model_path)
    
    print(f"ğŸ“‚ åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
    with open(test_data_path, 'r', encoding='utf-8') as f:
        test_data = json.load(f)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = ModelEvaluator(generator)
    
    # è¯„ä¼° (ä½¿ç”¨å‰100ä¸ªæ ·æœ¬å¿«é€Ÿæµ‹è¯•)
    results = evaluator.evaluate_on_dataset(
        test_data,
        num_samples=100,
        output_file="data/evaluation/model_evaluation_results.json"
    )
    
    # æ‰“å°æ‘˜è¦
    evaluator.print_summary(results)
