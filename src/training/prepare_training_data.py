"""
å°†ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼

è¾“å…¥ï¼šstep_level_instructions_weighted_variants_marked.jsonl
è¾“å‡ºï¼štraining_data.json (Alpacaæ ¼å¼)

æ ¼å¼ï¼š
{
    "instruction": "ç”¨æˆ·æŒ‡ä»¤",
    "input": "ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰",
    "output": "ç›®æ ‡JSONä»£ç "
}
"""

import json
import argparse
from pathlib import Path
from typing import Dict, List, Any
import logging
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class TrainingDataPreparer:
    """è®­ç»ƒæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, remove_weight_markers: bool = True):
        """
        Args:
            remove_weight_markers: æ˜¯å¦ç§»é™¤æƒé‡æ ‡è®°ï¼ˆ**å…³é”®** -> å…³é”®ï¼‰
        """
        self.remove_weight_markers = remove_weight_markers
    
    def clean_instruction(self, instruction: str) -> str:
        """æ¸…ç†æŒ‡ä»¤æ–‡æœ¬"""
        if self.remove_weight_markers:
            # ç§»é™¤æƒé‡æ ‡è®°
            instruction = instruction.replace('**', '').replace('*', '')
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        instruction = ' '.join(instruction.split())
        
        return instruction.strip()
    
    def convert_step_to_training_sample(self, step: Dict, workflow: Dict) -> Dict:
        """
        å°†æ­¥éª¤è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
        
        Args:
            step: æ­¥éª¤æ•°æ®ï¼ˆåŒ…å«instructionï¼‰
            workflow: åŸå§‹å·¥ä½œæµæ•°æ®ï¼ˆåŒ…å«JSONä»£ç ï¼‰
        
        Returns:
            è®­ç»ƒæ ·æœ¬ {instruction, input, output}
        """
        # è·å–æŒ‡ä»¤
        instruction = self.clean_instruction(step.get('instruction', ''))
        
        # æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
        input_context = self._build_context(step, workflow)
        
        # è·å–è¾“å‡ºJSONä»£ç 
        output_code = self._extract_step_code(step, workflow)
        
        return {
            "instruction": instruction,
            "input": input_context,
            "output": output_code
        }
    
    def _build_context(self, step: Dict, workflow: Dict) -> str:
        """
        æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡
        
        åŒ…å«ï¼š
        - å·¥ä½œæµç±»å‹ï¼ˆtest_appï¼‰
        - å½“å‰æ­¥éª¤åœ¨å·¥ä½œæµä¸­çš„ä½ç½®
        - å‰åºæ­¥éª¤çš„æ‘˜è¦ï¼ˆå¯é€‰ï¼‰
        """
        context_parts = []
        
        # åº”ç”¨ç±»å‹
        test_app = workflow.get('test_app', '')
        if test_app:
            context_parts.append(f"Application: {test_app}")
        
        # æ­¥éª¤ä½ç½®
        step_index = step.get('step_index', 0)
        total_steps = workflow.get('total_steps', 0)
        if total_steps > 0:
            context_parts.append(f"Step {step_index + 1} of {total_steps}")
        
        # æ•°æ®åº“ä¸Šä¸‹æ–‡
        database = workflow.get('database', '')
        if database:
            context_parts.append(f"Database: {database}")
        
        return " | ".join(context_parts) if context_parts else ""
    
    def _extract_step_code(self, step: Dict, workflow: Dict) -> str:
        """
        æå–æ­¥éª¤å¯¹åº”çš„JSONä»£ç 
        
        ä»åŸå§‹å·¥ä½œæµçš„stepsæ•°ç»„ä¸­æå–å¯¹åº”æ­¥éª¤çš„JSON
        """
        step_index = step.get('step_index', 0)
        steps = workflow.get('steps', [])
        
        if 0 <= step_index < len(steps):
            step_data = steps[step_index]
            # æ ¼å¼åŒ–JSONè¾“å‡º
            return json.dumps(step_data, indent=2, ensure_ascii=False)
        
        return "{}"
    
    def prepare_dataset(self, instructions_file: str, workflows_file: str,
                       output_file: str, max_samples: int = None,
                       split_ratio: float = 0.9):
        """
        å‡†å¤‡å®Œæ•´çš„è®­ç»ƒæ•°æ®é›†
        
        Args:
            instructions_file: æŒ‡ä»¤æ–‡ä»¶è·¯å¾„
            workflows_file: åŸå§‹å·¥ä½œæµæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            split_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0.9 = 90%è®­ç»ƒï¼Œ10%éªŒè¯ï¼‰
        """
        logger.info(f"ğŸ“– Loading data...")
        
        # åŠ è½½æŒ‡ä»¤æ•°æ®
        instructions = []
        with open(instructions_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    instructions.append(json.loads(line))
        
        logger.info(f"âœ… Loaded {len(instructions)} instructions")
        
        # åŠ è½½åŸå§‹å·¥ä½œæµ
        workflows = {}
        with open(workflows_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    wf = json.loads(line)
                    workflows[wf.get('file_id', '')] = wf
        
        logger.info(f"âœ… Loaded {len(workflows)} workflows")
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
        training_samples = []
        
        logger.info("ğŸ”„ Converting to training format...")
        for instr in tqdm(instructions, desc="Processing"):
            file_id = instr.get('file_id', '')
            workflow = workflows.get(file_id, {})
            
            if not workflow:
                continue
            
            # è½¬æ¢
            sample = self.convert_step_to_training_sample(instr, workflow)
            
            # è´¨é‡è¿‡æ»¤
            if self._is_valid_sample(sample):
                training_samples.append(sample)
            
            # é™åˆ¶æ•°é‡
            if max_samples and len(training_samples) >= max_samples:
                break
        
        logger.info(f"âœ… Created {len(training_samples)} training samples")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(training_samples) * split_ratio)
        train_data = training_samples[:split_idx]
        val_data = training_samples[split_idx:]
        
        logger.info(f"ğŸ“Š Split: {len(train_data)} train, {len(val_data)} validation")
        
        # ä¿å­˜æ•°æ®
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒé›†
        train_file = output_path.parent / f"{output_path.stem}_train.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Train data saved: {train_file}")
        
        # éªŒè¯é›†
        val_file = output_path.parent / f"{output_path.stem}_val.json"
        with open(val_file, 'w', encoding='utf-8') as f:
            json.dump(val_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Validation data saved: {val_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_samples": len(training_samples),
            "train_samples": len(train_data),
            "val_samples": len(val_data),
            "split_ratio": split_ratio,
            "source_instructions": instructions_file,
            "source_workflows": workflows_file,
            "avg_instruction_length": sum(len(s['instruction'].split()) for s in training_samples) / len(training_samples),
            "avg_output_length": sum(len(s['output']) for s in training_samples) / len(training_samples),
        }
        
        stats_file = output_path.parent / f"{output_path.stem}_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“ˆ Statistics saved: {stats_file}")
        
        return train_data, val_data, stats
    
    def _is_valid_sample(self, sample: Dict) -> bool:
        """éªŒè¯æ ·æœ¬è´¨é‡"""
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if not sample.get('instruction') or not sample.get('output'):
            return False
        
        # æ£€æŸ¥æŒ‡ä»¤é•¿åº¦ï¼ˆ5-200è¯ï¼‰
        instruction_length = len(sample['instruction'].split())
        if instruction_length < 5 or instruction_length > 200:
            return False
        
        # æ£€æŸ¥è¾“å‡ºä¸ä¸ºç©º
        if sample['output'] == '{}':
            return False
        
        return True


def main():
    parser = argparse.ArgumentParser(description="å‡†å¤‡æ¨¡å‹è®­ç»ƒæ•°æ®")
    parser.add_argument('--instructions', type=str,
                       default='data/processed/step_level_instructions_weighted_variants_marked.jsonl',
                       help='æŒ‡ä»¤æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workflows', type=str,
                       default='data/processed/parsed_workflows.jsonl',
                       help='åŸå§‹å·¥ä½œæµæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='data/training/training_data.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸å«_train/_valåç¼€ï¼‰')
    parser.add_argument('--max-samples', type=int,
                       help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--split-ratio', type=float, default=0.9,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.9ï¼‰')
    parser.add_argument('--keep-markers', action='store_true',
                       help='ä¿ç•™æƒé‡æ ‡è®°ï¼ˆ**å…³é”®**ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.instructions).exists():
        logger.error(f"âŒ Instructions file not found: {args.instructions}")
        return
    
    if not Path(args.workflows).exists():
        logger.error(f"âŒ Workflows file not found: {args.workflows}")
        return
    
    # å‡†å¤‡æ•°æ®
    preparer = TrainingDataPreparer(remove_weight_markers=not args.keep_markers)
    
    train_data, val_data, stats = preparer.prepare_dataset(
        instructions_file=args.instructions,
        workflows_file=args.workflows,
        output_file=args.output,
        max_samples=args.max_samples,
        split_ratio=args.split_ratio
    )
    
    # è¾“å‡ºæ‘˜è¦
    logger.info("\n" + "="*70)
    logger.info("ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info("="*70)
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  - è®­ç»ƒæ ·æœ¬: {stats['train_samples']:,}")
    logger.info(f"  - éªŒè¯æ ·æœ¬: {stats['val_samples']:,}")
    logger.info(f"  - å¹³å‡æŒ‡ä»¤é•¿åº¦: {stats['avg_instruction_length']:.1f} è¯")
    logger.info(f"  - å¹³å‡è¾“å‡ºé•¿åº¦: {stats['avg_output_length']:.1f} å­—ç¬¦")
    logger.info("="*70)
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if train_data:
        logger.info("\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
        sample = train_data[0]
        logger.info(f"  Instruction: {sample['instruction'][:100]}...")
        logger.info(f"  Input: {sample['input']}")
        logger.info(f"  Output: {sample['output'][:150]}...")


if __name__ == "__main__":
    main()
