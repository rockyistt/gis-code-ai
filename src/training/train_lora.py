"""
LoRAå¾®è°ƒè„šæœ¬ - è®­ç»ƒGISä»£ç ç”Ÿæˆæ¨¡å‹

ä½¿ç”¨Qwen2.5-Coder-7Bä½œä¸ºåŸºåº§æ¨¡å‹ï¼Œé€šè¿‡LoRAåœ¨GISæŒ‡ä»¤æ•°æ®ä¸Šå¾®è°ƒ

ä¾èµ–ï¼š
- transformers
- peft
- torch
- datasets
- accelerate
"""

import os
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, List, Optional
import torch
from dataclasses import dataclass, field

# Transformers imports
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)

# PEFT imports
from peft import (
    LoraConfig,
    get_peft_model,
    prepare_model_for_kbit_training,
    TaskType
)

# Dataset
from datasets import load_dataset, Dataset

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


@dataclass
class ModelArguments:
    """æ¨¡å‹å‚æ•°"""
    model_name_or_path: str = field(
        default="Qwen/Qwen2.5-Coder-7B-Instruct",
        metadata={"help": "åŸºåº§æ¨¡å‹è·¯å¾„"}
    )
    use_4bit: bool = field(
        default=True,
        metadata={"help": "ä½¿ç”¨4-bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰"}
    )
    use_8bit: bool = field(
        default=False,
        metadata={"help": "ä½¿ç”¨8-bité‡åŒ–"}
    )


@dataclass
class DataArguments:
    """æ•°æ®å‚æ•°"""
    train_file: str = field(
        default="data/training/training_data_train.json",
        metadata={"help": "è®­ç»ƒæ•°æ®æ–‡ä»¶"}
    )
    val_file: str = field(
        default="data/training/training_data_val.json",
        metadata={"help": "éªŒè¯æ•°æ®æ–‡ä»¶"}
    )
    max_length: int = field(
        default=2048,
        metadata={"help": "æœ€å¤§åºåˆ—é•¿åº¦"}
    )


@dataclass
class LoraArguments:
    """LoRAå‚æ•°"""
    lora_r: int = field(
        default=64,
        metadata={"help": "LoRAç§©"}
    )
    lora_alpha: int = field(
        default=16,
        metadata={"help": "LoRA alpha"}
    )
    lora_dropout: float = field(
        default=0.05,
        metadata={"help": "LoRA dropout"}
    )
    target_modules: List[str] = field(
        default_factory=lambda: ["q_proj", "k_proj", "v_proj", "o_proj",
                                 "gate_proj", "up_proj", "down_proj"],
        metadata={"help": "LoRAç›®æ ‡æ¨¡å—"}
    )


class GISTrainer:
    """GISä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(
        self,
        model_args: ModelArguments,
        data_args: DataArguments,
        lora_args: LoraArguments,
        training_args: TrainingArguments
    ):
        self.model_args = model_args
        self.data_args = data_args
        self.lora_args = lora_args
        self.training_args = training_args
        
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
    
    def load_tokenizer(self):
        """åŠ è½½tokenizer"""
        logger.info(f"ğŸ“– Loading tokenizer from {self.model_args.model_name_or_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_args.model_name_or_path,
            trust_remote_code=True,
            padding_side="right"
        )
        
        # è®¾ç½®ç‰¹æ®Štoken
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        logger.info(f"âœ… Tokenizer loaded: vocab_size={len(self.tokenizer)}")
    
    def load_model(self):
        """åŠ è½½æ¨¡å‹å¹¶åº”ç”¨LoRA"""
        logger.info(f"ğŸ¤– Loading model from {self.model_args.model_name_or_path}")
        
        # é‡åŒ–é…ç½®
        quantization_config = None
        if self.model_args.use_4bit:
            logger.info("ğŸ“‰ Using 4-bit quantization")
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4"
            )
        elif self.model_args.use_8bit:
            logger.info("ğŸ“‰ Using 8-bit quantization")
            quantization_config = BitsAndBytesConfig(
                load_in_8bit=True
            )
        
        # åŠ è½½åŸºåº§æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_args.model_name_or_path,
            quantization_config=quantization_config,
            device_map="auto",
            trust_remote_code=True,
            torch_dtype=torch.float16,
        )
        
        logger.info("âœ… Base model loaded")
        
        # å‡†å¤‡æ¨¡å‹ç”¨äºè®­ç»ƒ
        if quantization_config:
            self.model = prepare_model_for_kbit_training(self.model)
        
        # é…ç½®LoRA
        logger.info("ğŸ”§ Applying LoRA configuration")
        lora_config = LoraConfig(
            task_type=TaskType.CAUSAL_LM,
            inference_mode=False,
            r=self.lora_args.lora_r,
            lora_alpha=self.lora_args.lora_alpha,
            lora_dropout=self.lora_args.lora_dropout,
            target_modules=self.lora_args.target_modules,
            bias="none"
        )
        
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        
        logger.info("âœ… LoRA applied successfully")
    
    def prepare_datasets(self):
        """å‡†å¤‡è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†"""
        logger.info("ğŸ“Š Preparing datasets")
        
        # åŠ è½½æ•°æ®
        train_data = load_dataset('json', data_files=self.data_args.train_file, split='train')
        eval_data = load_dataset('json', data_files=self.data_args.val_file, split='train')
        
        logger.info(f"  Train: {len(train_data)} samples")
        logger.info(f"  Val: {len(eval_data)} samples")
        
        # æ ¼å¼åŒ–prompt
        def format_prompt(example):
            """æ ¼å¼åŒ–ä¸ºQwençš„å¯¹è¯æ ¼å¼"""
            instruction = example['instruction']
            input_text = example.get('input', '')
            output = example['output']
            
            # æ„å»ºprompt
            if input_text:
                prompt = f"""Below is an instruction that describes a task, paired with context information. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Context:
{input_text}

### Response:
{output}"""
            else:
                prompt = f"""Below is an instruction that describes a task. Write a response that appropriately completes the request.

### Instruction:
{instruction}

### Response:
{output}"""
            
            return {"text": prompt}
        
        # åº”ç”¨æ ¼å¼åŒ–
        train_data = train_data.map(format_prompt, remove_columns=train_data.column_names)
        eval_data = eval_data.map(format_prompt, remove_columns=eval_data.column_names)
        
        # Tokenize
        def tokenize_function(examples):
            tokenized = self.tokenizer(
                examples['text'],
                truncation=True,
                max_length=self.data_args.max_length,
                padding=False,
                return_tensors=None
            )
            tokenized["labels"] = tokenized["input_ids"].copy()
            return tokenized
        
        logger.info("ğŸ”„ Tokenizing datasets...")
        self.train_dataset = train_data.map(
            tokenize_function,
            batched=True,
            remove_columns=train_data.column_names,
            desc="Tokenizing train"
        )
        
        self.eval_dataset = eval_data.map(
            tokenize_function,
            batched=True,
            remove_columns=eval_data.column_names,
            desc="Tokenizing val"
        )
        
        logger.info("âœ… Datasets prepared")
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("ğŸš€ Starting training...")
        
        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            model=self.model,
            padding=True
        )
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=self.train_dataset,
            eval_dataset=self.eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
        )
        
        # è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        logger.info(f"ğŸ’¾ Saving final model to {self.training_args.output_dir}")
        trainer.save_model()
        self.tokenizer.save_pretrained(self.training_args.output_dir)
        
        logger.info("ğŸ‰ Training completed!")
        
        return trainer


def main():
    parser = argparse.ArgumentParser(description="LoRAå¾®è°ƒGISä»£ç ç”Ÿæˆæ¨¡å‹")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model-name', type=str,
                       default="Qwen/Qwen2.5-Coder-7B-Instruct",
                       help='åŸºåº§æ¨¡å‹åç§°')
    parser.add_argument('--use-4bit', action='store_true', default=True,
                       help='ä½¿ç”¨4-bité‡åŒ–')
    parser.add_argument('--use-8bit', action='store_true',
                       help='ä½¿ç”¨8-bité‡åŒ–')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--train-file', type=str,
                       default='data/training/training_data_train.json',
                       help='è®­ç»ƒæ•°æ®æ–‡ä»¶')
    parser.add_argument('--val-file', type=str,
                       default='data/training/training_data_val.json',
                       help='éªŒè¯æ•°æ®æ–‡ä»¶')
    parser.add_argument('--max-length', type=int, default=2048,
                       help='æœ€å¤§åºåˆ—é•¿åº¦')
    
    # LoRAå‚æ•°
    parser.add_argument('--lora-r', type=int, default=64,
                       help='LoRAç§©')
    parser.add_argument('--lora-alpha', type=int, default=16,
                       help='LoRA alpha')
    parser.add_argument('--lora-dropout', type=float, default=0.05,
                       help='LoRA dropout')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--output-dir', type=str,
                       default='models/qwen-gis-lora',
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--num-epochs', type=int, default=3,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=4,
                       help='è®­ç»ƒbatch size')
    parser.add_argument('--gradient-accumulation-steps', type=int, default=4,
                       help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--learning-rate', type=float, default=2e-4,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--warmup-steps', type=int, default=100,
                       help='é¢„çƒ­æ­¥æ•°')
    parser.add_argument('--logging-steps', type=int, default=10,
                       help='æ—¥å¿—è¾“å‡ºé¢‘ç‡')
    parser.add_argument('--save-steps', type=int, default=500,
                       help='æ¨¡å‹ä¿å­˜é¢‘ç‡')
    
    args = parser.parse_args()
    
    # åˆ›å»ºå‚æ•°å¯¹è±¡
    model_args = ModelArguments(
        model_name_or_path=args.model_name,
        use_4bit=args.use_4bit and not args.use_8bit,
        use_8bit=args.use_8bit
    )
    
    data_args = DataArguments(
        train_file=args.train_file,
        val_file=args.val_file,
        max_length=args.max_length
    )
    
    lora_args = LoraArguments(
        lora_r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout
    )
    
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        warmup_steps=args.warmup_steps,
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        eval_steps=args.save_steps,
        evaluation_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        fp16=True,
        optim="paged_adamw_8bit",
        lr_scheduler_type="cosine",
        save_total_limit=3,
        report_to="none",  # å¯æ”¹ä¸º"tensorboard"æˆ–"wandb"
    )
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.train_file).exists():
        logger.error(f"âŒ Train file not found: {args.train_file}")
        logger.info("ğŸ’¡ è¯·å…ˆè¿è¡Œ: python src/training/prepare_training_data.py")
        return
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    logger.info("="*70)
    logger.info("ğŸ¯ GISä»£ç ç”Ÿæˆæ¨¡å‹ - LoRAå¾®è°ƒ")
    logger.info("="*70)
    logger.info(f"ğŸ“¦ æ¨¡å‹: {args.model_name}")
    logger.info(f"ğŸ“Š è®­ç»ƒæ•°æ®: {args.train_file}")
    logger.info(f"ğŸ“Š éªŒè¯æ•°æ®: {args.val_file}")
    logger.info(f"ğŸ”§ LoRA r={args.lora_r}, alpha={args.lora_alpha}")
    logger.info(f"ğŸ“ˆ Epochs={args.num_epochs}, Batch={args.batch_size}, LR={args.learning_rate}")
    logger.info(f"ğŸ’¾ è¾“å‡º: {args.output_dir}")
    logger.info("="*70)
    
    trainer = GISTrainer(model_args, data_args, lora_args, training_args)
    
    # æ‰§è¡Œè®­ç»ƒæµç¨‹
    trainer.load_tokenizer()
    trainer.load_model()
    trainer.prepare_datasets()
    trainer.train()


if __name__ == "__main__":
    main()
