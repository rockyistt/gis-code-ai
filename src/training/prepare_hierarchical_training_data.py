#!/usr/bin/env python3
"""
æ„å»ºå±‚æ¬¡åŒ–è®­ç»ƒæ•°æ®ï¼ˆContext Windowç­–ç•¥ï¼‰

ä¸ºæ¯ä¸ªstepæ·»åŠ ä¸Šä¸‹æ–‡ï¼š
1. file_task: æ•´ä¸ªæ–‡ä»¶çš„ä»»åŠ¡æè¿°
2. previous_steps: ä¹‹å‰å·²å®Œæˆçš„æ­¥éª¤
3. remaining_objects: å‰©ä½™å¾…å¤„ç†çš„å¯¹è±¡

è¿™æ ·æ¨¡å‹åœ¨ç”Ÿæˆæ¯ä¸ªstepæ—¶èƒ½å¤Ÿï¼š
- çŸ¥é“æ•´ä½“ç›®æ ‡ï¼ˆfile_taskï¼‰
- äº†è§£è¿›åº¦ï¼ˆprevious_stepsï¼‰
- çŸ¥é“è¿˜éœ€å¤„ç†ä»€ä¹ˆï¼ˆremaining_objectsï¼‰
"""
import json
from pathlib import Path
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def extract_objects_from_step(step: Dict) -> List[str]:
    """ä»stepä¸­æå–æ“ä½œçš„å¯¹è±¡"""
    objects = []
    
    # ä»structure.objectæå–
    if 'structure' in step and 'object' in step['structure']:
        obj = step['structure']['object']
        if obj and obj not in ['object', 'objects']:
            # æ¸…ç†å¯¹è±¡å
            import re
            obj_clean = re.sub(r'\s+object$', '', obj, flags=re.IGNORECASE)
            objects.append(obj_clean)
    
    return objects


def build_context_for_step(
    step: Dict,
    file_instruction: str,
    all_steps: List[Dict],
    current_index: int
) -> Dict[str, Any]:
    """ä¸ºå•ä¸ªstepæ„å»ºä¸Šä¸‹æ–‡ä¿¡æ¯"""
    
    # 1. File taskï¼ˆæ•´ä½“ä»»åŠ¡ï¼‰
    file_task = file_instruction
    
    # 2. Previous stepsï¼ˆä¹‹å‰çš„æ­¥éª¤ï¼Œä¿ç•™æœ€è¿‘3ä¸ªï¼‰
    previous_steps = []
    start_idx = max(0, current_index - 3)
    for i in range(start_idx, current_index):
        prev_step = all_steps[i]
        previous_steps.append({
            'step_index': prev_step['step_index'],
            'instruction': prev_step['instruction'],
            'action': prev_step['structure'].get('action', '') if 'structure' in prev_step else ''
        })
    
    # 3. Remaining objectsï¼ˆå‰©ä½™å¯¹è±¡ï¼‰
    # æ”¶é›†æ‰€æœ‰å¯¹è±¡
    all_objects = []
    for s in all_steps:
        objs = extract_objects_from_step(s)
        all_objects.extend(objs)
    
    # å»é‡å¹¶ä¿åº
    unique_objects = []
    seen = set()
    for obj in all_objects:
        if obj not in seen:
            unique_objects.append(obj)
            seen.add(obj)
    
    # å·²å¤„ç†çš„å¯¹è±¡ï¼ˆå½“å‰æ­¥éª¤ä¹‹å‰ï¼‰
    processed_objects = set()
    for i in range(current_index):
        objs = extract_objects_from_step(all_steps[i])
        processed_objects.update(objs)
    
    # å‰©ä½™å¯¹è±¡
    remaining_objects = [obj for obj in unique_objects if obj not in processed_objects]
    
    # 4. å½“å‰æ­¥éª¤çš„å¯¹è±¡
    current_objects = extract_objects_from_step(step)
    
    return {
        'file_task': file_task,
        'previous_steps': previous_steps,
        'remaining_objects': remaining_objects[:5],  # æœ€å¤šæ˜¾ç¤º5ä¸ª
        'current_objects': current_objects,
        'progress': {
            'current_step': current_index + 1,
            'total_steps': len(all_steps),
            'processed_objects': len(processed_objects),
            'remaining_objects': len(remaining_objects)
        }
    }


def build_hierarchical_training_sample(
    step: Dict,
    context: Dict,
    output_json: Dict
) -> Dict[str, Any]:
    """æ„å»ºå•ä¸ªè®­ç»ƒæ ·æœ¬ï¼ˆå¸¦å±‚æ¬¡åŒ–ä¸Šä¸‹æ–‡ï¼‰"""
    
    # æ„å»ºå¢å¼ºçš„instructionï¼ˆåŒ…å«ä¸Šä¸‹æ–‡æç¤ºï¼‰
    instruction_parts = []
    
    # æ·»åŠ æ–‡ä»¶ä»»åŠ¡ä¸Šä¸‹æ–‡
    if context['file_task']:
        instruction_parts.append(f"File Task: {context['file_task']}")
    
    # æ·»åŠ è¿›åº¦ä¿¡æ¯
    progress = context['progress']
    instruction_parts.append(
        f"Progress: Step {progress['current_step']}/{progress['total_steps']}"
    )
    
    # æ·»åŠ ä¹‹å‰çš„æ­¥éª¤ï¼ˆå¦‚æœæœ‰ï¼‰
    if context['previous_steps']:
        prev_str = "; ".join([
            f"{ps['action']} {ps['instruction'].split()[1] if len(ps['instruction'].split()) > 1 else ''}"
            for ps in context['previous_steps']
        ])
        instruction_parts.append(f"Previous: {prev_str}")
    
    # æ·»åŠ å‰©ä½™å¯¹è±¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if context['remaining_objects']:
        remaining_str = ", ".join(context['remaining_objects'][:3])
        instruction_parts.append(f"Remaining: {remaining_str}")
    
    # å½“å‰æ­¥éª¤çš„æŒ‡ä»¤
    instruction_parts.append(f"\nCurrent Step: {step['instruction']}")
    
    full_instruction = "\n".join(instruction_parts)
    
    # æ„å»ºè®­ç»ƒæ ·æœ¬
    return {
        'instruction': full_instruction,
        'input': '',  # å¯¹äºinstruction-tuningï¼Œinputé€šå¸¸ä¸ºç©º
        'output': json.dumps(output_json, ensure_ascii=False),
        
        # å…ƒæ•°æ®ï¼ˆç”¨äºåˆ†æï¼Œä¸ç”¨äºè®­ç»ƒï¼‰
        'metadata': {
            'file_id': step['file_id'],
            'step_index': step['step_index'],
            'step_type': step['step_type'],
            'is_high_quality': step.get('is_high_quality', False),
            'provider': 'hierarchical_context_window',
            'keywords': step.get('keywords', []),
            'context': context
        }
    }


def main():
    """ä¸»å‡½æ•°"""
    logging.info("="*70)
    logging.info("ğŸ—ï¸  æ„å»ºå±‚æ¬¡åŒ–è®­ç»ƒæ•°æ®ï¼ˆContext Windowç­–ç•¥ï¼‰")
    logging.info("="*70)
    
    # 1. åŠ è½½æ•°æ®
    logging.info("\nğŸ“– åŠ è½½æ•°æ®...")
    
    # åŠ è½½stepçº§æŒ‡ä»¤
    step_insts = []
    with open('data/processed/step_level_instructions_weighted.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            step_insts.append(json.loads(line))
    logging.info(f"   âœ“ StepæŒ‡ä»¤: {len(step_insts)}")
    
    # åŠ è½½fileçº§æŒ‡ä»¤
    file_insts = {}
    with open('data/processed/file_level_instructions_aggregated.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            file_insts[data['file_id']] = data
    logging.info(f"   âœ“ FileæŒ‡ä»¤: {len(file_insts)}")
    
    # åŠ è½½åŸå§‹å·¥ä½œæµï¼ˆè·å–å®Œæ•´çš„stepè¾“å‡ºJSONï¼‰
    workflows = {}
    with open('data/processed/parsed_workflows.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line)
            workflows[data['file_id']] = data
    logging.info(f"   âœ“ å·¥ä½œæµ: {len(workflows)}")
    
    # 2. æŒ‰file_idåˆ†ç»„step
    logging.info("\nğŸ“Š æŒ‰æ–‡ä»¶åˆ†ç»„æ­¥éª¤...")
    steps_by_file = {}
    for step in step_insts:
        file_id = step['file_id']
        if file_id not in steps_by_file:
            steps_by_file[file_id] = []
        steps_by_file[file_id].append(step)
    
    # ç¡®ä¿æ¯ä¸ªæ–‡ä»¶çš„æ­¥éª¤æŒ‰step_indexæ’åº
    for file_id in steps_by_file:
        steps_by_file[file_id].sort(key=lambda s: s['step_index'])
    
    logging.info(f"   âœ“ æ–‡ä»¶æ•°: {len(steps_by_file)}")
    
    # 3. æ„å»ºå±‚æ¬¡åŒ–è®­ç»ƒæ ·æœ¬
    logging.info("\nğŸ—ï¸  æ„å»ºè®­ç»ƒæ ·æœ¬...")
    training_samples = []
    
    for file_id, steps in steps_by_file.items():
        file_inst = file_insts.get(file_id)
        workflow = workflows.get(file_id)
        
        if not file_inst or not workflow:
            logging.warning(f"   âš ï¸  è·³è¿‡ {file_id}: ç¼ºå°‘fileæŒ‡ä»¤æˆ–å·¥ä½œæµ")
            continue
        
        file_instruction = file_inst['instruction']
        
        # ä¸ºæ¯ä¸ªstepæ„å»ºä¸Šä¸‹æ–‡å’Œè®­ç»ƒæ ·æœ¬
        for i, step in enumerate(steps):
            # è·å–stepçš„è¾“å‡ºJSONï¼ˆä»åŸå§‹å·¥ä½œæµï¼‰
            if i < len(workflow['steps']):
                output_json = workflow['steps'][i]
            else:
                logging.warning(f"   âš ï¸  {file_id} step {i}: æ— æ³•æ‰¾åˆ°å¯¹åº”çš„åŸå§‹step")
                continue
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = build_context_for_step(step, file_instruction, steps, i)
            
            # æ„å»ºè®­ç»ƒæ ·æœ¬
            sample = build_hierarchical_training_sample(step, context, output_json)
            training_samples.append(sample)
    
    logging.info(f"   âœ“ ç”Ÿæˆæ ·æœ¬æ•°: {len(training_samples)}")
    
    # 4. ä¿å­˜ç»“æœ
    output_path = Path('data/processed/hierarchical_training_data.json')
    logging.info(f"\nğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(training_samples, f, ensure_ascii=False, indent=2)
    
    # 5. ç»Ÿè®¡ä¿¡æ¯
    logging.info("\n" + "="*70)
    logging.info("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
    logging.info("="*70)
    logging.info(f"æ€»æ ·æœ¬æ•°: {len(training_samples)}")
    logging.info(f"æ€»æ–‡ä»¶æ•°: {len(steps_by_file)}")
    logging.info(f"å¹³å‡æ¯æ–‡ä»¶æ­¥éª¤æ•°: {len(training_samples) / len(steps_by_file):.1f}")
    
    # é«˜è´¨é‡æ ·æœ¬ç»Ÿè®¡
    high_quality = sum(1 for s in training_samples if s['metadata'].get('is_high_quality', False))
    logging.info(f"é«˜è´¨é‡æ ·æœ¬: {high_quality} ({high_quality/len(training_samples)*100:.1f}%)")
    
    # 6. æ˜¾ç¤ºæ ·æœ¬
    logging.info("\n" + "="*70)
    logging.info("ğŸ“ æ ·æœ¬ç¤ºä¾‹")
    logging.info("="*70)
    
    sample = training_samples[0]
    logging.info(f"\næ–‡ä»¶: {sample['metadata']['file_id']}")
    logging.info(f"æ­¥éª¤: {sample['metadata']['step_index'] + 1}")
    logging.info(f"\nã€Instructionã€‘:")
    logging.info(sample['instruction'][:500] + "...")
    logging.info(f"\nã€Outputã€‘:")
    logging.info(sample['output'][:300] + "...")
    
    # æ˜¾ç¤ºä¸Šä¸‹æ–‡ä¿¡æ¯
    context = sample['metadata']['context']
    logging.info(f"\nã€Context Infoã€‘:")
    logging.info(f"  File Task: {context['file_task']}")
    logging.info(f"  Previous Steps: {len(context['previous_steps'])}")
    logging.info(f"  Remaining Objects: {context['remaining_objects'][:3]}")
    logging.info(f"  Progress: {context['progress']['current_step']}/{context['progress']['total_steps']}")
    
    logging.info("\n" + "="*70)
    logging.info("âœ… å±‚æ¬¡åŒ–è®­ç»ƒæ•°æ®æ„å»ºå®Œæˆï¼")
    logging.info("="*70)
    logging.info(f"è¾“å‡ºæ–‡ä»¶: {output_path}")
    logging.info(f"æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024 / 1024:.2f} MB")
    logging.info("="*70)


if __name__ == '__main__':
    main()
