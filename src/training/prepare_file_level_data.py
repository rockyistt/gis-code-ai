"""
æ–‡ä»¶çº§è®­ç»ƒæ•°æ®å‡†å¤‡è„šæœ¬

å°†file_level_instructionsè½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
æ¯ä¸ªæ ·æœ¬å¯¹åº”ä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµï¼ˆåŒ…å«æ‰€æœ‰stepsçš„JSONï¼‰

ä¼˜åŠ¿ï¼š
- æ¨¡å‹å­¦ä¹ å®Œæ•´å·¥ä½œæµçš„ç»“æ„
- è¾“å‡ºæ˜¯å®é™…å¯ç”¨çš„æµ‹è¯•è„šæœ¬
- ç¬¦åˆGISå¹³å°ä½¿ç”¨åœºæ™¯ï¼ˆä¸€æ¬¡æ€§ç”Ÿæˆå®Œæ•´å·¥ä½œæµï¼‰
"""

import json
from pathlib import Path
from typing import Dict, List, Any
import logging
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FileeLevelTrainingDataPreparer:
    """æ–‡ä»¶çº§è®­ç»ƒæ•°æ®å‡†å¤‡å™¨"""
    
    def __init__(self, remove_weight_markers: bool = True):
        """
        Args:
            remove_weight_markers: æ˜¯å¦ç§»é™¤æƒé‡æ ‡è®°ï¼ˆ**å…³é”®** -> å…³é”®ï¼‰
        """
        self.remove_weight_markers = remove_weight_markers
    
    def clean_instruction(self, instruction: str) -> str:
        """æ¸…ç†æŒ‡ä»¤æ–‡æœ¬"""
        if self.remove_weight_markers:
            # ç§»é™¤æƒé‡æ ‡è®°
            instruction = instruction.replace('**', '').replace('*', '')
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        instruction = ' '.join(instruction.split())
        
        return instruction.strip()
    
    def convert_file_to_training_sample(self, file_instr: Dict, workflow: Dict) -> Dict:
        """
        å°†æ–‡ä»¶çº§æŒ‡ä»¤è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
        
        Args:
            file_instr: æ–‡ä»¶çº§æŒ‡ä»¤ï¼ˆåŒ…å«instructionï¼‰
            workflow: åŸå§‹å·¥ä½œæµæ•°æ®ï¼ˆåŒ…å«æ‰€æœ‰stepsï¼‰
        
        Returns:
            è®­ç»ƒæ ·æœ¬ {instruction, input, output}
        """
        # è·å–æŒ‡ä»¤
        instruction = self.clean_instruction(file_instr.get('instruction', ''))
        
        # æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡
        input_context = self._build_context(file_instr, workflow)
        
        # è·å–è¾“å‡ºï¼šå®Œæ•´çš„å·¥ä½œæµJSONï¼ˆåŒ…å«æ‰€æœ‰stepsï¼‰
        output_code = self._extract_workflow_json(workflow)
        
        return {
            "instruction": instruction,
            "input": input_context,
            "output": output_code
        }
    
    def _build_context(self, file_instr: Dict, workflow: Dict) -> str:
        """
        æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡
        
        åŒ…å«ï¼š
        - åº”ç”¨ç±»å‹ï¼ˆtest_appï¼‰
        - æ•°æ®åº“ç±»å‹
        - å·¥ä½œæµç»Ÿè®¡
        """
        context_parts = []
        
        # åº”ç”¨ç±»å‹
        test_app = workflow.get('test_app', '')
        if test_app:
            context_parts.append(f"Application: {test_app}")
        
        # æ•°æ®åº“
        database = workflow.get('database', '')
        if database:
            context_parts.append(f"Database: {database}")
        
        # æ­¥éª¤æ•°é‡
        total_steps = workflow.get('total_steps', 0)
        if total_steps > 0:
            context_parts.append(f"Steps: {total_steps}")
        
        # å¯¹è±¡ç±»å‹
        objects = workflow.get('objects', [])
        if objects:
            obj_str = ', '.join(objects[:3])  # æœ€å¤šæ˜¾ç¤º3ä¸ªå¯¹è±¡
            context_parts.append(f"Objects: {obj_str}")
        
        return " | ".join(context_parts) if context_parts else ""
    
    def _extract_workflow_json(self, workflow: Dict) -> str:
        """
        æå–å®Œæ•´çš„å·¥ä½œæµJSON
        
        åŒ…å«æ‰€æœ‰stepsï¼Œè¿™æ˜¯æ¨¡å‹éœ€è¦å­¦ä¹ ç”Ÿæˆçš„å®Œæ•´ç»“æ„
        """
        steps = workflow.get('steps', [])
        
        if not steps:
            return "{}"
        
        # åˆ›å»ºå·¥ä½œæµç»“æ„
        workflow_output = {
            "workflow": {
                "metadata": {
                    "test_app": workflow.get('test_app', ''),
                    "database": workflow.get('database', ''),
                    "total_steps": len(steps)
                },
                "steps": steps
            }
        }
        
        return json.dumps(workflow_output, indent=2, ensure_ascii=False)
    
    def prepare_dataset(self, instructions_file: str, workflows_file: str,
                       output_file: str, max_samples: int = None,
                       split_ratio: float = 0.9):
        """
        å‡†å¤‡å®Œæ•´çš„æ–‡ä»¶çº§è®­ç»ƒæ•°æ®é›†
        
        Args:
            instructions_file: æ–‡ä»¶çº§æŒ‡ä»¤æ–‡ä»¶è·¯å¾„
            workflows_file: åŸå§‹å·¥ä½œæµæ–‡ä»¶è·¯å¾„
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            split_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0.9 = 90%è®­ç»ƒï¼Œ10%éªŒè¯ï¼‰
        """
        logger.info(f"ğŸ“– Loading file-level instructions...")
        
        # åŠ è½½æŒ‡ä»¤æ•°æ®
        instructions = {}
        with open(instructions_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    item = json.loads(line)
                    file_id = item.get('file_id', '')
                    instructions[file_id] = item
        
        logger.info(f"âœ… Loaded {len(instructions)} file-level instructions")
        
        # åŠ è½½åŸå§‹å·¥ä½œæµ
        workflows = {}
        with open(workflows_file, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    wf = json.loads(line)
                    workflows[wf.get('file_id', '')] = wf
        
        logger.info(f"âœ… Loaded {len(workflows)} workflows")
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ·æœ¬
        training_samples = []
        
        logger.info("ğŸ”„ Converting to training format (file-level)...")
        
        processed_count = 0
        for file_id, instr in tqdm(instructions.items(), desc="Processing"):
            workflow = workflows.get(file_id, {})
            
            if not workflow:
                continue
            
            # è½¬æ¢
            sample = self.convert_file_to_training_sample(instr, workflow)
            
            # è´¨é‡è¿‡æ»¤
            if self._is_valid_sample(sample):
                training_samples.append(sample)
            
            processed_count += 1
            
            # é™åˆ¶æ•°é‡
            if max_samples and processed_count >= max_samples:
                break
        
        logger.info(f"âœ… Created {len(training_samples)} training samples")
        
        # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
        split_idx = int(len(training_samples) * split_ratio)
        train_data = training_samples[:split_idx]
        val_data = training_samples[split_idx:]
        
        logger.info(f"ğŸ“Š Split: {len(train_data)} train, {len(val_data)} validation")
        
        # ä¿å­˜æ•°æ®
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # è®­ç»ƒé›†
        train_file = output_path.parent / f"{output_path.stem}_train.json"
        with open(train_file, 'w', encoding='utf-8') as f:
            json.dump(train_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Train data saved: {train_file}")
        
        # éªŒè¯é›†
        val_file = output_path.parent / f"{output_path.stem}_val.json"
        with open(val_file, 'w', encoding='utf-8') as f:
            json.dump(val_data, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ’¾ Validation data saved: {val_file}")
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯
        stats = {
            "total_samples": len(training_samples),
            "train_samples": len(train_data),
            "val_samples": len(val_data),
            "split_ratio": split_ratio,
            "source_instructions": instructions_file,
            "source_workflows": workflows_file,
            "data_level": "file-level",
            "avg_instruction_length": sum(len(s['instruction'].split()) for s in training_samples) / len(training_samples) if training_samples else 0,
            "avg_output_length": sum(len(s['output']) for s in training_samples) / len(training_samples) if training_samples else 0,
        }
        
        stats_file = output_path.parent / f"{output_path.stem}_stats.json"
        with open(stats_file, 'w', encoding='utf-8') as f:
            json.dump(stats, f, indent=2, ensure_ascii=False)
        logger.info(f"ğŸ“ˆ Statistics saved: {stats_file}")
        
        return train_data, val_data, stats
    
    def _is_valid_sample(self, sample: Dict) -> bool:
        """éªŒè¯æ ·æœ¬è´¨é‡"""
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        if not sample.get('instruction') or not sample.get('output'):
            return False
        
        # æ£€æŸ¥æŒ‡ä»¤é•¿åº¦ï¼ˆ5-300è¯ï¼‰
        instruction_length = len(sample['instruction'].split())
        if instruction_length < 5 or instruction_length > 300:
            return False
        
        # æ£€æŸ¥è¾“å‡ºä¸ä¸ºç©ºJSON
        if sample['output'] == '{}':
            return False
        
        # æ£€æŸ¥è¾“å‡ºåŒ…å«"workflow"å…³é”®è¯
        if '"workflow"' not in sample['output']:
            return False
        
        return True


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="å‡†å¤‡æ–‡ä»¶çº§æ¨¡å‹è®­ç»ƒæ•°æ®")
    parser.add_argument('--instructions', type=str,
                       default='data/processed/file_level_instructions_weighted_variants_marked.jsonl',
                       help='æ–‡ä»¶çº§æŒ‡ä»¤æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--workflows', type=str,
                       default='data/processed/parsed_workflows.jsonl',
                       help='åŸå§‹å·¥ä½œæµæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str,
                       default='data/training/file_level_training_data.json',
                       help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆä¸å«_train/_valåç¼€ï¼‰')
    parser.add_argument('--max-samples', type=int,
                       help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--split-ratio', type=float, default=0.9,
                       help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤0.9ï¼‰')
    parser.add_argument('--keep-markers', action='store_true',
                       help='ä¿ç•™æƒé‡æ ‡è®°ï¼ˆ**å…³é”®**ï¼‰')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not Path(args.instructions).exists():
        logger.error(f"âŒ Instructions file not found: {args.instructions}")
        return
    
    if not Path(args.workflows).exists():
        logger.error(f"âŒ Workflows file not found: {args.workflows}")
        return
    
    # å‡†å¤‡æ•°æ®
    preparer = FileeLevelTrainingDataPreparer(remove_weight_markers=not args.keep_markers)
    
    train_data, val_data, stats = preparer.prepare_dataset(
        instructions_file=args.instructions,
        workflows_file=args.workflows,
        output_file=args.output,
        max_samples=args.max_samples,
        split_ratio=args.split_ratio
    )
    
    # è¾“å‡ºæ‘˜è¦
    logger.info("\n" + "="*70)
    logger.info("ğŸ‰ æ–‡ä»¶çº§æ•°æ®å‡†å¤‡å®Œæˆï¼")
    logger.info("="*70)
    logger.info(f"ğŸ“Š ç»Ÿè®¡:")
    logger.info(f"  - æ•°æ®ç²’åº¦: æ–‡ä»¶çº§ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰")
    logger.info(f"  - è®­ç»ƒæ ·æœ¬: {stats['train_samples']:,}")
    logger.info(f"  - éªŒè¯æ ·æœ¬: {stats['val_samples']:,}")
    logger.info(f"  - å¹³å‡æŒ‡ä»¤é•¿åº¦: {stats['avg_instruction_length']:.1f} è¯")
    logger.info(f"  - å¹³å‡è¾“å‡ºé•¿åº¦: {stats['avg_output_length']:.1f} å­—ç¬¦")
    logger.info("="*70)
    
    # æ˜¾ç¤ºç¤ºä¾‹
    if train_data:
        logger.info("\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:")
        sample = train_data[0]
        logger.info(f"  Instruction: {sample['instruction'][:100]}...")
        logger.info(f"  Input: {sample['input']}")
        logger.info(f"  Output: {sample['output'][:200]}...")


if __name__ == "__main__":
    main()
