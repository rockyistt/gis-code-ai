# ğŸš€ Colabå†…å­˜ä¼˜åŒ–æŒ‡å—

## é—®é¢˜è¯Šæ–­

å½“ä½ çœ‹åˆ°é”™è¯¯ä¿¡æ¯æ—¶ï¼š
```
RuntimeError: CUDA out of memory. Tried to allocate XXX.XX GiB
```

æˆ–è€…kernelç›´æ¥crashï¼ˆæ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼Œsessionè‡ªåŠ¨é‡å¯ï¼‰ï¼Œè¯´æ˜æ˜¾å­˜å·²ç”¨å°½ã€‚

---

## ğŸ”¥ å·²å®ç°çš„ä¼˜åŒ–ï¼ˆTrain_GIS_Model_Colab.ipynbï¼‰

### 1. **8-bité‡åŒ–åŠ è½½** â­â­â­ (æœ€é‡è¦)
```python
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    load_in_8bit=True,  # å…³é”®ï¼
    device_map="auto",
)
```
- **æ•ˆæœ**ï¼šå‡å°‘40%å†…å­˜å ç”¨ï¼ˆ14GB â†’ 8GBï¼‰
- **ç²¾åº¦æŸå¤±**ï¼š<1%ï¼ŒåŸºæœ¬æ— æ„ŸçŸ¥
- **å‰ç½®æ¡ä»¶**ï¼šéœ€è¦å®‰è£…`bitsandbytes`

**å¯¹æ¯”ï¼š**
| åŠ è½½æ–¹å¼ | å†…å­˜å ç”¨ | ç²¾åº¦ | é€Ÿåº¦ |
|---------|--------|------|------|
| float32 | 14GB+ | 100% | æ­£å¸¸ |
| float16 | 7GB+ | 99.9% | æ­£å¸¸ |
| **8-bit** | **4-5GB** | **99%** | **æ­£å¸¸** |
| 4-bit | 2-3GB | 98% | æ­£å¸¸ |

### 2. **device_map="auto"** â­â­
```python
device_map="auto"  # è‡ªåŠ¨åˆ†å¸ƒæ¨¡å‹å±‚
```
- **æ•ˆæœ**ï¼šæ¨¡å‹å±‚è‡ªåŠ¨åˆ†å¸ƒåœ¨GPUå’ŒCPUé—´
- **åŸç†**ï¼šæ»¡è½½GPUä¼˜å…ˆï¼Œå‰©ä½™å±‚æ”¾CPUå†…å­˜
- **ç¼ºç‚¹**ï¼šGPU-CPUé—´æ•°æ®ç§»åŠ¨æœ‰æ€§èƒ½å¼€é”€ï¼ˆ10-20%ï¼‰

### 3. **å‡å°‘max_new_tokens**
```python
outputs = model.generate(
    ...,
    max_new_tokens=256,  # åŸæ¥512ï¼Œç°åœ¨256
)
```
- **æ•ˆæœ**ï¼šå‡å°‘KVç¼“å­˜å ç”¨
- **å…³ç³»**ï¼šKVç¼“å­˜ = batch_size Ã— seq_len Ã— hidden_dim
- **æƒè¡¡**ï¼šç”Ÿæˆé•¿åº¦å˜çŸ­ï¼Œä½†é€šå¸¸è¶³å¤Ÿ

### 4. **æ¸…ç†å†…å­˜åƒåœ¾**
```python
import gc
gc.collect()
torch.cuda.empty_cache()  # é‡Šæ”¾æ˜¾å­˜ç¢ç‰‡
```
- **ä½•æ—¶è¿è¡Œ**ï¼š
  - åŠ è½½å‰ï¼šæ¸…ç†å‰é¢çš„å˜é‡
  - æ¨ç†åï¼šé‡Šæ”¾è¾“å…¥tensor
  - è¯„ä¼°ç»“æŸï¼šå¸è½½æ¨¡å‹

### 5. **å†…å­˜é¢„æ£€æŸ¥**
```python
# è¯„ä¼°å‰æ£€æŸ¥å¯ç”¨å†…å­˜
if available_memory < 12GB:
    print("âš ï¸ å†…å­˜ä¸è¶³")
```
- **å¥½å¤„**ï¼šæå‰å‘ç°é—®é¢˜ï¼Œé¿å…OOMä¸­é€”crash

---

## ğŸ“Š å†…å­˜å ç”¨å¿«é€Ÿå‚è€ƒ

å‡è®¾ä½¿ç”¨CodeLlama-7Båœ¨T4 GPUï¼ˆ12GBï¼‰ä¸Šï¼š

| ç»„ä»¶ | å ç”¨ | å¤‡æ³¨ |
|------|------|------|
| **float16åŸºç¡€æ¨¡å‹** | 7GB | 7B Ã— 2å­—èŠ‚ |
| **float32åŸºç¡€æ¨¡å‹** | 14GB | 7B Ã— 4å­—èŠ‚ï¼ˆè¶…å‡ºT4ï¼‰ |
| **8-bité‡åŒ–** | 4GB | 7B Ã— 1å­—èŠ‚ |
| **LoRAæƒé‡** | 0.2GB | é€šå¸¸å¾ˆå° |
| **Tokenizer** | <0.1GB | è¯æ±‡è¡¨ |
| **æ¨ç†KVç¼“å­˜** | 1-2GB | éšåºåˆ—é•¿åº¦å¢é•¿ |
| **ç³»ç»Ÿä¿ç•™** | ~2GB | OSå’Œå…¶ä»–åº”ç”¨ |

**æ€»è®¡**ï¼šfloat16ç‰ˆæœ¬çº¦11GBï¼ˆT4å‹‰å¼ºï¼‰ï¼Œ8-bitç‰ˆæœ¬çº¦6GBï¼ˆèˆ’é€‚ï¼‰

---

## âœ… ä½¿ç”¨æ¸…å•

### åŠ è½½æ¨¡å‹å‰ï¼š
- [ ] é‡å¯kernelï¼ˆç¡®ä¿å†…å­˜å¹²å‡€ï¼‰
- [ ] è¿è¡Œ"å†…å­˜é¢„æ£€æŸ¥"cell
- [ ] ç¡®è®¤å¯ç”¨å†…å­˜>12GB
- [ ] å·²å®‰è£…bitsandbytesï¼ˆç”¨äº8-bitï¼‰

### åŠ è½½æ¨¡å‹æ—¶ï¼š
- [ ] ä½¿ç”¨`load_in_8bit=True`
- [ ] ä½¿ç”¨`device_map="auto"`
- [ ] è®¾ç½®`use_cache=False`
- [ ] åŠ è½½å‰è¿è¡Œ`gc.collect()`

### æ¨ç†æ—¶ï¼š
- [ ] é™åˆ¶`max_new_tokens`ï¼ˆæ¨è256æˆ–æ›´å°‘ï¼‰
- [ ] æ¨ç†åé‡Šæ”¾tensor
- [ ] æ¯10ä¸ªæ ·æœ¬è°ƒç”¨ä¸€æ¬¡`torch.cuda.empty_cache()`

### è¯„ä¼°æ—¶ï¼š
- [ ] å‡å°‘è¯„ä¼°æ ·æœ¬æ•°ï¼ˆ50-100ä¸ªè¶³å¤Ÿï¼‰
- [ ] ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦ï¼ˆçŸ¥é“è¿˜è¦å¤šä¹…ï¼‰
- [ ] OOMæ—¶ç«‹å³åœæ­¢ï¼Œä¸è¦ç»§ç»­è¿è¡Œ

---

## ğŸ†˜ åº”æ€¥æ–¹æ¡ˆ

### å¦‚æœä»ç„¶OOMï¼š

**æ–¹æ¡ˆ1ï¼šæ›´æ¿€è¿›çš„é‡åŒ–**
```python
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    load_in_4bit=True,  # 4-bitï¼Œæ›´æ¿€è¿›
    device_map="auto",
)
# é¢„è®¡å†…å­˜ï¼š2-3GB
```

**æ–¹æ¡ˆ2ï¼šå‡å°‘åºåˆ—é•¿åº¦**
```python
# é™åˆ¶è¾“å…¥é•¿åº¦
max_length = 256  # åŸæ¥512

inputs = tokenizer(
    prompt, 
    return_tensors="pt",
    max_length=max_length,
    truncation=True
)
```

**æ–¹æ¡ˆ3ï¼šCPUæ¨ç†ï¼ˆæœ€åæ‰‹æ®µï¼‰**
```python
# åŠ è½½åˆ°CPU
model = model.to("cpu")

# æ¨ç†æ—¶
inputs = tokenizer(prompt, return_tensors="pt")  # CPUä¸Š
outputs = model.generate(**inputs)  # åœ¨CPUä¸Šè¿è¡Œ

# ç¼ºç‚¹ï¼šéå¸¸æ…¢ï¼ˆç§’çº§ â†’ åˆ†é’Ÿçº§ï¼‰
```

**æ–¹æ¡ˆ4ï¼šä½¿ç”¨æ›´å°çš„æ¨¡å‹**
```python
# æ”¹ä¸º7Bæ”¹ä¸º3Bï¼ˆå†…å­˜å ç”¨å‡ä¸€åŠï¼‰
BASE_MODEL = "codellama/CodeLlama-3b-Instruct-hf"

# ä½†è¦ç¡®ä¿è®­ç»ƒæ—¶ä¹Ÿç”¨çš„æ˜¯3B
```

---

## ğŸ“ˆ æ€§èƒ½è°ƒä¼˜

### å†…å­˜ vs æ€§èƒ½æƒè¡¡ï¼š

| è®¾ç½® | å†…å­˜ | é€Ÿåº¦ | è´¨é‡ | æ¨è |
|------|------|------|------|------|
| float32 | âŒ | âš¡âš¡âš¡ | â­â­â­â­â­ | âŒ ä¸é€‚åˆColab |
| float16 | âš ï¸ | âš¡âš¡âš¡ | â­â­â­â­â­ | âœ… A100/H100 |
| **8-bit** | âœ… | âš¡âš¡ | â­â­â­â­â­ | **âœ… T4æ¨è** |
| 4-bit | âœ… | âš¡ | â­â­â­â­ | âœ… æ˜¾å­˜å¾ˆç´§ |
| CPUæ¨ç† | âœ…âœ… | ğŸ¢ | â­â­â­â­â­ | âŒ å¤ªæ…¢ |

---

## ğŸ” ç›‘æ§å†…å­˜ç”¨é‡

### Colabä¸­å®æ—¶ç›‘æ§ï¼š
```python
import psutil
import torch

# ç³»ç»Ÿå†…å­˜
mem = psutil.virtual_memory()
print(f"ç³»ç»Ÿ: {mem.available/1024**3:.1f}GB å¯ç”¨")

# GPUå†…å­˜
print(f"GPU: {torch.cuda.memory_allocated()/1024**3:.1f}GB å·²ç”¨")
print(f"GPU: {torch.cuda.memory_reserved()/1024**3:.1f}GB ä¿ç•™")

# å¯è§†åŒ–ï¼ˆGPUï¼‰
!nvidia-smi
```

---

## å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ

### Error 1: "CUDA out of memory"
```
RuntimeError: CUDA out of memory. Tried to allocate X.XX GiB
```
**åŸå› **ï¼šå•ä¸ªæ“ä½œè¶…è¿‡æ˜¾å­˜
**ä¿®å¤**ï¼š
1. å‡å°‘batch_sizeï¼ˆæ¨ç†ä¸­é€šå¸¸æ˜¯1ï¼‰
2. å‡å°‘max_lengthæˆ–max_new_tokens
3. å¯ç”¨8-bitæˆ–4-bité‡åŒ–

### Error 2: "Model file not found"
```
FileNotFoundError: /content/drive/MyDrive/gis-models/...
```
**åŸå› **ï¼šæ¨¡å‹æ²¡æœ‰ä¿å­˜åˆ°Google Drive
**æ£€æŸ¥**ï¼š
```python
import os
path = "/content/drive/MyDrive/gis-models/codellama-gis-lora"
os.listdir(path)  # çœ‹çœ‹æœ‰ä»€ä¹ˆæ–‡ä»¶
```

### Error 3: Kernel crashï¼ˆç›´æ¥é‡å¯ï¼Œæ²¡æœ‰é”™è¯¯ä¿¡æ¯ï¼‰
```
Your session crashed after X minutes
```
**åŸå› **ï¼šç³»ç»Ÿå†…å­˜ï¼ˆRAMï¼‰ç”¨å®Œäº†ï¼Œä¸ä»…ä»…æ˜¯GPUæ˜¾å­˜
**åŸå› 2**ï¼šæŸä¸ªæ“ä½œå¯¼è‡´æ— æ³•æ¢å¤çš„å†…å­˜æ³„æ¼
**ä¿®å¤**ï¼š
1. é‡å¯kernel
2. å‡å°‘æ ·æœ¬æ•°
3. ç¡®ä¿æ­£ç¡®è°ƒç”¨äº†cleanupä»£ç 

---

## ğŸ“š å‚è€ƒèµ„æº

- [Hugging Face Transformers - Memory Efficient Inference](https://huggingface.co/docs/transformers/perf_infer_gpu_one)
- [PEFT - 8-bit Quantization](https://github.com/huggingface/peft)
- [bitsandbytes Documentation](https://github.com/TimDettmers/bitsandbytes)
- [Colab GPUé…ç½®](https://colab.research.google.com/?utm_source=scs-index)

---

## ğŸ¯ æ€»ç»“

**æ¨èçš„Colabæœ€å°é…ç½®ï¼š**
- âœ… Runtime: T4 GPUï¼ˆå…è´¹ï¼‰æˆ–A100ï¼ˆProï¼‰
- âœ… æ¨¡å‹åŠ è½½: `load_in_8bit=True, device_map="auto"`
- âœ… æ¨ç†è®¾ç½®: `max_new_tokens=256`
- âœ… è¯„ä¼°æ ·æœ¬: 50-100ä¸ª
- âœ… å…¶ä»–: å®šæœŸè°ƒç”¨`gc.collect()`å’Œ`torch.cuda.empty_cache()`

**é¢„æœŸæ€§èƒ½ï¼š**
- æ¨¡å‹åŠ è½½æ—¶é—´ï¼š1-2åˆ†é’Ÿ
- å•ä¸ªæ¨ç†æ—¶é—´ï¼š3-5ç§’ï¼ˆ8-bitï¼‰
- 50ä¸ªæ ·æœ¬å®Œæ•´è¯„ä¼°ï¼š3-5åˆ†é’Ÿ

**å¦‚æœä»ç„¶OOMï¼š**
1. é‡å¯kernel
2. å¯ç”¨4-bité‡åŒ–
3. å‡å°‘è¯„ä¼°æ ·æœ¬åˆ°10-20ä¸ª
4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆ3Bè€Œä¸æ˜¯7Bï¼‰

---

æœ€åæ›´æ–°ï¼š2026å¹´1æœˆ
