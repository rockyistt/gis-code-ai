"""
æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•è¯„ä¼°ç³»ç»Ÿ

è¯„ä¼°ä¸‰ç§ä¸éœ€è¦APIçš„æ–¹æ³•ï¼š
1. åŸºç¡€è§„åˆ™æ¨¡æ¿
2. æ”¹è¿›è§„åˆ™æ¨¡æ¿ï¼ˆå¢å¼ºç‰ˆï¼‰
3. æœ¬åœ°å°å‹æ¨¡å‹ï¼ˆå¯é€‰ï¼Œéœ€è¦ä¸‹è½½ï¼‰

è¯„ä¼°æŒ‡æ ‡ï¼š
- è‡ªåŠ¨åŒ–æŒ‡æ ‡ï¼šç›¸ä¼¼åº¦ã€BLEUã€ROUGE
- è´¨é‡æŒ‡æ ‡ï¼šå®Œæ•´æ€§ã€å‡†ç¡®æ€§ã€å¯è¯»æ€§
- æ•ˆç‡æŒ‡æ ‡ï¼šé€Ÿåº¦ã€èµ„æºå ç”¨
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import statistics


# ============================================================
# è¯„ä¼°æŒ‡æ ‡å®šä¹‰
# ============================================================

class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡è®¡ç®—"""
    
    @staticmethod
    def word_overlap_score(generated: str, reference: str) -> float:
        """è¯é‡å ç‡ - ç®€å•ä½†æœ‰æ•ˆçš„æŒ‡æ ‡"""
        gen_words = set(generated.lower().split())
        ref_words = set(reference.lower().split())
        
        if not ref_words:
            return 0.0
        
        overlap = len(gen_words & ref_words)
        return overlap / len(ref_words)
    
    @staticmethod
    def length_ratio_score(generated: str, reference: str) -> float:
        """é•¿åº¦æ¯”ä¾‹ - è¯„ä¼°ç”Ÿæˆé•¿åº¦æ˜¯å¦åˆç†"""
        gen_len = len(generated.split())
        ref_len = len(reference.split())
        
        if ref_len == 0:
            return 0.0
        
        ratio = gen_len / ref_len
        # ç†æƒ³æ¯”ä¾‹åœ¨0.8-1.2ä¹‹é—´
        if 0.8 <= ratio <= 1.2:
            return 1.0
        elif 0.6 <= ratio <= 1.5:
            return 0.8
        else:
            return 0.5
    
    @staticmethod
    def keyword_coverage_score(generated: str, keywords: List[str]) -> float:
        """å…³é”®è¯è¦†ç›–ç‡"""
        generated_lower = generated.lower()
        covered = sum(1 for kw in keywords if kw.lower() in generated_lower)
        
        if not keywords:
            return 1.0
        
        return covered / len(keywords)
    
    @staticmethod
    def completeness_score(generated: str) -> float:
        """å®Œæ•´æ€§è¯„åˆ† - æ£€æŸ¥ç”Ÿæˆçš„æŒ‡ä»¤æ˜¯å¦å®Œæ•´"""
        score = 0.0
        
        # 1. é•¿åº¦åˆç† (10-200è¯)
        word_count = len(generated.split())
        if 10 <= word_count <= 200:
            score += 0.3
        elif 5 <= word_count <= 250:
            score += 0.15
        
        # 2. åŒ…å«åŠ¨è¯ï¼ˆåŠ¨ä½œï¼‰
        action_verbs = ['create', 'open', 'update', 'delete', 'select', 'click', 
                       'verify', 'switch', 'navigate', 'perform', 'set', 'add']
        if any(verb in generated.lower() for verb in action_verbs):
            score += 0.3
        
        # 3. åŒ…å«å¯¹è±¡
        if any(c.isupper() for c in generated):  # åŒ…å«å¤§å†™å­—æ¯ï¼ˆé€šå¸¸æ˜¯å¯¹è±¡åï¼‰
            score += 0.2
        
        # 4. è¯­æ³•ç»“æ„ï¼ˆç®€å•æ£€æŸ¥ï¼‰
        if generated[0].isupper() and not generated.endswith('...'):
            score += 0.2
        
        return min(score, 1.0)
    
    @staticmethod
    def readability_score(generated: str) -> float:
        """å¯è¯»æ€§è¯„åˆ†"""
        score = 1.0
        
        # æƒ©ç½šå› ç´ 
        # 1. è¿‡é•¿çš„å¥å­
        if len(generated) > 300:
            score -= 0.2
        
        # 2. åŒ…å«æŠ€æœ¯ç»†èŠ‚è¿‡å¤š
        tech_patterns = ['FLD_CSTM', 'gis_program_manager', 'predicate.eq']
        if any(pattern in generated for pattern in tech_patterns):
            score -= 0.3
        
        # 3. åŒ…å«åŸå§‹å­—æ®µå
        if '{' in generated or '}' in generated:
            score -= 0.2
        
        # 4. é‡å¤è¯è¿‡å¤š
        words = generated.lower().split()
        if len(words) > 0:
            unique_ratio = len(set(words)) / len(words)
            if unique_ratio < 0.5:
                score -= 0.2
        
        return max(score, 0.0)
    
    @staticmethod
    def calculate_bleu(generated: str, reference: str) -> float:
        """ç®€åŒ–çš„BLEUåˆ†æ•°ï¼ˆ1-gramå’Œ2-gramï¼‰"""
        gen_words = generated.lower().split()
        ref_words = reference.lower().split()
        
        if not gen_words or not ref_words:
            return 0.0
        
        # 1-gram precision
        gen_1gram = set(gen_words)
        ref_1gram = set(ref_words)
        precision_1 = len(gen_1gram & ref_1gram) / len(gen_1gram) if gen_1gram else 0
        
        # 2-gram precision
        gen_2gram = set(zip(gen_words[:-1], gen_words[1:]))
        ref_2gram = set(zip(ref_words[:-1], ref_words[1:]))
        precision_2 = len(gen_2gram & ref_2gram) / len(gen_2gram) if gen_2gram else 0
        
        # ç»„åˆåˆ†æ•°
        bleu = (precision_1 + precision_2) / 2
        
        # é•¿åº¦æƒ©ç½š
        length_penalty = min(len(gen_words) / len(ref_words), 1.0) if ref_words else 0
        
        return bleu * length_penalty


# ============================================================
# è¯„ä¼°å™¨
# ============================================================

class MethodEvaluator:
    """æ–¹æ³•è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = EvaluationMetrics()
        self.results = defaultdict(list)
    
    def extract_key_info(self, step: Dict) -> Dict[str, Any]:
        """æå–æ­¥éª¤çš„å…³é”®ä¿¡æ¯ç”¨äºè¯„ä¼°"""
        return {
            "module": step.get("module", ""),
            "method": step.get("method", ""),
            "object": step.get("object", ""),
            "database": step.get("database", ""),
        }
    
    def create_reference_instruction(self, step: Dict) -> str:
        """åˆ›å»ºå‚è€ƒæŒ‡ä»¤ï¼ˆç”¨äºå¯¹æ¯”ï¼‰"""
        key_info = self.extract_key_info(step)
        
        # ç®€å•çš„å‚è€ƒæŒ‡ä»¤æ¨¡æ¿
        templates = {
            "Create": f"Create {key_info['object']} object",
            "Update": f"Update {key_info['object']} object", 
            "Delete": f"Delete {key_info['object']} object",
            "Open Object": f"Open {key_info['object']} in editor",
            "Select Tab": f"Select {key_info['object']} tab",
        }
        
        return templates.get(key_info['method'], 
                           f"{key_info['method']} {key_info['object']}")
    
    def evaluate_single(self, generated: str, step: Dict, 
                       method_name: str) -> Dict[str, float]:
        """è¯„ä¼°å•ä¸ªç”Ÿæˆç»“æœ"""
        reference = self.create_reference_instruction(step)
        key_info = self.extract_key_info(step)
        
        # æå–å…³é”®è¯
        keywords = [
            key_info['method'],
            key_info['object'],
            key_info['module']
        ]
        keywords = [k for k in keywords if k]
        
        # è®¡ç®—å„é¡¹æŒ‡æ ‡
        scores = {
            "word_overlap": self.metrics.word_overlap_score(generated, reference),
            "length_ratio": self.metrics.length_ratio_score(generated, reference),
            "keyword_coverage": self.metrics.keyword_coverage_score(generated, keywords),
            "completeness": self.metrics.completeness_score(generated),
            "readability": self.metrics.readability_score(generated),
            "bleu": self.metrics.calculate_bleu(generated, reference)
        }
        
        # ç»¼åˆè¯„åˆ†
        scores["overall"] = (
            scores["word_overlap"] * 0.2 +
            scores["keyword_coverage"] * 0.25 +
            scores["completeness"] * 0.25 +
            scores["readability"] * 0.15 +
            scores["bleu"] * 0.15
        )
        
        # ä¿å­˜ç»“æœ
        self.results[method_name].append(scores)
        
        return scores
    
    def evaluate_method(self, inferencer, steps: List[Dict], 
                       method_name: str) -> Dict[str, Any]:
        """è¯„ä¼°ä¸€ä¸ªæ–¹æ³•åœ¨æ‰€æœ‰æ­¥éª¤ä¸Šçš„è¡¨ç°"""
        print(f"\nè¯„ä¼°æ–¹æ³•: {method_name}")
        print("-" * 60)
        
        all_scores = []
        start_time = time.time()
        
        for i, step in enumerate(steps):
            if (i + 1) % 100 == 0:
                print(f"  è¿›åº¦: {i + 1}/{len(steps)}")
            
            try:
                # ç”ŸæˆæŒ‡ä»¤
                generated = inferencer.infer_step_instruction(step)
                
                # è¯„ä¼°
                scores = self.evaluate_single(generated, step, method_name)
                all_scores.append(scores)
                
            except Exception as e:
                print(f"  âš ï¸  æ­¥éª¤ {i} å¤±è´¥: {e}")
                continue
        
        elapsed_time = time.time() - start_time
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = self.calculate_summary(all_scores, elapsed_time, len(steps))
        
        return summary
    
    def calculate_summary(self, all_scores: List[Dict[str, float]], 
                         elapsed_time: float, total_steps: int) -> Dict[str, Any]:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        if not all_scores:
            return {"error": "No valid scores"}
        
        summary = {
            "total_steps": total_steps,
            "successful_steps": len(all_scores),
            "elapsed_time": elapsed_time,
            "steps_per_second": len(all_scores) / elapsed_time if elapsed_time > 0 else 0,
        }
        
        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡
        for metric in all_scores[0].keys():
            values = [s[metric] for s in all_scores]
            summary[metric] = {
                "mean": statistics.mean(values),
                "median": statistics.median(values),
                "stdev": statistics.stdev(values) if len(values) > 1 else 0,
                "min": min(values),
                "max": max(values)
            }
        
        return summary
    
    def compare_methods(self, summaries: Dict[str, Dict]) -> str:
        """ç”Ÿæˆæ–¹æ³•å¯¹æ¯”æŠ¥å‘Š"""
        report = [
            "\n" + "="*70,
            "æ–¹æ³•å¯¹æ¯”æŠ¥å‘Š",
            "="*70,
        ]
        
        # æå–å…³é”®æŒ‡æ ‡
        metrics_to_compare = ["overall", "completeness", "readability", "keyword_coverage"]
        
        report.append("\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”:")
        report.append("-" * 70)
        
        # è¡¨å¤´
        header = f"{'æŒ‡æ ‡':<20} " + " ".join(f"{name:>15}" for name in summaries.keys())
        report.append(header)
        report.append("-" * 70)
        
        # å„æŒ‡æ ‡å¯¹æ¯”
        for metric in metrics_to_compare:
            values = []
            for method_name, summary in summaries.items():
                if metric in summary and "mean" in summary[metric]:
                    values.append(f"{summary[metric]['mean']:.3f}")
                else:
                    values.append("N/A")
            
            row = f"{metric:<20} " + " ".join(f"{v:>15}" for v in values)
            report.append(row)
        
        # æ€§èƒ½å¯¹æ¯”
        report.append("\nâš¡ æ€§èƒ½å¯¹æ¯”:")
        report.append("-" * 70)
        
        for method_name, summary in summaries.items():
            sps = summary.get('steps_per_second', 0)
            time = summary.get('elapsed_time', 0)
            report.append(f"{method_name:<20} {sps:>10.1f} steps/sec  ({time:.2f}s total)")
        
        # æ¨è
        report.append("\nğŸ† æ¨è:")
        report.append("-" * 70)
        
        # æ‰¾å‡ºæœ€ä½³æ–¹æ³•
        best_overall = max(summaries.items(), 
                          key=lambda x: x[1].get('overall', {}).get('mean', 0))
        fastest = max(summaries.items(),
                     key=lambda x: x[1].get('steps_per_second', 0))
        
        report.append(f"æœ€ä½³è´¨é‡: {best_overall[0]} "
                     f"(ç»¼åˆè¯„åˆ†: {best_overall[1]['overall']['mean']:.3f})")
        report.append(f"æœ€å¿«é€Ÿåº¦: {fastest[0]} "
                     f"({fastest[1]['steps_per_second']:.1f} steps/sec)")
        
        return "\n".join(report)
    
    def save_detailed_results(self, output_file: str):
        """ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(dict(self.results), f, indent=2, ensure_ascii=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜: {output_file}")


# ============================================================
# ä¸‰ç§æ–¹æ³•çš„å®ç°
# ============================================================

class Method1_BasicRules:
    """æ–¹æ³•1: åŸºç¡€è§„åˆ™æ¨¡æ¿"""
    
    def __init__(self):
        self.templates = {
            "Create": "Create {object}",
            "Update": "Update {object}",
            "Delete": "Delete {object}",
            "Open Object": "Open {object}",
            "Select Tab": "Select {object} tab",
            "Click Oneshot Button": "Click {object} button",
        }
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = step.get('object', '')
        
        template = self.templates.get(method, "{method} {object}")
        return template.format(method=method, object=obj)


class Method2_EnhancedRules:
    """æ–¹æ³•2: å¢å¼ºè§„åˆ™æ¨¡æ¿ï¼ˆæ›´è¯¦ç»†çš„è§„åˆ™ï¼‰"""
    
    def __init__(self):
        self.action_verbs = {
            "Create": "Create a new",
            "Update": "Update the existing",
            "Delete": "Delete the",
            "Open Object": "Open",
            "Open Object with ID": "Open",
            "Switch Spatial Context": "Switch to",
            "Verify Field": "Verify",
            "Select Tab": "Navigate to the",
            "Click Oneshot Button": "Click the",
            "Select first HV object": "Select the first",
            "Select second HV object": "Select the second",
            "Datamodel Check": "Perform consistency check on"
        }
    
    def clean_object_name(self, obj: str) -> str:
        if obj.startswith(':'):
            obj = obj[1:]
        return obj
    
    def extract_attributes_count(self, step: Dict) -> int:
        """æå–å±æ€§æ•°é‡"""
        test_data = step.get('test_data', {})
        for section in ['create', 'update']:
            if section in test_data and test_data[section]:
                data = test_data[section]
                for key, value in data.items():
                    if key.startswith('FLD_CSTM') and isinstance(value, dict):
                        return len([k for k in value.keys() if k != 'ID'])
        return 0
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = self.clean_object_name(step.get('object', ''))
        database = step.get('database', '').replace(':', '')
        
        action = self.action_verbs.get(method, method)
        
        # æ ¹æ®æ–¹æ³•ç±»å‹ç”Ÿæˆæ›´è¯¦ç»†çš„æè¿°
        if method == "Create":
            attr_count = self.extract_attributes_count(step)
            if attr_count > 0:
                return f"{action} {obj} object with {attr_count} specified attributes in {database}"
            return f"{action} {obj} object in {database}"
        
        elif method in ["Open Object", "Open Object with ID"]:
            return f"{action} {obj} object in the {database} dataset"
        
        elif method == "Update":
            return f"{action} {obj} object with modified field values"
        
        elif method == "Select Tab":
            return f"{action} {obj} tab in the interface"
        
        elif "HV object" in method:
            position = "first" if "first" in method else "second"
            return f"{action} {obj} in the hierarchy viewer"
        
        else:
            return f"{action} {obj}"


class Method3_ContextAware:
    """æ–¹æ³•3: ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ™ï¼ˆè€ƒè™‘å·¥ä½œæµä¸Šä¸‹æ–‡ï¼‰"""
    
    def __init__(self):
        self.action_verbs = {
            "Create": "Create",
            "Update": "Update",
            "Delete": "Delete",
            "Open Object": "Open",
            "Select Tab": "Navigate to",
            "Click Oneshot Button": "Click",
        }
        self.context_history = []
    
    def clean_object_name(self, obj: str) -> str:
        if obj.startswith(':'):
            obj = obj[1:]
        return obj
    
    def extract_context(self, step: Dict) -> Dict:
        """æå–ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        test_data = step.get('test_data', {})
        context = {}
        
        for section in ['create', 'update', 'editor']:
            if section in test_data and test_data[section]:
                data = test_data[section]
                for key, value in data.items():
                    if key.startswith('FLD_CSTM') and isinstance(value, dict):
                        context['spatial_context'] = value.get('Spatial Context')
                        context['station'] = value.get('Station Nummer')
                        context['attributes'] = [k for k in value.keys() 
                                                if k not in ['ID', 'Spatial Context', 'Station Nummer']]
        
        return context
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = self.clean_object_name(step.get('object', ''))
        database = step.get('database', '').replace(':', '')
        module = step.get('module', '')
        
        action = self.action_verbs.get(method, method)
        context = self.extract_context(step)
        
        # æ„å»ºè¯¦ç»†æŒ‡ä»¤
        instruction_parts = [action, obj]
        
        # æ·»åŠ æ“ä½œç±»å‹
        if method == "Create":
            if context.get('attributes'):
                key_attrs = context['attributes'][:2]  # å‰ä¸¤ä¸ªå±æ€§
                if key_attrs:
                    instruction_parts.append(f"with properties: {', '.join(key_attrs)}")
        
        # æ·»åŠ ä½ç½®ä¿¡æ¯
        if context.get('spatial_context'):
            instruction_parts.append(f"in {context['spatial_context']} context")
        elif database:
            instruction_parts.append(f"in {database} dataset")
        
        # æ·»åŠ æ¨¡å—ä¿¡æ¯ï¼ˆå¯¹äºç‰¹æ®Šæ“ä½œï¼‰
        if module in ['Hierarchy Viewer', 'Datamodel Consistency Check']:
            instruction_parts.append(f"using {module}")
        
        instruction = " ".join(instruction_parts)
        
        # ä¿å­˜åˆ°ä¸Šä¸‹æ–‡
        self.context_history.append({
            'step': step.get('step_index'),
            'object': obj,
            'method': method
        })
        
        return instruction


# ============================================================
# ä¸»è¯„ä¼°ç¨‹åº
# ============================================================

def run_evaluation(test_size: int = 500):
    """è¿è¡Œå®Œæ•´è¯„ä¼°"""
    
    print("="*70)
    print("æŒ‡ä»¤ç”Ÿæˆæ–¹æ³•è¯„ä¼°ç³»ç»Ÿ")
    print("="*70)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path("data/processed/parsed_workflows.jsonl")
    
    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return
    
    # æ”¶é›†æµ‹è¯•æ­¥éª¤
    print(f"\nğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ® (å–å‰{test_size}æ­¥)...")
    test_steps = []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                workflow = json.loads(line)
                test_steps.extend(workflow['steps'])
                if len(test_steps) >= test_size:
                    break
    
    test_steps = test_steps[:test_size]
    print(f"âœ… åŠ è½½äº† {len(test_steps)} ä¸ªæµ‹è¯•æ­¥éª¤")
    
    # åˆå§‹åŒ–ä¸‰ç§æ–¹æ³•
    methods = {
        "æ–¹æ³•1-åŸºç¡€è§„åˆ™": Method1_BasicRules(),
        "æ–¹æ³•2-å¢å¼ºè§„åˆ™": Method2_EnhancedRules(),
        "æ–¹æ³•3-ä¸Šä¸‹æ–‡æ„ŸçŸ¥": Method3_ContextAware(),
    }
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = MethodEvaluator()
    
    # è¯„ä¼°æ¯ç§æ–¹æ³•
    summaries = {}
    for method_name, inferencer in methods.items():
        summary = evaluator.evaluate_method(inferencer, test_steps, method_name)
        summaries[method_name] = summary
        
        # æ‰“å°ç®€è¦ç»“æœ
        print(f"\n{method_name} ç»“æœ:")
        print(f"  ç»¼åˆè¯„åˆ†: {summary['overall']['mean']:.3f}")
        print(f"  å®Œæ•´æ€§: {summary['completeness']['mean']:.3f}")
        print(f"  å¯è¯»æ€§: {summary['readability']['mean']:.3f}")
        print(f"  é€Ÿåº¦: {summary['steps_per_second']:.1f} steps/sec")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report = evaluator.compare_methods(summaries)
    print(report)
    
    # ä¿å­˜è¯¦ç»†ç»“æœ
    output_dir = Path("data/processed/evaluation")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    evaluator.save_detailed_results(str(output_dir / "detailed_scores.json"))
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    with open(output_dir / "summary_report.json", 'w', encoding='utf-8') as f:
        json.dump(summaries, f, indent=2, ensure_ascii=False)
    
    with open(output_dir / "comparison_report.txt", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    
    return summaries, evaluator


if __name__ == "__main__":
    import sys
    
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæµ‹è¯•è§„æ¨¡
    test_size = int(sys.argv[1]) if len(sys.argv) > 1 else 500
    
    print(f"æµ‹è¯•è§„æ¨¡: {test_size} æ­¥éª¤")
    summaries, evaluator = run_evaluation(test_size)
