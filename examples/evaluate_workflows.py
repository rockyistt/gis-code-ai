"""
æ–‡ä»¶çº§åˆ«ï¼ˆå·¥ä½œæµçº§åˆ«ï¼‰çš„è¯„ä¼°ç³»ç»Ÿ

è¯„ä¼°æŒ‡æ ‡ï¼š
1. å·¥ä½œæµæè¿°å‡†ç¡®æ€§
2. æ­¥éª¤åºåˆ—è¿è´¯æ€§
3. å…³é”®å¯¹è±¡è¦†ç›–ç‡
4. æ“ä½œæµç¨‹å®Œæ•´æ€§
5. ä¸šåŠ¡é€»è¾‘å‡†ç¡®æ€§
"""

import json
import time
from pathlib import Path
from typing import Dict, List, Tuple, Any
from collections import defaultdict
import statistics


# ============================================================
# æ–‡ä»¶çº§åˆ«è¯„ä¼°æŒ‡æ ‡
# ============================================================

class WorkflowEvaluationMetrics:
    """å·¥ä½œæµçº§åˆ«çš„è¯„ä¼°æŒ‡æ ‡"""
    
    @staticmethod
    def workflow_description_quality(file_instruction: str, workflow: Dict) -> float:
        """è¯„ä¼°å·¥ä½œæµæè¿°è´¨é‡"""
        score = 0.0
        
        # 1. é•¿åº¦åˆç†æ€§ (20-150è¯)
        word_count = len(file_instruction.split())
        if 20 <= word_count <= 150:
            score += 0.25
        elif 10 <= word_count <= 200:
            score += 0.15
        
        # 2. åŒ…å«å…³é”®ä¿¡æ¯
        file_id = workflow.get('file_id', '')
        
        # æå–å·¥ä½œæµä¸­çš„æ‰€æœ‰å¯¹è±¡
        objects = set()
        operations = set()
        for step in workflow.get('steps', []):
            obj = step.get('object', '')
            if obj:
                objects.add(obj)
            method = step.get('method', '')
            if method:
                operations.add(method)
        
        # æ£€æŸ¥æ˜¯å¦æåŠä¸»è¦å¯¹è±¡
        mentioned_objects = sum(1 for obj in objects if obj in file_instruction)
        if objects:
            object_coverage = mentioned_objects / len(objects)
            score += object_coverage * 0.30
        
        # æ£€æŸ¥æ˜¯å¦æåŠä¸»è¦æ“ä½œ
        mentioned_ops = sum(1 for op in operations if op.lower() in file_instruction.lower())
        if operations:
            op_coverage = mentioned_ops / len(operations)
            score += op_coverage * 0.25
        
        # 3. æ˜¯å¦è¯´æ˜äº†ä¸šåŠ¡ç›®çš„
        purpose_keywords = ['test', 'create', 'configure', 'setup', 'workflow', 
                           'manage', 'verify', 'check', 'process']
        if any(kw in file_instruction.lower() for kw in purpose_keywords):
            score += 0.20
        
        return min(score, 1.0)
    
    @staticmethod
    def step_coherence_score(step_instructions: List[str]) -> float:
        """è¯„ä¼°æ­¥éª¤æŒ‡ä»¤çš„è¿è´¯æ€§"""
        if len(step_instructions) < 2:
            return 1.0
        
        score = 0.0
        
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤æ­¥éª¤æè¿°
        unique_ratio = len(set(step_instructions)) / len(step_instructions)
        if unique_ratio > 0.9:
            score += 0.3
        elif unique_ratio > 0.7:
            score += 0.2
        
        # 2. æ£€æŸ¥æ­¥éª¤æ˜¯å¦æœ‰é€»è¾‘é¡ºåºè¯
        sequence_words = ['first', 'then', 'next', 'after', 'finally', 'before']
        has_sequence = sum(1 for inst in step_instructions 
                          if any(sw in inst.lower() for sw in sequence_words))
        if has_sequence > 0:
            score += 0.2
        
        # 3. æ£€æŸ¥ç›¸é‚»æ­¥éª¤æ˜¯å¦ç›¸å…³
        coherent_pairs = 0
        for i in range(len(step_instructions) - 1):
            inst1 = set(step_instructions[i].lower().split())
            inst2 = set(step_instructions[i + 1].lower().split())
            overlap = len(inst1 & inst2)
            if overlap > 0:  # æœ‰å…±åŒè¯æ±‡
                coherent_pairs += 1
        
        if len(step_instructions) > 1:
            coherence_ratio = coherent_pairs / (len(step_instructions) - 1)
            score += coherence_ratio * 0.3
        
        # 4. æ­¥éª¤é•¿åº¦ä¸€è‡´æ€§
        lengths = [len(inst.split()) for inst in step_instructions]
        if len(lengths) > 1:
            avg_len = statistics.mean(lengths)
            variance = statistics.variance(lengths)
            if variance < avg_len:  # æ–¹å·®å°äºå‡å€¼è¯´æ˜æ¯”è¾ƒä¸€è‡´
                score += 0.2
        
        return min(score, 1.0)
    
    @staticmethod
    def key_object_coverage(file_instruction: str, step_instructions: List[str], 
                           workflow: Dict) -> float:
        """è¯„ä¼°å…³é”®å¯¹è±¡çš„è¦†ç›–ç‡"""
        # æå–å·¥ä½œæµä¸­çš„æ‰€æœ‰å¯¹è±¡
        objects = []
        for step in workflow.get('steps', []):
            obj = step.get('object', '')
            if obj and obj not in ['Object Editor', 'Hierarchy Viewer']:
                objects.append(obj)
        
        if not objects:
            return 1.0
        
        # ç»Ÿè®¡å”¯ä¸€å¯¹è±¡
        unique_objects = set(objects)
        
        # æ£€æŸ¥æ–‡ä»¶çº§æŒ‡ä»¤çš„è¦†ç›–
        file_coverage = sum(1 for obj in unique_objects 
                          if obj in file_instruction)
        
        # æ£€æŸ¥æ­¥éª¤çº§æŒ‡ä»¤çš„è¦†ç›–
        all_step_text = ' '.join(step_instructions)
        step_coverage = sum(1 for obj in unique_objects 
                          if obj in all_step_text)
        
        # ç»¼åˆè¯„åˆ†
        file_score = file_coverage / len(unique_objects) if unique_objects else 1.0
        step_score = step_coverage / len(unique_objects) if unique_objects else 1.0
        
        return (file_score * 0.4 + step_score * 0.6)
    
    @staticmethod
    def operation_flow_completeness(step_instructions: List[str], workflow: Dict) -> float:
        """è¯„ä¼°æ“ä½œæµç¨‹çš„å®Œæ•´æ€§"""
        steps = workflow.get('steps', [])
        
        if not steps:
            return 1.0
        
        score = 0.0
        
        # 1. æ‰€æœ‰æ­¥éª¤éƒ½æœ‰æŒ‡ä»¤
        if len(step_instructions) == len(steps):
            score += 0.3
        else:
            score += (len(step_instructions) / len(steps)) * 0.3
        
        # 2. å…³é”®æ“ä½œç±»å‹éƒ½è¢«è¦†ç›–
        operation_types = set(step.get('method', '') for step in steps)
        mentioned_operations = set()
        
        for inst in step_instructions:
            inst_lower = inst.lower()
            for op in operation_types:
                if op.lower() in inst_lower:
                    mentioned_operations.add(op)
        
        if operation_types:
            op_coverage = len(mentioned_operations) / len(operation_types)
            score += op_coverage * 0.3
        
        # 3. CRUDæ“ä½œçš„å®Œæ•´æ€§
        crud_ops = {'Create': False, 'Update': False, 'Delete': False}
        workflow_has_crud = set()
        
        for step in steps:
            method = step.get('method', '')
            if method in crud_ops:
                workflow_has_crud.add(method)
        
        if workflow_has_crud:
            crud_mentioned = set()
            for inst in step_instructions:
                inst_lower = inst.lower()
                for op in workflow_has_crud:
                    if op.lower() in inst_lower:
                        crud_mentioned.add(op)
            
            crud_score = len(crud_mentioned) / len(workflow_has_crud)
            score += crud_score * 0.2
        else:
            score += 0.2  # æ²¡æœ‰CRUDæ“ä½œä¹Ÿç»™åˆ†
        
        # 4. æ­¥éª¤é¡ºåºçš„ä¿æŒ
        # æ£€æŸ¥æ˜¯å¦ä¿æŒäº†åŸå§‹çš„å¯¹è±¡é¡ºåº
        workflow_objects = [step.get('object', '') for step in steps if step.get('object')]
        instruction_objects = []
        for inst in step_instructions:
            for obj in workflow_objects:
                if obj in inst and obj not in instruction_objects:
                    instruction_objects.append(obj)
        
        # è®¡ç®—é¡ºåºä¿æŒç‡
        if len(workflow_objects) > 1 and len(instruction_objects) > 1:
            order_preserved = sum(1 for i in range(len(instruction_objects) - 1)
                                if workflow_objects.index(instruction_objects[i]) < 
                                   workflow_objects.index(instruction_objects[i + 1]))
            order_score = order_preserved / (len(instruction_objects) - 1)
            score += order_score * 0.2
        else:
            score += 0.2
        
        return min(score, 1.0)
    
    @staticmethod
    def business_logic_accuracy(file_instruction: str, workflow: Dict) -> float:
        """è¯„ä¼°ä¸šåŠ¡é€»è¾‘çš„å‡†ç¡®æ€§"""
        score = 0.0
        
        # 1. è¯†åˆ«å·¥ä½œæµç±»å‹
        file_id = workflow.get('file_id', '').lower()
        is_template = workflow.get('is_high_quality', False)
        
        # å¦‚æœæ˜¯æ¨¡æ¿ï¼Œåº”è¯¥æåŠ
        if is_template:
            if 'template' in file_instruction.lower():
                score += 0.2
        
        # 2. è¯†åˆ«æµ‹è¯•ç±»å‹
        test_cases = workflow.get('test_cases', [])
        if test_cases and test_cases[0]:
            test_type = test_cases[0][0] if test_cases[0] else ''
            
            # æ£€æŸ¥æ˜¯å¦æ­£ç¡®è¯†åˆ«æµ‹è¯•ç±»å‹
            type_keywords = {
                'CRUD': ['create', 'update', 'delete', 'crud', 'data'],
                'Editor': ['editor', 'open', 'edit'],
                'Navigation': ['navigate', 'select', 'tab', 'button']
            }
            
            for key, keywords in type_keywords.items():
                if key.lower() in test_type.lower():
                    if any(kw in file_instruction.lower() for kw in keywords):
                        score += 0.2
                        break
        
        # 3. è¯†åˆ«ä¸»è¦æ•°æ®åº“/åº”ç”¨
        test_app = workflow.get('test_app', '')
        if test_app:
            # æå–åº”ç”¨çš„å…³é”®è¯
            app_words = test_app.lower().split()
            mentioned = sum(1 for word in app_words 
                          if len(word) > 3 and word in file_instruction.lower())
            if mentioned > 0:
                score += 0.2
        
        # 4. è¯†åˆ«ä¸»è¦æ“ä½œæ¨¡å¼
        steps = workflow.get('steps', [])
        create_count = sum(1 for s in steps if s.get('method') == 'Create')
        update_count = sum(1 for s in steps if s.get('method') == 'Update')
        delete_count = sum(1 for s in steps if s.get('method') == 'Delete')
        
        # å¦‚æœæœ‰åˆ›å»ºæ“ä½œï¼Œåº”è¯¥æåŠ
        if create_count > 0 and 'create' in file_instruction.lower():
            score += 0.15
        
        # å¦‚æœæœ‰æ›´æ–°æ“ä½œï¼Œåº”è¯¥æåŠ
        if update_count > 0 and 'update' in file_instruction.lower():
            score += 0.15
        
        # å¦‚æœæ˜¯å®Œæ•´çš„CRUDï¼Œåº”è¯¥æœ‰å®Œæ•´æ€§çš„æè¿°
        if create_count > 0 and update_count > 0 and delete_count > 0:
            if any(word in file_instruction.lower() 
                   for word in ['full', 'complete', 'entire', 'comprehensive']):
                score += 0.1
        
        return min(score, 1.0)


# ============================================================
# æ–‡ä»¶çº§åˆ«è¯„ä¼°å™¨
# ============================================================

class WorkflowEvaluator:
    """å·¥ä½œæµçº§åˆ«çš„è¯„ä¼°å™¨"""
    
    def __init__(self):
        self.metrics = WorkflowEvaluationMetrics()
        self.results = []
    
    def evaluate_workflow(self, file_instruction: str, step_instructions: List[str],
                         workflow: Dict, method_name: str) -> Dict[str, float]:
        """è¯„ä¼°å•ä¸ªå·¥ä½œæµ"""
        scores = {
            "description_quality": self.metrics.workflow_description_quality(
                file_instruction, workflow
            ),
            "step_coherence": self.metrics.step_coherence_score(step_instructions),
            "object_coverage": self.metrics.key_object_coverage(
                file_instruction, step_instructions, workflow
            ),
            "flow_completeness": self.metrics.operation_flow_completeness(
                step_instructions, workflow
            ),
            "business_logic": self.metrics.business_logic_accuracy(
                file_instruction, workflow
            )
        }
        
        # è®¡ç®—ç»¼åˆè¯„åˆ†
        scores["overall"] = (
            scores["description_quality"] * 0.25 +
            scores["step_coherence"] * 0.20 +
            scores["object_coverage"] * 0.20 +
            scores["flow_completeness"] * 0.20 +
            scores["business_logic"] * 0.15
        )
        
        return scores
    
    def evaluate_method(self, inferencer, workflows: List[Dict], 
                       method_name: str) -> Dict[str, Any]:
        """è¯„ä¼°ä¸€ä¸ªæ–¹æ³•åœ¨æ‰€æœ‰å·¥ä½œæµä¸Šçš„è¡¨ç°"""
        print(f"\nè¯„ä¼°æ–¹æ³•: {method_name}")
        print("-" * 60)
        
        all_scores = []
        start_time = time.time()
        
        for i, workflow in enumerate(workflows):
            if (i + 1) % 10 == 0:
                print(f"  è¿›åº¦: {i + 1}/{len(workflows)}")
            
            try:
                # ç”Ÿæˆæ–‡ä»¶çº§æŒ‡ä»¤
                file_instruction = inferencer.infer_workflow_instruction(workflow)
                
                # ç”Ÿæˆæ­¥éª¤çº§æŒ‡ä»¤
                step_instructions = []
                for step in workflow['steps']:
                    step_inst = inferencer.infer_step_instruction(step)
                    step_instructions.append(step_inst)
                
                # è¯„ä¼°
                scores = self.evaluate_workflow(
                    file_instruction, step_instructions, workflow, method_name
                )
                scores['file_id'] = workflow['file_id']
                scores['is_high_quality'] = workflow.get('is_high_quality', False)
                all_scores.append(scores)
                
            except Exception as e:
                print(f"  âš ï¸  å·¥ä½œæµ {workflow.get('file_id')} å¤±è´¥: {e}")
                continue
        
        elapsed_time = time.time() - start_time
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        summary = self.calculate_summary(all_scores, elapsed_time, len(workflows))
        summary['method_name'] = method_name
        
        return summary, all_scores
    
    def calculate_summary(self, all_scores: List[Dict[str, float]], 
                         elapsed_time: float, total_workflows: int) -> Dict[str, Any]:
        """è®¡ç®—æ±‡æ€»ç»Ÿè®¡"""
        if not all_scores:
            return {"error": "No valid scores"}
        
        summary = {
            "total_workflows": total_workflows,
            "successful_workflows": len(all_scores),
            "elapsed_time": elapsed_time,
            "workflows_per_second": len(all_scores) / elapsed_time if elapsed_time > 0 else 0,
        }
        
        # è®¡ç®—æ¯ä¸ªæŒ‡æ ‡çš„ç»Ÿè®¡
        metrics = ["overall", "description_quality", "step_coherence", 
                  "object_coverage", "flow_completeness", "business_logic"]
        
        for metric in metrics:
            values = [s[metric] for s in all_scores]
            summary[metric] = {
                "mean": statistics.mean(values),
                "median": statistics.median(values),
                "stdev": statistics.stdev(values) if len(values) > 1 else 0,
                "min": min(values),
                "max": max(values)
            }
        
        # æŒ‰è´¨é‡åˆ†ç»„ç»Ÿè®¡
        high_quality = [s for s in all_scores if s.get('is_high_quality', False)]
        regular = [s for s in all_scores if not s.get('is_high_quality', False)]
        
        if high_quality:
            summary['high_quality_mean'] = statistics.mean(
                [s['overall'] for s in high_quality]
            )
        
        if regular:
            summary['regular_mean'] = statistics.mean(
                [s['overall'] for s in regular]
            )
        
        return summary
    
    def generate_report(self, summaries: Dict[str, Dict], 
                       detailed_scores: Dict[str, List]) -> str:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        report = [
            "\n" + "="*70,
            "æ–‡ä»¶çº§åˆ«è¯„ä¼°å¯¹æ¯”æŠ¥å‘Š",
            "="*70,
        ]
        
        # æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”
        report.append("\nğŸ“Š æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯” (å·¥ä½œæµçº§åˆ«):")
        report.append("-" * 70)
        
        metrics_display = {
            "ç»¼åˆè¯„åˆ†": "overall",
            "æè¿°è´¨é‡": "description_quality",
            "æ­¥éª¤è¿è´¯æ€§": "step_coherence",
            "å¯¹è±¡è¦†ç›–": "object_coverage",
            "æµç¨‹å®Œæ•´æ€§": "flow_completeness",
            "ä¸šåŠ¡é€»è¾‘": "business_logic"
        }
        
        header = f"{'æŒ‡æ ‡':<15} " + " ".join(f"{name:>18}" for name in summaries.keys())
        report.append(header)
        report.append("-" * 70)
        
        for display_name, metric in metrics_display.items():
            row = f"{display_name:<15} "
            for method_name in summaries.keys():
                if metric in summaries[method_name]:
                    value = summaries[method_name][metric]['mean']
                    row += f"{value:>18.3f} "
                else:
                    row += f"{'N/A':>18} "
            report.append(row)
        
        # æ€§èƒ½å¯¹æ¯”
        report.append("\nâš¡ æ€§èƒ½å¯¹æ¯”:")
        report.append("-" * 70)
        for method_name, summary in summaries.items():
            wps = summary.get('workflows_per_second', 0)
            time_val = summary.get('elapsed_time', 0)
            report.append(f"{method_name:<20} {wps:>10.1f} workflows/sec  ({time_val:.2f}s total)")
        
        # æŒ‰è´¨é‡åˆ†ç»„å¯¹æ¯”
        report.append("\nğŸ“ˆ æŒ‰æ•°æ®è´¨é‡åˆ†ç»„:")
        report.append("-" * 70)
        
        for method_name, summary in summaries.items():
            report.append(f"\n{method_name}:")
            if 'high_quality_mean' in summary:
                report.append(f"  é«˜è´¨é‡æ¨¡æ¿: {summary['high_quality_mean']:.3f}")
            if 'regular_mean' in summary:
                report.append(f"  æ™®é€šå·¥ä½œæµ: {summary['regular_mean']:.3f}")
        
        # æ¨è
        report.append("\n\nğŸ† æ¨è:")
        report.append("-" * 70)
        
        best_overall = max(summaries.items(), 
                          key=lambda x: x[1].get('overall', {}).get('mean', 0))
        fastest = max(summaries.items(),
                     key=lambda x: x[1].get('workflows_per_second', 0))
        
        report.append(f"æœ€ä½³è´¨é‡: {best_overall[0]} "
                     f"(ç»¼åˆè¯„åˆ†: {best_overall[1]['overall']['mean']:.3f})")
        report.append(f"æœ€å¿«é€Ÿåº¦: {fastest[0]} "
                     f"({fastest[1]['workflows_per_second']:.1f} workflows/sec)")
        
        return "\n".join(report)


# ============================================================
# ä¸‰ç§æ–¹æ³•çš„å®ç°ï¼ˆçœŸæ­£ä¸åŒçš„å®ç°ï¼‰
# ============================================================

import sys
from pathlib import Path
# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥SimpleInferencer
import importlib.util
spec = importlib.util.spec_from_file_location(
    "demo_inference", 
    project_root / "examples" / "demo_inference.py"
)
demo_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(demo_module)
SimpleInferencer = demo_module.SimpleInferencer


class Method1_BasicRules:
    """æ–¹æ³•1: åŸºç¡€è§„åˆ™ - ç®€æ´å¿«é€Ÿ"""
    
    def __init__(self):
        self.action_verbs = {
            "Create": "Create",
            "Update": "Update",
            "Delete": "Delete",
            "Open Object": "Open",
            "Select Tab": "Navigate to",
            "Click Oneshot Button": "Click",
        }
    
    def clean_object_name(self, obj: str) -> str:
        if obj.startswith(':'):
            obj = obj[1:]
        return obj
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = self.clean_object_name(step.get('object', ''))
        action = self.action_verbs.get(method, method)
        return f"{action} {obj}"
    
    def infer_workflow_instruction(self, workflow: Dict) -> str:
        """ç®€æ´çš„å·¥ä½œæµæè¿°"""
        steps = workflow.get('steps', [])
        
        # æå–ä¸»è¦æ“ä½œ
        create_objects = []
        for step in steps:
            if step.get('method') == 'Create':
                obj = self.clean_object_name(step.get('object', ''))
                if obj not in create_objects:
                    create_objects.append(obj)
        
        if create_objects:
            if len(create_objects) == 1:
                return f"Test workflow to create {create_objects[0]} in the GIS system"
            else:
                return f"Test workflow to create multiple objects ({', '.join(create_objects[:3])}) in the GIS system"
        else:
            return f"Test workflow for {workflow.get('file_id', 'GIS operations')}"


class Method2_EnhancedRules:
    """æ–¹æ³•2: å¢å¼ºè§„åˆ™ - è¯¦ç»†å®Œæ•´"""
    
    def __init__(self):
        self.action_verbs = {
            "Create": "Create a new",
            "Update": "Update the existing",
            "Delete": "Delete the",
            "Open Object": "Open",
            "Select Tab": "Navigate to the",
            "Click Oneshot Button": "Click the",
        }
    
    def clean_object_name(self, obj: str) -> str:
        if obj.startswith(':'):
            obj = obj[1:]
        return obj
    
    def extract_attributes_count(self, step: Dict) -> int:
        """æå–å±æ€§æ•°é‡"""
        test_data = step.get('test_data', {})
        for section in ['create', 'update']:
            if section in test_data and test_data[section]:
                data = test_data[section]
                for key, value in data.items():
                    if key.startswith('FLD_CSTM') and isinstance(value, dict):
                        return len([k for k in value.keys() if k != 'ID'])
        return 0
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = self.clean_object_name(step.get('object', ''))
        database = step.get('database', '').replace(':', '')
        action = self.action_verbs.get(method, method)
        
        if method == "Create":
            attr_count = self.extract_attributes_count(step)
            if attr_count > 0:
                return f"{action} {obj} object with {attr_count} specified attributes in {database}"
            return f"{action} {obj} object in {database}"
        elif method in ["Open Object", "Open Object with ID"]:
            return f"{action} {obj} object in the {database} dataset"
        else:
            return f"{action} {obj}"
    
    def infer_workflow_instruction(self, workflow: Dict) -> str:
        """è¯¦ç»†çš„å·¥ä½œæµæè¿°ï¼ŒåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ä¿¡æ¯"""
        file_id = workflow.get('file_id', '')
        is_template = workflow.get('is_high_quality', False)
        test_app = workflow.get('test_app', '')
        steps = workflow.get('steps', [])
        
        # åˆ†ææ“ä½œç±»å‹
        create_objects = []
        update_objects = []
        delete_objects = []
        databases = set()
        
        for step in steps:
            method = step.get('method', '')
            obj = self.clean_object_name(step.get('object', ''))
            db = step.get('database', '').replace(':', '')
            
            if db:
                databases.add(db)
            
            if method == 'Create' and obj not in create_objects:
                create_objects.append(obj)
            elif method == 'Update' and obj not in update_objects:
                update_objects.append(obj)
            elif method == 'Delete' and obj not in delete_objects:
                delete_objects.append(obj)
        
        # æ„å»ºæè¿°
        prefix = "Template workflow" if is_template else "Test workflow"
        
        # æ·»åŠ åº”ç”¨ä¿¡æ¯
        if test_app:
            prefix += f" for {test_app}"
        
        # æ·»åŠ æ“ä½œæè¿°
        operations = []
        if create_objects:
            obj_list = ', '.join(create_objects[:3])
            if len(create_objects) > 3:
                obj_list += f" and {len(create_objects) - 3} more"
            operations.append(f"create {obj_list}")
        
        if update_objects:
            operations.append(f"update {len(update_objects)} objects")
        
        if delete_objects:
            operations.append(f"delete {len(delete_objects)} objects")
        
        # æ·»åŠ æ•°æ®åº“ä¿¡æ¯
        db_info = ""
        if databases:
            db_list = ', '.join(list(databases)[:2])
            db_info = f" in {db_list} dataset"
        
        if operations:
            return f"{prefix}: {', '.join(operations)}{db_info}"
        else:
            return f"{prefix}: perform operations on GIS objects{db_info}"


class Method3_ContextAware:
    """æ–¹æ³•3: ä¸Šä¸‹æ–‡æ„ŸçŸ¥ - è€ƒè™‘ä¸šåŠ¡é€»è¾‘"""
    
    def __init__(self):
        self.action_verbs = {
            "Create": "Create",
            "Update": "Update",
            "Delete": "Delete",
            "Open Object": "Open",
            "Select Tab": "Navigate to",
        }
        
        # æœ¯è¯­æ˜ å°„
        self.term_mapping = {
            "E MS Kabel": "Medium Voltage Cable",
            "E HS Kabel": "High Voltage Cable", 
            "E LS Kabel": "Low Voltage Cable",
            "E Stationcomplex": "Station Complex",
            "E MS Installatie": "MS Installation",
            "E HS Aardingstrafo": "HS Grounding Transformer",
            "E MS Aardingstrafo": "MS Grounding Transformer",
        }
    
    def clean_object_name(self, obj: str) -> str:
        if obj.startswith(':'):
            obj = obj[1:]
        return obj
    
    def translate_object(self, obj: str) -> str:
        """ç¿»è¯‘æŠ€æœ¯åç§°ä¸ºå‹å¥½åç§°"""
        return self.term_mapping.get(obj, obj)
    
    def infer_step_instruction(self, step: Dict) -> str:
        method = step.get('method', '')
        obj = self.clean_object_name(step.get('object', ''))
        friendly_obj = self.translate_object(obj)
        action = self.action_verbs.get(method, method)
        
        return f"{action} {friendly_obj}"
    
    def identify_workflow_type(self, workflow: Dict) -> str:
        """è¯†åˆ«å·¥ä½œæµç±»å‹"""
        test_cases = workflow.get('test_cases', [])
        if test_cases and test_cases[0]:
            test_type = test_cases[0][0] if test_cases[0] else ''
            
            if 'CRUD' in test_type:
                return "CRUD operations"
            elif 'Editor' in test_type:
                return "editor operations"
            elif 'Navigation' in test_type:
                return "navigation"
        
        # ä»æ­¥éª¤æ¨æ–­
        steps = workflow.get('steps', [])
        has_create = any(s.get('method') == 'Create' for s in steps)
        has_update = any(s.get('method') == 'Update' for s in steps)
        has_delete = any(s.get('method') == 'Delete' for s in steps)
        
        if has_create and has_update and has_delete:
            return "full CRUD operations"
        elif has_create:
            return "object creation"
        elif has_update:
            return "object modification"
        
        return "GIS operations"
    
    def infer_workflow_instruction(self, workflow: Dict) -> str:
        """ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å·¥ä½œæµæè¿°ï¼Œå¼ºè°ƒä¸šåŠ¡é€»è¾‘"""
        file_id = workflow.get('file_id', '')
        is_template = workflow.get('is_high_quality', False)
        test_app = workflow.get('test_app', '')
        steps = workflow.get('steps', [])
        
        # è¯†åˆ«å·¥ä½œæµç±»å‹
        workflow_type = self.identify_workflow_type(workflow)
        
        # æå–å…³é”®å¯¹è±¡å¹¶ç¿»è¯‘
        key_objects = set()
        for step in steps:
            method = step.get('method', '')
            if method in ['Create', 'Update']:
                obj = self.clean_object_name(step.get('object', ''))
                friendly_obj = self.translate_object(obj)
                key_objects.add(friendly_obj)
        
        # æ„å»ºæè¿°
        prefix = "Template" if is_template else "Test workflow"
        
        # æ·»åŠ åº”ç”¨ä¿¡æ¯
        app_info = ""
        if test_app:
            # æå–åº”ç”¨å…³é”®è¯
            if "Elektra" in test_app:
                app_info = " for electrical network"
            elif "Gas" in test_app:
                app_info = " for gas network"
            else:
                app_info = f" for {test_app}"
        
        # æ·»åŠ å¯¹è±¡ä¿¡æ¯
        obj_info = ""
        if key_objects:
            obj_list = list(key_objects)[:3]
            if len(obj_list) == 1:
                obj_info = f" involving {obj_list[0]}"
            elif len(obj_list) == 2:
                obj_info = f" involving {obj_list[0]} and {obj_list[1]}"
            else:
                obj_info = f" involving {', '.join(obj_list[:2])}, and more"
        
        return f"{prefix}{app_info}: {workflow_type}{obj_info}"


# ============================================================
# ä¸»è¯„ä¼°ç¨‹åº
# ============================================================

def run_workflow_evaluation(test_size: int = 100):
    """è¿è¡Œå·¥ä½œæµçº§åˆ«çš„è¯„ä¼°"""
    
    print("="*70)
    print("å·¥ä½œæµçº§åˆ«è¯„ä¼°ç³»ç»Ÿ")
    print("="*70)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    data_file = Path("data/processed/parsed_workflows.jsonl")
    
    if not data_file.exists():
        print(f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_file}")
        return
    
    print(f"\nğŸ“¥ åŠ è½½æµ‹è¯•æ•°æ® (å–å‰{test_size}ä¸ªå·¥ä½œæµ)...")
    workflows = []
    
    with open(data_file, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= test_size:
                break
            if line.strip():
                workflows.append(json.loads(line))
    
    print(f"âœ… åŠ è½½äº† {len(workflows)} ä¸ªå·¥ä½œæµ")
    print(f"   - é«˜è´¨é‡æ¨¡æ¿: {sum(1 for w in workflows if w.get('is_high_quality'))}")
    print(f"   - æ™®é€šå·¥ä½œæµ: {sum(1 for w in workflows if not w.get('is_high_quality'))}")
    
    # åˆå§‹åŒ–ä¸‰ç§æ–¹æ³•
    methods = {
        "æ–¹æ³•1-åŸºç¡€è§„åˆ™": Method1_BasicRules(),
        "æ–¹æ³•2-å¢å¼ºè§„åˆ™": Method2_EnhancedRules(),
        "æ–¹æ³•3-ä¸Šä¸‹æ–‡æ„ŸçŸ¥": Method3_ContextAware(),
    }
    
    # åˆå§‹åŒ–è¯„ä¼°å™¨
    evaluator = WorkflowEvaluator()
    
    # è¯„ä¼°æ¯ç§æ–¹æ³•
    all_summaries = {}
    all_detailed_scores = {}
    
    for method_name, inferencer in methods.items():
        summary, detailed = evaluator.evaluate_method(inferencer, workflows, method_name)
        all_summaries[method_name] = summary
        all_detailed_scores[method_name] = detailed
        
        # æ‰“å°ç®€è¦ç»“æœ
        print(f"\n{method_name} ç»“æœ:")
        print(f"  ç»¼åˆè¯„åˆ†: {summary['overall']['mean']:.3f}")
        print(f"  æè¿°è´¨é‡: {summary['description_quality']['mean']:.3f}")
        print(f"  æ­¥éª¤è¿è´¯: {summary['step_coherence']['mean']:.3f}")
        print(f"  å¯¹è±¡è¦†ç›–: {summary['object_coverage']['mean']:.3f}")
        print(f"  æµç¨‹å®Œæ•´: {summary['flow_completeness']['mean']:.3f}")
        print(f"  ä¸šåŠ¡é€»è¾‘: {summary['business_logic']['mean']:.3f}")
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report = evaluator.generate_report(all_summaries, all_detailed_scores)
    print(report)
    
    # ä¿å­˜ç»“æœ
    output_dir = Path("data/processed/evaluation")
    output_dir.mkdir(exist_ok=True, parents=True)
    
    # ä¿å­˜æ±‡æ€»
    with open(output_dir / "workflow_summary.json", 'w', encoding='utf-8') as f:
        json.dump(all_summaries, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è¯¦ç»†è¯„åˆ†
    with open(output_dir / "workflow_detailed_scores.json", 'w', encoding='utf-8') as f:
        json.dump(all_detailed_scores, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æŠ¥å‘Š
    with open(output_dir / "workflow_report.txt", 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"   - workflow_summary.json: æ±‡æ€»ç»Ÿè®¡")
    print(f"   - workflow_detailed_scores.json: è¯¦ç»†è¯„åˆ†")
    print(f"   - workflow_report.txt: æ–‡æœ¬æŠ¥å‘Š")
    
    return all_summaries, all_detailed_scores


if __name__ == "__main__":
    import sys
    
    # å¯ä»¥é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæµ‹è¯•è§„æ¨¡
    test_size = int(sys.argv[1]) if len(sys.argv) > 1 else 100
    
    print(f"æµ‹è¯•è§„æ¨¡: {test_size} ä¸ªå·¥ä½œæµ")
    summaries, detailed = run_workflow_evaluation(test_size)
