{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockyistt/gis-code-ai/blob/main/notebooks/Train_GIS_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Disable optional backends to prevent import conflicts (PyTorch-only training)\n",
        "import os\n",
        "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
        "os.environ[\"TRANSFORMERS_NO_TORCHVISION\"] = \"1\"\n",
        "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
        "# Reduce CUDA fragmentation for large models\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "print(\"âœ… Env set: disabled TF/FLAX/TORCHVISION for transformers.\")\n",
        "print(\"âœ… CUDA alloc conf: expandable_segments enabled.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeB5NXZ-CMpB",
        "outputId": "c9c590fc-9ff4-4fae-ac2c-938a27847e83"
      },
      "id": "oeB5NXZ-CMpB",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Env set: disabled TF/FLAX/TORCHVISION for transformers.\n",
            "âœ… CUDA alloc conf: expandable_segments enabled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe0c37",
      "metadata": {
        "id": "33fe0c37"
      },
      "source": [
        "# ğŸš€ GISä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒ - Google Colab (CodeLlama)\n",
        "\n",
        "æœ¬Notebookåœ¨Google Colabä¸Šè®­ç»ƒGISä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆ**æ–‡ä»¶çº§ + CodeLlama**ï¼‰\n",
        "\n",
        "**æ–‡ä»¶çº§è®­ç»ƒ** - æ¨¡å‹å­¦ä¹ ç”Ÿæˆå®Œæ•´çš„å·¥ä½œæµè€Œä¸æ˜¯å•ä¸ªæ­¥éª¤\n",
        "- è¾“å…¥ï¼šç”¨æˆ·çš„é«˜å±‚æŒ‡ä»¤ï¼ˆè‹±è¯­/è·å…°è¯­ï¼Œå¦‚ï¼š\"Create MS and HS cable objects\"ï¼‰\n",
        "- è¾“å‡ºï¼šå®Œæ•´çš„å·¥ä½œæµJSONä»£ç ï¼ˆåŒ…å«æ‰€æœ‰æ“ä½œæ­¥éª¤ï¼‰\n",
        "- ä¼˜åŠ¿ï¼šä¸€æ¬¡æ¨ç†ç”Ÿæˆæ•´ä¸ªæµ‹è¯•è„šæœ¬\n",
        "- **æ¨¡å‹**ï¼šCodeLlama-7B-Instructï¼ˆä¸“ä¸ºä»£ç ç”Ÿæˆä¼˜åŒ–ï¼‰\n",
        "\n",
        "**ä½¿ç”¨å‰å‡†å¤‡ï¼š**\n",
        "1. è¿è¡Œç¯å¢ƒï¼š`Runtime > Change runtime type > T4 GPU`ï¼ˆå…è´¹ï¼‰æˆ– `A100 GPU`ï¼ˆColab Proï¼‰\n",
        "2. æ•°æ®å‡†å¤‡ï¼šç¡®ä¿å·²ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶\n",
        "3. é¢„è®¡æ—¶é—´ï¼š4-6å°æ—¶ï¼ˆT4ï¼‰/ 1-2å°æ—¶ï¼ˆA100ï¼‰\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130ae856",
      "metadata": {
        "id": "130ae856"
      },
      "source": [
        "## ğŸ“‹ æ­¥éª¤1ï¼šç¯å¢ƒè®¾ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "859e1522",
      "metadata": {
        "id": "859e1522",
        "outputId": "b3abe9ec-073e-4766-b797-3f84dd53427a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 19 08:53:47 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8d621428",
      "metadata": {
        "id": "8d621428",
        "outputId": "df100bdf-0ce9-40a4-ba62-9a453fbbf248",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "âœ… Core dependencies installed! If running in Colab, restart runtime after this cell.\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£…ä¾èµ–ï¼ˆçº¦3-5åˆ†é’Ÿï¼‰\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "\n",
        "# å…ˆé”å®šå…³é”®åŸºç¡€åŒ…ï¼ˆé¿å…è‡ªåŠ¨å‡çº§ï¼‰\n",
        "!pip install -q torch==2.9.0 --no-deps\n",
        "!pip install -q fsspec==2024.3.1\n",
        "!pip install -q numpy==2.0.2 --no-deps\n",
        "\n",
        "# å®‰è£…ä¸»è¦è®­ç»ƒåº“ï¼ˆæŒ‡å®šå…¼å®¹ç‰ˆæœ¬ï¼‰\n",
        "!pip install -q transformers==4.46.0\n",
        "!pip install -q peft==0.13.0\n",
        "!pip install -q datasets==2.19.0\n",
        "!pip install -q \"accelerate>=1.0.0\"\n",
        "!pip install -q sentencepiece==0.2.0\n",
        "!pip install -q tqdm\n",
        "!pip install -q huggingface-hub==0.26.0\n",
        "\n",
        "print(\"âœ… Core dependencies installed! If running in Colab, restart runtime after this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchvision"
      ],
      "metadata": {
        "id": "Ntp_pEd3DF9n"
      },
      "id": "Ntp_pEd3DF9n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "62236c8b",
      "metadata": {
        "id": "62236c8b"
      },
      "source": [
        "## ğŸ’¾ æ­¥éª¤2ï¼šæŒ‚è½½Google Driveï¼ˆä¿å­˜æ¨¡å‹ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6b086f8d",
      "metadata": {
        "id": "6b086f8d",
        "outputId": "c900101f-bf9d-4610-91fa-921d68082490",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "!mkdir -p /content/drive/MyDrive/gis-models\n",
        "print(\"âœ… Google Drive mounted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752feee6",
      "metadata": {
        "id": "752feee6"
      },
      "source": [
        "## ğŸ“‚ æ­¥éª¤3ï¼šä¸Šä¼ æ•°æ®\n",
        "\n",
        "**ä¸¤ç§æ–¹å¼ä»»é€‰å…¶ä¸€ï¼š**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5240de6f",
      "metadata": {
        "id": "5240de6f"
      },
      "source": [
        "### æ–¹å¼Aï¼šä»GitHubå…‹éš†ä»“åº“ï¼ˆæ¨èï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7ff6558e",
      "metadata": {
        "id": "7ff6558e",
        "outputId": "c84ff28d-a923-43ab-8311-d4f2d34849f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Current directory: /content\n",
            "ğŸ”„ Cloning repository...\n",
            "âœ… Repository cloned successfully\n",
            "ğŸ“ Working directory: /content/gis-code-ai\n",
            "\n",
            "ğŸ“‹ Checking data files...\n",
            "âœ… data/processed/file_level_instructions_weighted_variants_marked.jsonl (2.1 MB)\n",
            "âœ… data/processed/parsed_workflows.jsonl (13.8 MB)\n",
            "\n",
            "ğŸ‰ All data files ready! Ready to proceed with data preparation\n"
          ]
        }
      ],
      "source": [
        "# å…‹éš†ä»“åº“åˆ°å½“å‰ç›®å½•\n",
        "import os\n",
        "import subprocess\n",
        "\n",
        "print(\"ğŸ“ Current directory:\", os.getcwd())\n",
        "\n",
        "# æ£€æŸ¥æ˜¯å¦å·²å…‹éš†\n",
        "if os.path.exists('/content/gis-code-ai'):\n",
        "    print(\"âœ… Repository already exists at /content/gis-code-ai\")\n",
        "    os.chdir('/content/gis-code-ai')\n",
        "elif os.path.exists('gis-code-ai'):\n",
        "    print(\"âœ… Repository already exists locally\")\n",
        "    os.chdir('gis-code-ai')\n",
        "else:\n",
        "    print(\"ğŸ”„ Cloning repository...\")\n",
        "    result = subprocess.run(\n",
        "        ['git', 'clone', 'https://github.com/rockyistt/gis-code-ai.git'],\n",
        "        cwd='/content',\n",
        "        capture_output=True,\n",
        "        text=True\n",
        "    )\n",
        "    if result.returncode == 0:\n",
        "        os.chdir('/content/gis-code-ai')\n",
        "        print(\"âœ… Repository cloned successfully\")\n",
        "    else:\n",
        "        print(f\"âŒ Clone failed: {result.stderr}\")\n",
        "        raise RuntimeError(\"Failed to clone repository\")\n",
        "\n",
        "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
        "print(\"\\nğŸ“‹ Checking data files...\")\n",
        "required_files = [\n",
        "    'data/processed/file_level_instructions_weighted_variants_marked.jsonl',\n",
        "    'data/processed/parsed_workflows.jsonl'\n",
        "]\n",
        "\n",
        "all_exist = True\n",
        "for f in required_files:\n",
        "    if os.path.exists(f):\n",
        "        size_mb = os.path.getsize(f) / (1024*1024)\n",
        "        print(f\"âœ… {f} ({size_mb:.1f} MB)\")\n",
        "    else:\n",
        "        print(f\"âŒ {f}\")\n",
        "        all_exist = False\n",
        "\n",
        "if all_exist:\n",
        "    print(\"\\nğŸ‰ All data files ready! Ready to proceed with data preparation\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸ Some data files are missing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f32dcb",
      "metadata": {
        "id": "23f32dcb"
      },
      "source": [
        "## ğŸ”„ æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a303fdcd",
      "metadata": {
        "id": "a303fdcd",
        "outputId": "92653982-398d-4127-ea01-f71aad057735",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Data preparation function defined\n"
          ]
        }
      ],
      "source": [
        "# å®šä¹‰æ•°æ®å‡†å¤‡å‡½æ•°ï¼ˆç›´æ¥åœ¨notebookä¸­è¿è¡Œï¼‰\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "def prepare_file_level_training_data(instructions_file, workflows_file, output_dir, max_samples=None, split_ratio=0.9):\n",
        "    \"\"\"å‡†å¤‡æ–‡ä»¶çº§è®­ç»ƒæ•°æ®\"\"\"\n",
        "\n",
        "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
        "    if not os.path.exists(instructions_file):\n",
        "        print(f\"âŒ Instructions file not found: {instructions_file}\")\n",
        "        print(\"\\nğŸ“‚ Available files in data/processed/:\")\n",
        "        if os.path.exists('data/processed'):\n",
        "            for f in os.listdir('data/processed'):\n",
        "                if os.path.isfile(f'data/processed/{f}'):\n",
        "                    size = os.path.getsize(f'data/processed/{f}') / (1024*1024)\n",
        "                    print(f\"  - {f} ({size:.1f} MB)\")\n",
        "        else:\n",
        "            print(\"  âŒ data/processed/ directory not found!\")\n",
        "        return None, None\n",
        "\n",
        "    if not os.path.exists(workflows_file):\n",
        "        print(f\"âŒ Workflows file not found: {workflows_file}\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"ğŸ“– Loading file-level instructions from {instructions_file}\")\n",
        "    instructions = {}\n",
        "    with open(instructions_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                item = json.loads(line)\n",
        "                file_id = item.get('file_id', '')\n",
        "                instructions[file_id] = item\n",
        "    print(f\"âœ… Loaded {len(instructions)} file-level instructions\")\n",
        "\n",
        "    print(f\"ğŸ“– Loading workflows from {workflows_file}\")\n",
        "    workflows = {}\n",
        "    with open(workflows_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                wf = json.loads(line)\n",
        "                workflows[wf.get('file_id', '')] = wf\n",
        "    print(f\"âœ… Loaded {len(workflows)} workflows\")\n",
        "\n",
        "    print(\"ğŸ”„ Converting to training format (file-level)...\")\n",
        "    training_samples = []\n",
        "\n",
        "    count = 0\n",
        "    for file_id, instr in tqdm(instructions.items()):\n",
        "        workflow = workflows.get(file_id, {})\n",
        "\n",
        "        if not workflow:\n",
        "            continue\n",
        "\n",
        "        # æ¸…ç†æŒ‡ä»¤\n",
        "        instruction_text = instr.get('instruction', '').replace('**', '').replace('*', '')\n",
        "        instruction_text = ' '.join(instruction_text.split()).strip()\n",
        "\n",
        "        # æ„å»ºä¸Šä¸‹æ–‡\n",
        "        test_app = workflow.get('test_app', '')\n",
        "        database = workflow.get('database', '')\n",
        "        total_steps = workflow.get('total_steps', 0)\n",
        "\n",
        "        context_parts = []\n",
        "        if test_app:\n",
        "            context_parts.append(f\"Application: {test_app}\")\n",
        "        if database:\n",
        "            context_parts.append(f\"Database: {database}\")\n",
        "        if total_steps > 0:\n",
        "            context_parts.append(f\"Steps: {total_steps}\")\n",
        "        input_context = \" | \".join(context_parts)\n",
        "\n",
        "        # æå–å®Œæ•´å·¥ä½œæµJSONï¼ˆæ‰€æœ‰stepsï¼‰\n",
        "        steps = workflow.get('steps', [])\n",
        "        if not steps:\n",
        "            continue\n",
        "\n",
        "        workflow_output = {\n",
        "            \"workflow\": {\n",
        "                \"metadata\": {\n",
        "                    \"test_app\": test_app,\n",
        "                    \"database\": database,\n",
        "                    \"total_steps\": len(steps)\n",
        "                },\n",
        "                \"steps\": steps\n",
        "            }\n",
        "        }\n",
        "        output_code = json.dumps(workflow_output, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # è´¨é‡è¿‡æ»¤\n",
        "        if len(instruction_text.split()) < 5 or output_code == '{}':\n",
        "            continue\n",
        "\n",
        "        training_samples.append({\n",
        "            \"instruction\": instruction_text,\n",
        "            \"input\": input_context,\n",
        "            \"output\": output_code\n",
        "        })\n",
        "\n",
        "        count += 1\n",
        "        if max_samples and count >= max_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"âœ… Created {len(training_samples)} training samples\")\n",
        "\n",
        "    if len(training_samples) == 0:\n",
        "        print(\"âŒ No training samples created! Check your data files.\")\n",
        "        return None, None\n",
        "\n",
        "    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "    split_idx = int(len(training_samples) * split_ratio)\n",
        "    train_data = training_samples[:split_idx]\n",
        "    val_data = training_samples[split_idx:]\n",
        "\n",
        "    print(f\"ğŸ“Š Split: {len(train_data)} train, {len(val_data)} validation\")\n",
        "\n",
        "    # ä¿å­˜\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_file = output_dir / \"training_data_train.json\"\n",
        "    val_file = output_dir / \"training_data_val.json\"\n",
        "\n",
        "    with open(train_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"ğŸ’¾ Train data saved: {train_file}\")\n",
        "\n",
        "    with open(val_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"ğŸ’¾ Validation data saved: {val_file}\")\n",
        "\n",
        "    return train_data, val_data\n",
        "\n",
        "print(\"âœ… Data preparation function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3b1b1c77",
      "metadata": {
        "id": "3b1b1c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "257972fa-4275-40af-a2e0-ea6e3523efd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "ğŸ”„ å¼€å§‹æ•°æ®å‡†å¤‡...\n",
            "ğŸ“ Working directory: /content/gis-code-ai\n",
            "======================================================================\n",
            "ğŸ“– Loading file-level instructions from data/processed/file_level_instructions_weighted_variants_marked.jsonl\n",
            "âœ… Loaded 1012 file-level instructions\n",
            "ğŸ“– Loading workflows from data/processed/parsed_workflows.jsonl\n",
            "âœ… Loaded 1012 workflows\n",
            "ğŸ”„ Converting to training format (file-level)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1012/1012 [00:00<00:00, 1886.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Created 1012 training samples\n",
            "ğŸ“Š Split: 910 train, 102 validation\n",
            "ğŸ’¾ Train data saved: data/training/training_data_train.json\n",
            "ğŸ’¾ Validation data saved: data/training/training_data_val.json\n",
            "\n",
            "======================================================================\n",
            "ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼\n",
            "ğŸ“Š æ•°æ®ç²’åº¦: æ–‡ä»¶çº§ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰\n",
            "ğŸ“Š è®­ç»ƒæ ·æœ¬: 910\n",
            "ğŸ“Š éªŒè¯æ ·æœ¬: 102\n",
            "======================================================================\n",
            "\n",
            "ğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\n",
            "Instruction: Workflow: create E MS Kabel, E HS Kabel, E LS Kabel in elektra in NRG Beheerkaart Elektra MS...\n",
            "Input: Application: NRG Beheerkaart Elektra MS | Steps: 7\n",
            "Output preview: {\n",
            "  \"workflow\": {\n",
            "    \"metadata\": {\n",
            "      \"test_app\": \"NRG Beheerkaart Elektra MS\",\n",
            "      \"database\": \"\",\n",
            "      \"total_steps\": 7\n",
            "    },\n",
            "    \"steps\": [\n",
            "      {\n",
            "        \"step_index\": 0,\n",
            "        \"databas...\n"
          ]
        }
      ],
      "source": [
        "# è¿è¡Œæ•°æ®å‡†å¤‡ï¼ˆç›´æ¥è°ƒç”¨å‡½æ•°ï¼‰\n",
        "import os\n",
        "\n",
        "# ç¡®ä¿åœ¨æ­£ç¡®çš„ç›®å½•\n",
        "if os.path.exists('/content/gis-code-ai'):\n",
        "    os.chdir('/content/gis-code-ai')\n",
        "elif os.path.exists('gis-code-ai'):\n",
        "    os.chdir('gis-code-ai')\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ğŸ”„ å¼€å§‹æ•°æ®å‡†å¤‡...\")\n",
        "print(f\"ğŸ“ Working directory: {os.getcwd()}\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "train_data, val_data = prepare_file_level_training_data(\n",
        "    instructions_file='data/processed/file_level_instructions_weighted_variants_marked.jsonl',\n",
        "    workflows_file='data/processed/parsed_workflows.jsonl',\n",
        "    output_dir='data/training',\n",
        "    max_samples=None,  # ä½¿ç”¨å…¨éƒ¨æ•°æ®ï¼Œæ”¹ä¸ºæ•´æ•°å¯é™åˆ¶æ ·æœ¬æ•°ï¼ˆå¦‚500ï¼‰\n",
        "    split_ratio=0.9\n",
        ")\n",
        "\n",
        "if train_data is not None:\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n",
        "    print(f\"ğŸ“Š æ•°æ®ç²’åº¦: æ–‡ä»¶çº§ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰\")\n",
        "    print(f\"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_data):,}\")\n",
        "    print(f\"ğŸ“Š éªŒè¯æ ·æœ¬: {len(val_data):,}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # æ˜¾ç¤ºç¤ºä¾‹\n",
        "    if train_data:\n",
        "        print(\"\\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\")\n",
        "        sample = train_data[0]\n",
        "        print(f\"Instruction: {sample['instruction'][:100]}...\")\n",
        "        print(f\"Input: {sample['input']}\")\n",
        "        print(f\"Output preview: {sample['output'][:200]}...\")\n",
        "else:\n",
        "    print(\"\\nâŒ æ•°æ®å‡†å¤‡å¤±è´¥ï¼è¯·æ£€æŸ¥ä¸Šé¢çš„é”™è¯¯ä¿¡æ¯ã€‚\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e032229f",
      "metadata": {
        "id": "e032229f"
      },
      "source": [
        "## ğŸš€ æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "6a171cf7",
      "metadata": {
        "id": "6a171cf7",
        "outputId": "cc3a200f-c58e-4670-acd2-800690eb9d98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Setting up training with CodeLlama...\n",
            "ğŸ“¦ Model: codellama/CodeLlama-7b-Instruct-hf\n",
            "ğŸ’¾ Output: /content/drive/MyDrive/gis-models/codellama-gis-lora\n",
            "ğŸ“Š Data: FILE-LEVEL (complete workflows)\n",
            "ğŸ“ˆ Epochs: 3, Batch: 1, Accumulation: 2, LR: 0.0002\n",
            "ğŸ“„ Max Length: 512\n",
            "ğŸ¯ LoRA Rank: 32 (T4-optimized)\n",
            "ğŸ¯ Optimized for: T4 GPU (14GB VRAM) - memory-efficient config\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"ğŸ”§ Setting up training with CodeLlama...\")\n",
        "\n",
        "# ============================================================\n",
        "# é…ç½®å‚æ•° - CodeLlama (è‹±è¯­/è·å…°è¯­ä»£ç ç”Ÿæˆä¼˜åŒ–)\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/gis-models/codellama-gis-lora\"  # ä¿å­˜åˆ°Google Drive\n",
        "TRAIN_FILE = \"data/training/training_data_train.json\"\n",
        "VAL_FILE = \"data/training/training_data_val.json\"\n",
        "\n",
        "# è®­ç»ƒå‚æ•°ï¼ˆT4 GPUå†…å­˜ä¼˜åŒ–ç‰ˆ - 14GBæ˜¾å­˜é™åˆ¶ï¼‰\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 1  # Per-device batch size\n",
        "GRADIENT_ACCUMULATION = 2  # Reduced from 4 â†’ effective batch = 2\n",
        "LEARNING_RATE = 2e-4  # CodeLlamaæ¨èå­¦ä¹ ç‡\n",
        "MAX_LENGTH = 512  # Reduced from 768 â†’ significant memory savings\n",
        "\n",
        "# LoRAå‚æ•°ï¼ˆä»£ç ç”Ÿæˆä»»åŠ¡ä¼˜åŒ– - T4ä¼˜åŒ–ç‰ˆï¼‰\n",
        "LORA_R = 32  # Reduced from 64 â†’ less trainable parameters\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(f\"ğŸ“¦ Model: {MODEL_NAME}\")\n",
        "print(f\"ğŸ’¾ Output: {OUTPUT_DIR}\")\n",
        "print(f\"ğŸ“Š Data: FILE-LEVEL (complete workflows)\")\n",
        "print(f\"ğŸ“ˆ Epochs: {NUM_EPOCHS}, Batch: {BATCH_SIZE}, Accumulation: {GRADIENT_ACCUMULATION}, LR: {LEARNING_RATE}\")\n",
        "print(f\"ğŸ“„ Max Length: {MAX_LENGTH}\")\n",
        "print(f\"ğŸ¯ LoRA Rank: {LORA_R} (T4-optimized)\")\n",
        "print(f\"ğŸ¯ Optimized for: T4 GPU (14GB VRAM) - memory-efficient config\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "774b429b",
      "metadata": {
        "id": "774b429b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e142717-02bb-494f-ba36-8835bc110290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“– Loading CodeLlama tokenizer...\n",
            "âœ… Tokenizer loaded: vocab_size=32016\n"
          ]
        }
      ],
      "source": [
        "# åŠ è½½tokenizer (CodeLlama)\n",
        "print(\"ğŸ“– Loading CodeLlama tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ… Tokenizer loaded: vocab_size={len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# âš ï¸ å¿…é¡»åœ¨model loadingå‰è¿è¡Œï¼å‡çº§å…³é”®åº“\n",
        "print(\"ğŸ”§ Upgrading transformers, accelerate, and installing bitsandbytes...\")\n",
        "\n",
        "# å‡çº§transformersåˆ°å…¼å®¹ç‰ˆæœ¬\n",
        "!pip install -q --upgrade transformers==4.46.0\n",
        "\n",
        "# å‡çº§accelerateåˆ°æœ€æ–°ç¨³å®šç‰ˆä¿®å¤optimizer.train()é”™è¯¯\n",
        "!pip install -q --upgrade accelerate>=1.0.0\n",
        "\n",
        "# å®‰è£…bitsandbytesç”¨äº8-bitä¼˜åŒ–å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰\n",
        "!pip install -q bitsandbytes\n",
        "\n",
        "print(\"âœ… Libraries upgraded!\")\n",
        "print(f\"  transformers: 4.46.0\")\n",
        "print(f\"  accelerate: >=1.0.0 (latest stable, fixes optimizer.train() bug)\")\n",
        "print(f\"  bitsandbytes: installed (8-bit optimizer for memory efficiency)\")\n",
        "print(\"âš ï¸ IMPORTANT: Runtime restart recommended!\")\n",
        "print(\"   Please go to: Runtime > Restart runtime\")\n",
        "print(\"   Then re-run all cells in order\")"
      ],
      "metadata": {
        "id": "uqeoQv1HIRN3",
        "outputId": "0f676b74-a637-4b9a-c107-5843380be306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uqeoQv1HIRN3",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Upgrading transformers, accelerate, and installing bitsandbytes...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Libraries upgraded!\n",
            "  transformers: 4.46.0\n",
            "  accelerate: >=1.0.0 (latest stable, fixes optimizer.train() bug)\n",
            "  bitsandbytes: installed (8-bit optimizer for memory efficiency)\n",
            "âš ï¸ IMPORTANT: Runtime restart recommended!\n",
            "   Please go to: Runtime > Restart runtime\n",
            "   Then re-run all cells in order\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "7b4deb72",
      "metadata": {
        "id": "7b4deb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "eccd0ef0b0554a25b714d6b4b644fa88",
            "1f9b436ef6c049a799f0d33f2673f5a7",
            "e03dadf8ab4440828dfef1de3ead8212",
            "fd46f5c1c3f2485196c00822c81ea5d9",
            "37b085c719e44d72bb42c734b72d8ce5",
            "ca6594a99db249cb9d48e075d00f5296",
            "12fcebff7ee4475592d2d4381157c11b",
            "f58bb16455274768aa4d37022235a663",
            "e8e990cf4d514de1964ec60eb01a0f5f",
            "1942abeeaef34692b453725876588c59",
            "20d5495a979148fe96161b6239e89573"
          ]
        },
        "outputId": "dffacd90-a28d-4a02-97c6-78e76e57e083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¤– Loading CodeLlama-7B with float16 and device offloading...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/accelerate/utils/modeling.py:1566: UserWarning: Current model requires 4224 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eccd0ef0b0554a25b714d6b4b644fa88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… CodeLlama base model loaded (float16 with device offloading)\n",
            "ğŸ”§ Applying LoRA...\n",
            "trainable params: 79,953,920 || all params: 6,818,500,608 || trainable%: 1.1726\n",
            "âœ… LoRA applied!\n"
          ]
        }
      ],
      "source": [
        "# ç¡®ä¿å¯¼å…¥äº†å¿…è¦çš„åº“\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "import torch\n",
        "\n",
        "# ä½¿ç”¨float16åŠ è½½æ¨¡å‹ï¼ˆä¸é‡åŒ–ï¼Œç¨³å®šé€‚é…T4æ˜¾å­˜ï¼‰\n",
        "print(\"ğŸ¤– Loading CodeLlama-7B with float16 and device offloading...\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    low_cpu_mem_usage=True,\n",
        ")\n",
        "\n",
        "# è®­ç»ƒä¸­å¿…é¡»ç¦ç”¨ç¼“å­˜ä»¥é…åˆæ¢¯åº¦æ£€æŸ¥ç‚¹\n",
        "model.config.use_cache = False\n",
        "\n",
        "# å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "print(\"âœ… CodeLlama base model loaded (float16 with device offloading)\")\n",
        "\n",
        "# åº”ç”¨LoRA\n",
        "print(\"ğŸ”§ Applying LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"âœ… LoRA applied!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b3a6aeaa",
      "metadata": {
        "id": "b3a6aeaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "b6e0dcfa-2e0d-4929-ab02-8161da7c7676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“Š Preparing datasets...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'TRAIN_FILE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-798126021.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ğŸ“Š Preparing datasets...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0meval_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVAL_FILE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TRAIN_FILE' is not defined"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# å‡†å¤‡æ•°æ®é›†\n",
        "print(\"ğŸ“Š Preparing datasets...\")\n",
        "\n",
        "train_data = load_dataset('json', data_files=TRAIN_FILE, split='train')\n",
        "eval_data = load_dataset('json', data_files=VAL_FILE, split='train')\n",
        "\n",
        "print(f\"  Train: {len(train_data)} samples\")\n",
        "print(f\"  Val: {len(eval_data)} samples\")\n",
        "\n",
        "# æ ¼å¼åŒ–prompt (CodeLlamaä¼˜åŒ–æ ¼å¼)\n",
        "def format_prompt(example):\n",
        "    instruction = example['instruction']\n",
        "    input_text = example.get('input', '')\n",
        "    output = example['output']\n",
        "\n",
        "    # CodeLlamaæ›´é€‚åˆç›´æ¥çš„ä»£ç ç”Ÿæˆæ ¼å¼\n",
        "    if input_text:\n",
        "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {instruction}\n",
        "Context: {input_text}\n",
        "\n",
        "JSON Code:\n",
        "{output}\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {instruction}\n",
        "\n",
        "JSON Code:\n",
        "{output}\"\"\"\n",
        "\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "train_data = train_data.map(format_prompt, remove_columns=train_data.column_names)\n",
        "eval_data = eval_data.map(format_prompt, remove_columns=eval_data.column_names)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=False,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"ğŸ”„ Tokenizing...\")\n",
        "train_dataset = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_data.column_names,\n",
        "    desc=\"Tokenizing train\"\n",
        ")\n",
        "\n",
        "eval_dataset = eval_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_data.column_names,\n",
        "    desc=\"Tokenizing val\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Datasets ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "246a2b8b",
      "metadata": {
        "id": "246a2b8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "e37a49dc-42a1-4648-c79b-d9ab26b3f794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âš™ï¸ Configuring training...\n",
            "âœ… Trainer ready!\n",
            "\n",
            "======================================================================\n",
            "ğŸš€ Starting training...\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3540246281.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 4103 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 71.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3540246281.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# å¼€å§‹è®­ç»ƒ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2120\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2122\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2123\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2525\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_pre_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2527\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2529\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_optimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_patched_step_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m         )\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_opt_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stage\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOptState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTEPPED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/grad_scaler.py\u001b[0m in \u001b[0;36m_maybe_opt_step\u001b[0;34m(self, optimizer, optimizer_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mretval\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"found_inf_per_device\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mretval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/optimizer.py\u001b[0m in \u001b[0;36mpatched_step\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0maccelerated_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accelerate_step_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpatched_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opt_called\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[union-attr]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                 \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrapped_by_lr_sched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"differentiable\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    235\u001b[0m             \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"betas\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             has_complex = self._init_group(\n\u001b[0m\u001b[1;32m    238\u001b[0m                 \u001b[0mgroup\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36m_init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    179\u001b[0m                     )\n\u001b[1;32m    180\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                     state[\"exp_avg_sq\"] = torch.zeros_like(\n\u001b[0m\u001b[1;32m    182\u001b[0m                         \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     )\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 12.12 MiB is free. Process 4103 has 14.73 GiB memory in use. Of the allocated memory 14.52 GiB is allocated by PyTorch, and 71.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# é…ç½®è®­ç»ƒ\n",
        "print(\"âš™ï¸ Configuring training...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    eval_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,  # Enable fp16 compute\n",
        "    bf16=False,\n",
        "    optim=\"adamw_torch\",  # Standard optimizer for float16\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        "    ddp_find_unused_parameters=False,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=False,\n",
        "    gradient_checkpointing=True,  # Ensure Trainer-aware checkpointing\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ready!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ Starting training...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# è®­ç»ƒå‰æ¸…ç†æ˜¾å­˜ï¼Œå‡å°‘ç¢ç‰‡\n",
        "import gc\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ Training completed!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1492a771",
      "metadata": {
        "id": "1492a771"
      },
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¨¡å‹\n",
        "print(\"ğŸ’¾ Saving model...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"âœ… Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# ä¿å­˜è®­ç»ƒä¿¡æ¯\n",
        "import json\n",
        "training_info = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"lora_r\": LORA_R,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"train_samples\": len(train_dataset),\n",
        "    \"val_samples\": len(eval_dataset),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/training_info.json\", 'w') as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(\"\\nğŸ“Š Training Summary:\")\n",
        "for key, value in training_info.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe3514",
      "metadata": {
        "id": "66fe3514"
      },
      "source": [
        "## ğŸ§ª æ­¥éª¤6ï¼šæµ‹è¯•æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7233b4",
      "metadata": {
        "id": "ba7233b4"
      },
      "outputs": [],
      "source": [
        "# å¿«é€Ÿæµ‹è¯• (CodeLlama)\n",
        "print(\"ğŸ§ª Testing CodeLlama model inference...\")\n",
        "\n",
        "test_instruction = \"Create a new MS cable object at coordinates (186355533, 439556907)\"\n",
        "test_context = \"Application: PowerGrid | Database: ND | Steps: 5\"\n",
        "\n",
        "prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {test_instruction}\n",
        "Context: {test_context}\n",
        "\n",
        "JSON Code:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"\\nğŸ”® Generating...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response = response.split(\"JSON Code:\")[-1].strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“ Test Result:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Instruction: {test_instruction}\")\n",
        "print(f\"Context: {test_context}\")\n",
        "print(f\"\\nGenerated Output:\\n{response[:500]}...\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91162b41",
      "metadata": {
        "id": "91162b41"
      },
      "source": [
        "## ğŸ“¦ æ­¥éª¤7ï¼šä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8ae61a",
      "metadata": {
        "id": "9f8ae61a"
      },
      "outputs": [],
      "source": [
        "# æ‰“åŒ…æ¨¡å‹ç”¨äºä¸‹è½½\n",
        "!cd /content/drive/MyDrive/gis-models && zip -r codellama-gis-lora.zip codellama-gis-lora/\n",
        "print(\"âœ… Model zipped!\")\n",
        "print(f\"ğŸ“¦ Location: /content/drive/MyDrive/gis-models/codellama-gis-lora.zip\")\n",
        "print(\"ğŸ’¡ You can download it from Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7dcf9b",
      "metadata": {
        "id": "bc7dcf9b"
      },
      "source": [
        "## ğŸ¯ ä¸‹ä¸€æ­¥\n",
        "\n",
        "è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ï¼š\n",
        "\n",
        "1. **åœ¨Colabä¸­ç»§ç»­æµ‹è¯•**ï¼šä½¿ç”¨ä¸Šé¢çš„æµ‹è¯•å•å…ƒæ ¼\n",
        "2. **ä¸‹è½½æ¨¡å‹**ï¼šä»Google Driveä¸‹è½½æ‰“åŒ…çš„æ¨¡å‹\n",
        "3. **æœ¬åœ°éƒ¨ç½²**ï¼šå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°è¿›è¡Œæ¨ç†\n",
        "4. **è¯„ä¼°æ¨¡å‹**ï¼šè¿è¡Œå®Œæ•´çš„è¯„ä¼°è„šæœ¬\n",
        "\n",
        "### æœ¬åœ°ä½¿ç”¨æ¨¡å‹\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# åŠ è½½CodeLlamaåŸºåº§æ¨¡å‹\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
        "model = PeftModel.from_pretrained(base_model, \"path/to/codellama-gis-lora\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"path/to/codellama-gis-lora\")\n",
        "\n",
        "# æ¨ç†\n",
        "# ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**è®­ç»ƒå‚æ•°è°ƒä¼˜å»ºè®®ï¼š**\n",
        "- T4 GPU: batch_size=4, gradient_accumulation=4\n",
        "- A100 GPU: batch_size=8-12, gradient_accumulation=2-4\n",
        "- å¦‚æœæ˜¾å­˜ä¸è¶³ï¼šå‡å°batch_sizeæˆ–max_length\n",
        "- è®­ç»ƒæ—¶é—´ä¼°è®¡ï¼šT4çº¦4-6å°æ—¶ï¼ŒA100çº¦1-2å°æ—¶\n",
        "\n",
        "**ğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†GISä»£ç ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒï¼ˆCodeLlamaç‰ˆï¼‰ï¼**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "eccd0ef0b0554a25b714d6b4b644fa88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f9b436ef6c049a799f0d33f2673f5a7",
              "IPY_MODEL_e03dadf8ab4440828dfef1de3ead8212",
              "IPY_MODEL_fd46f5c1c3f2485196c00822c81ea5d9"
            ],
            "layout": "IPY_MODEL_37b085c719e44d72bb42c734b72d8ce5"
          }
        },
        "1f9b436ef6c049a799f0d33f2673f5a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca6594a99db249cb9d48e075d00f5296",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_12fcebff7ee4475592d2d4381157c11b",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "e03dadf8ab4440828dfef1de3ead8212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f58bb16455274768aa4d37022235a663",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e8e990cf4d514de1964ec60eb01a0f5f",
            "value": 2
          }
        },
        "fd46f5c1c3f2485196c00822c81ea5d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1942abeeaef34692b453725876588c59",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_20d5495a979148fe96161b6239e89573",
            "value": "â€‡2/2â€‡[00:53&lt;00:00,â€‡53.84s/it]"
          }
        },
        "37b085c719e44d72bb42c734b72d8ce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca6594a99db249cb9d48e075d00f5296": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12fcebff7ee4475592d2d4381157c11b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f58bb16455274768aa4d37022235a663": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e990cf4d514de1964ec60eb01a0f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1942abeeaef34692b453725876588c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20d5495a979148fe96161b6239e89573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}