{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33fe0c37",
   "metadata": {},
   "source": [
    "# ğŸš€ GISä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒ - Google Colab (CodeLlama)\n",
    "\n",
    "æœ¬Notebookåœ¨Google Colabä¸Šè®­ç»ƒGISä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆ**æ–‡ä»¶çº§ + CodeLlama**ï¼‰\n",
    "\n",
    "**æ–‡ä»¶çº§è®­ç»ƒ** - æ¨¡å‹å­¦ä¹ ç”Ÿæˆå®Œæ•´çš„å·¥ä½œæµè€Œä¸æ˜¯å•ä¸ªæ­¥éª¤\n",
    "- è¾“å…¥ï¼šç”¨æˆ·çš„é«˜å±‚æŒ‡ä»¤ï¼ˆè‹±è¯­/è·å…°è¯­ï¼Œå¦‚ï¼š\"Create MS and HS cable objects\"ï¼‰\n",
    "- è¾“å‡ºï¼šå®Œæ•´çš„å·¥ä½œæµJSONä»£ç ï¼ˆåŒ…å«æ‰€æœ‰æ“ä½œæ­¥éª¤ï¼‰\n",
    "- ä¼˜åŠ¿ï¼šä¸€æ¬¡æ¨ç†ç”Ÿæˆæ•´ä¸ªæµ‹è¯•è„šæœ¬\n",
    "- **æ¨¡å‹**ï¼šCodeLlama-7B-Instructï¼ˆä¸“ä¸ºä»£ç ç”Ÿæˆä¼˜åŒ–ï¼‰\n",
    "\n",
    "**ä½¿ç”¨å‰å‡†å¤‡ï¼š**\n",
    "1. è¿è¡Œç¯å¢ƒï¼š`Runtime > Change runtime type > T4 GPU`ï¼ˆå…è´¹ï¼‰æˆ– `A100 GPU`ï¼ˆColab Proï¼‰\n",
    "2. æ•°æ®å‡†å¤‡ï¼šç¡®ä¿å·²ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶\n",
    "3. é¢„è®¡æ—¶é—´ï¼š4-6å°æ—¶ï¼ˆT4ï¼‰/ 1-2å°æ—¶ï¼ˆA100ï¼‰\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ae856",
   "metadata": {},
   "source": [
    "## ğŸ“‹ æ­¥éª¤1ï¼šç¯å¢ƒè®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e1522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ£€æŸ¥GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d621428",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–ï¼ˆçº¦3-5åˆ†é’Ÿï¼‰\n",
    "print(\"ğŸ“¦ Installing dependencies...\")\n",
    "\n",
    "# å®‰è£…ä¸»è¦è®­ç»ƒåº“ï¼ˆä½¿ç”¨å…¼å®¹çš„ç‰ˆæœ¬ï¼‰\n",
    "!pip install -q transformers==4.45.0\n",
    "!pip install -q peft==0.13.0\n",
    "!pip install -q datasets==2.19.0\n",
    "!pip install -q accelerate==0.30.0\n",
    "!pip install -q sentencepiece==0.2.0\n",
    "!pip install -q tqdm\n",
    "\n",
    "print(\"âœ… Core dependencies installed!\")\n",
    "\n",
    "# ä¿®å¤bitsandbytes CUDAé—®é¢˜ï¼ˆColabç‰¹å®šï¼‰\n",
    "print(\"\\nğŸ”§ Setting up bitsandbytes for Colab...\")\n",
    "!pip install -q --upgrade bitsandbytes==0.43.0\n",
    "\n",
    "# è®¾ç½®CUDAç¯å¢ƒå˜é‡ä»¥ä¿®å¤CUDAæ£€æµ‹\n",
    "import os\n",
    "os.environ['CUDA_HOME'] = '/usr/local/cuda'\n",
    "os.environ['LD_LIBRARY_PATH'] = '/usr/local/cuda/lib64:$LD_LIBRARY_PATH'\n",
    "\n",
    "# é‡æ–°å®‰è£…bitsandbytesä»¥åº”ç”¨ç¯å¢ƒå˜é‡\n",
    "!pip install -q --force-reinstall bitsandbytes==0.43.0\n",
    "\n",
    "print(\"âœ… Bitsandbytes configured!\")\n",
    "\n",
    "# ä¿®å¤å…¶ä»–ç‰ˆæœ¬å†²çª\n",
    "print(\"\\nğŸ”§ Fixing dependency conflicts...\")\n",
    "!pip install -q --force-reinstall fsspec==2024.2.0\n",
    "!pip install -q --force-reinstall huggingface-hub==0.23.0\n",
    "!pip install -q --force-reinstall numpy==2.1.3\n",
    "\n",
    "print(\"âœ… All dependencies installed and configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62236c8b",
   "metadata": {},
   "source": [
    "## ğŸ’¾ æ­¥éª¤2ï¼šæŒ‚è½½Google Driveï¼ˆä¿å­˜æ¨¡å‹ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b086f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
    "!mkdir -p /content/drive/MyDrive/gis-models\n",
    "print(\"âœ… Google Drive mounted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752feee6",
   "metadata": {},
   "source": [
    "## ğŸ“‚ æ­¥éª¤3ï¼šä¸Šä¼ æ•°æ®\n",
    "\n",
    "**ä¸¤ç§æ–¹å¼ä»»é€‰å…¶ä¸€ï¼š**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5240de6f",
   "metadata": {},
   "source": [
    "### æ–¹å¼Aï¼šä»GitHubå…‹éš†ä»“åº“ï¼ˆæ¨èï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6558e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å…‹éš†ä»“åº“\n",
    "!git clone https://github.com/YOUR_USERNAME/gis-code-ai.git\n",
    "%cd gis-code-ai\n",
    "\n",
    "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
    "import os\n",
    "required_files = [\n",
    "    'data/processed/step_level_instructions_weighted_variants_marked.jsonl',\n",
    "    'data/processed/parsed_workflows.jsonl'\n",
    "]\n",
    "\n",
    "for f in required_files:\n",
    "    if os.path.exists(f):\n",
    "        print(f\"âœ… {f}\")\n",
    "    else:\n",
    "        print(f\"âŒ {f} - æ–‡ä»¶ä¸å­˜åœ¨ï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90a1f39",
   "metadata": {},
   "source": [
    "### æ–¹å¼Bï¼šä»Google Driveä¸Šä¼ ï¼ˆå¦‚æœæ•°æ®å·²åœ¨Driveï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95292343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¦‚æœæ•°æ®åœ¨Google Driveä¸­\n",
    "# !mkdir -p data/processed\n",
    "# !cp /content/drive/MyDrive/gis-data/step_level_instructions_weighted_variants_marked.jsonl data/processed/\n",
    "# !cp /content/drive/MyDrive/gis-data/parsed_workflows.jsonl data/processed/\n",
    "# print(\"âœ… Data copied from Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fffddf",
   "metadata": {},
   "source": [
    "### æ–¹å¼Cï¼šæ‰‹åŠ¨ä¸Šä¼ æ–‡ä»¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9db50be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰‹åŠ¨ä¸Šä¼ ï¼ˆç‚¹å‡»å·¦ä¾§æ–‡ä»¶å›¾æ ‡ï¼Œä¸Šä¼ æ–‡ä»¶ï¼‰\n",
    "# from google.colab import files\n",
    "# !mkdir -p data/processed\n",
    "# uploaded = files.upload()  # ä¸Šä¼  step_level_instructions å’Œ parsed_workflows\n",
    "# print(\"âœ… Files uploaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f32dcb",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303fdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile prepare_data.py\n",
    "# æ–‡ä»¶çº§æ•°æ®å‡†å¤‡è„šæœ¬\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def prepare_file_level_training_data(instructions_file, workflows_file, output_dir, max_samples=None, split_ratio=0.9):\n",
    "    \"\"\"å‡†å¤‡æ–‡ä»¶çº§è®­ç»ƒæ•°æ®\"\"\"\n",
    "    \n",
    "    print(f\"ğŸ“– Loading file-level instructions from {instructions_file}\")\n",
    "    instructions = {}\n",
    "    with open(instructions_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                item = json.loads(line)\n",
    "                file_id = item.get('file_id', '')\n",
    "                instructions[file_id] = item\n",
    "    print(f\"âœ… Loaded {len(instructions)} file-level instructions\")\n",
    "    \n",
    "    print(f\"ğŸ“– Loading workflows from {workflows_file}\")\n",
    "    workflows = {}\n",
    "    with open(workflows_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                wf = json.loads(line)\n",
    "                workflows[wf.get('file_id', '')] = wf\n",
    "    print(f\"âœ… Loaded {len(workflows)} workflows\")\n",
    "    \n",
    "    print(\"ğŸ”„ Converting to training format (file-level)...\")\n",
    "    training_samples = []\n",
    "    \n",
    "    count = 0\n",
    "    for file_id, instr in tqdm(instructions.items()):\n",
    "        workflow = workflows.get(file_id, {})\n",
    "        \n",
    "        if not workflow:\n",
    "            continue\n",
    "        \n",
    "        # æ¸…ç†æŒ‡ä»¤\n",
    "        instruction_text = instr.get('instruction', '').replace('**', '').replace('*', '')\n",
    "        instruction_text = ' '.join(instruction_text.split()).strip()\n",
    "        \n",
    "        # æ„å»ºä¸Šä¸‹æ–‡\n",
    "        test_app = workflow.get('test_app', '')\n",
    "        database = workflow.get('database', '')\n",
    "        total_steps = workflow.get('total_steps', 0)\n",
    "        \n",
    "        context_parts = []\n",
    "        if test_app:\n",
    "            context_parts.append(f\"Application: {test_app}\")\n",
    "        if database:\n",
    "            context_parts.append(f\"Database: {database}\")\n",
    "        if total_steps > 0:\n",
    "            context_parts.append(f\"Steps: {total_steps}\")\n",
    "        input_context = \" | \".join(context_parts)\n",
    "        \n",
    "        # æå–å®Œæ•´å·¥ä½œæµJSONï¼ˆæ‰€æœ‰stepsï¼‰\n",
    "        steps = workflow.get('steps', [])\n",
    "        if not steps:\n",
    "            continue\n",
    "        \n",
    "        workflow_output = {\n",
    "            \"workflow\": {\n",
    "                \"metadata\": {\n",
    "                    \"test_app\": test_app,\n",
    "                    \"database\": database,\n",
    "                    \"total_steps\": len(steps)\n",
    "                },\n",
    "                \"steps\": steps\n",
    "            }\n",
    "        }\n",
    "        output_code = json.dumps(workflow_output, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        # è´¨é‡è¿‡æ»¤\n",
    "        if len(instruction_text.split()) < 5 or output_code == '{}':\n",
    "            continue\n",
    "        \n",
    "        training_samples.append({\n",
    "            \"instruction\": instruction_text,\n",
    "            \"input\": input_context,\n",
    "            \"output\": output_code\n",
    "        })\n",
    "        \n",
    "        count += 1\n",
    "        if max_samples and count >= max_samples:\n",
    "            break\n",
    "    \n",
    "    print(f\"âœ… Created {len(training_samples)} training samples\")\n",
    "    \n",
    "    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "    split_idx = int(len(training_samples) * split_ratio)\n",
    "    train_data = training_samples[:split_idx]\n",
    "    val_data = training_samples[split_idx:]\n",
    "    \n",
    "    print(f\"ğŸ“Š Split: {len(train_data)} train, {len(val_data)} validation\")\n",
    "    \n",
    "    # ä¿å­˜\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    train_file = output_dir / \"training_data_train.json\"\n",
    "    val_file = output_dir / \"training_data_val.json\"\n",
    "    \n",
    "    with open(train_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ğŸ’¾ Train data saved: {train_file}\")\n",
    "    \n",
    "    with open(val_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"ğŸ’¾ Validation data saved: {val_file}\")\n",
    "    \n",
    "    return train_data, val_data\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    max_samples = int(sys.argv[1]) if len(sys.argv) > 1 else None\n",
    "    \n",
    "    train_data, val_data = prepare_file_level_training_data(\n",
    "        instructions_file='data/processed/file_level_instructions_weighted_variants_marked.jsonl',\n",
    "        workflows_file='data/processed/parsed_workflows.jsonl',\n",
    "        output_dir='data/training',\n",
    "        max_samples=max_samples,\n",
    "        split_ratio=0.9\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æ•°æ®ç²’åº¦: æ–‡ä»¶çº§ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰\")\n",
    "    print(f\"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_data):,}\")\n",
    "    print(f\"ğŸ“Š éªŒè¯æ ·æœ¬: {len(val_data):,}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # æ˜¾ç¤ºç¤ºä¾‹\n",
    "    if train_data:\n",
    "        print(\"\\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\")\n",
    "        sample = train_data[0]\n",
    "        print(f\"Instruction: {sample['instruction'][:100]}...\")\n",
    "        print(f\"Input: {sample['input']}\")\n",
    "        print(f\"Output: {sample['output'][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1b1c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿è¡Œæ•°æ®å‡†å¤‡ï¼ˆæ–‡ä»¶çº§ï¼‰\n",
    "# ä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
    "!python prepare_data.py\n",
    "\n",
    "# æˆ–è€…å¿«é€Ÿæµ‹è¯•ï¼ˆåªç”¨500ä¸ªæ–‡ä»¶ï¼‰\n",
    "# !python prepare_data.py 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e032229f",
   "metadata": {},
   "source": [
    "## ğŸš€ æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a171cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"ğŸ”§ Setting up training with CodeLlama...\")\n",
    "\n",
    "# ============================================================\n",
    "# é…ç½®å‚æ•° - CodeLlama (è‹±è¯­/è·å…°è¯­ä»£ç ç”Ÿæˆä¼˜åŒ–)\n",
    "# ============================================================\n",
    "\n",
    "MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/gis-models/codellama-gis-lora\"  # ä¿å­˜åˆ°Google Drive\n",
    "TRAIN_FILE = \"data/training/training_data_train.json\"\n",
    "VAL_FILE = \"data/training/training_data_val.json\"\n",
    "\n",
    "# è®­ç»ƒå‚æ•°ï¼ˆCodeLlamaä¼˜åŒ–é…ç½®ï¼‰\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 4  # T4: 4, A100: 8-12\n",
    "GRADIENT_ACCUMULATION = 4  # æœ‰æ•ˆbatch = 16\n",
    "LEARNING_RATE = 2e-4  # CodeLlamaæ¨èå­¦ä¹ ç‡\n",
    "MAX_LENGTH = 2048  # JSONä»£ç æœ€å¤§é•¿åº¦\n",
    "\n",
    "# LoRAå‚æ•°ï¼ˆä»£ç ç”Ÿæˆä»»åŠ¡ä¼˜åŒ–ï¼‰\n",
    "LORA_R = 64  # è¾ƒå¤§çš„rå€¼é€‚åˆå¤æ‚ä»£ç ç”Ÿæˆ\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "print(f\"ğŸ“¦ Model: {MODEL_NAME}\")\n",
    "print(f\"ğŸ’¾ Output: {OUTPUT_DIR}\")\n",
    "print(f\"ğŸ“Š Data: FILE-LEVEL (complete workflows)\")\n",
    "print(f\"ğŸ“ˆ Epochs: {NUM_EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
    "print(f\"ğŸ¯ Optimized for: English/Dutch â†’ JSON code generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b429b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½tokenizer (CodeLlama)\n",
    "print(\"ğŸ“– Loading CodeLlama tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"âœ… Tokenizer loaded: vocab_size={len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4deb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½æ¨¡å‹ï¼ˆ4-bité‡åŒ–ï¼‰\n",
    "print(\"ğŸ¤– Loading CodeLlama-7B with 4-bit quantization...\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "print(\"âœ… CodeLlama base model loaded\")\n",
    "\n",
    "# å‡†å¤‡æ¨¡å‹\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# åº”ç”¨LoRA\n",
    "print(\"ğŸ”§ Applying LoRA...\")\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    bias=\"none\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"âœ… LoRA applied!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6aeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡æ•°æ®é›†\n",
    "print(\"ğŸ“Š Preparing datasets...\")\n",
    "\n",
    "train_data = load_dataset('json', data_files=TRAIN_FILE, split='train')\n",
    "eval_data = load_dataset('json', data_files=VAL_FILE, split='train')\n",
    "\n",
    "print(f\"  Train: {len(train_data)} samples\")\n",
    "print(f\"  Val: {len(eval_data)} samples\")\n",
    "\n",
    "# æ ¼å¼åŒ–prompt (CodeLlamaä¼˜åŒ–æ ¼å¼)\n",
    "def format_prompt(example):\n",
    "    instruction = example['instruction']\n",
    "    input_text = example.get('input', '')\n",
    "    output = example['output']\n",
    "    \n",
    "    # CodeLlamaæ›´é€‚åˆç›´æ¥çš„ä»£ç ç”Ÿæˆæ ¼å¼\n",
    "    if input_text:\n",
    "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
    "\n",
    "Instruction: {instruction}\n",
    "Context: {input_text}\n",
    "\n",
    "JSON Code:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
    "\n",
    "Instruction: {instruction}\n",
    "\n",
    "JSON Code:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "train_data = train_data.map(format_prompt, remove_columns=train_data.column_names)\n",
    "eval_data = eval_data.map(format_prompt, remove_columns=eval_data.column_names)\n",
    "\n",
    "# Tokenize\n",
    "def tokenize_function(examples):\n",
    "    tokenized = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=False,\n",
    "        return_tensors=None\n",
    "    )\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "print(\"ğŸ”„ Tokenizing...\")\n",
    "train_dataset = train_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "eval_dataset = eval_data.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_data.column_names,\n",
    "    desc=\"Tokenizing val\"\n",
    ")\n",
    "\n",
    "print(\"âœ… Datasets ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246a2b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# é…ç½®è®­ç»ƒ\n",
    "print(\"âš™ï¸ Configuring training...\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=100,\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"âœ… Trainer ready!\")\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ‰ Training completed!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1492a771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¿å­˜æ¨¡å‹\n",
    "print(\"ğŸ’¾ Saving model...\")\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"âœ… Model saved to {OUTPUT_DIR}\")\n",
    "\n",
    "# ä¿å­˜è®­ç»ƒä¿¡æ¯\n",
    "import json\n",
    "training_info = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"num_epochs\": NUM_EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_ALPHA,\n",
    "    \"train_samples\": len(train_dataset),\n",
    "    \"val_samples\": len(eval_dataset),\n",
    "}\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"\\nğŸ“Š Training Summary:\")\n",
    "for key, value in training_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fe3514",
   "metadata": {},
   "source": [
    "## ğŸ§ª æ­¥éª¤6ï¼šæµ‹è¯•æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7233b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¿«é€Ÿæµ‹è¯• (CodeLlama)\n",
    "print(\"ğŸ§ª Testing CodeLlama model inference...\")\n",
    "\n",
    "test_instruction = \"Create a new MS cable object at coordinates (186355533, 439556907)\"\n",
    "test_context = \"Application: PowerGrid | Database: ND | Steps: 5\"\n",
    "\n",
    "prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
    "\n",
    "Instruction: {test_instruction}\n",
    "Context: {test_context}\n",
    "\n",
    "JSON Code:\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "print(\"\\nğŸ”® Generating...\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "response = response.split(\"JSON Code:\")[-1].strip()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ Test Result:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Instruction: {test_instruction}\")\n",
    "print(f\"Context: {test_context}\")\n",
    "print(f\"\\nGenerated Output:\\n{response[:500]}...\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91162b41",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ­¥éª¤7ï¼šä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8ae61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰“åŒ…æ¨¡å‹ç”¨äºä¸‹è½½\n",
    "!cd /content/drive/MyDrive/gis-models && zip -r codellama-gis-lora.zip codellama-gis-lora/\n",
    "print(\"âœ… Model zipped!\")\n",
    "print(f\"ğŸ“¦ Location: /content/drive/MyDrive/gis-models/codellama-gis-lora.zip\")\n",
    "print(\"ğŸ’¡ You can download it from Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7dcf9b",
   "metadata": {},
   "source": [
    "## ğŸ¯ ä¸‹ä¸€æ­¥\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "1. **åœ¨Colabä¸­ç»§ç»­æµ‹è¯•**ï¼šä½¿ç”¨ä¸Šé¢çš„æµ‹è¯•å•å…ƒæ ¼\n",
    "2. **ä¸‹è½½æ¨¡å‹**ï¼šä»Google Driveä¸‹è½½æ‰“åŒ…çš„æ¨¡å‹\n",
    "3. **æœ¬åœ°éƒ¨ç½²**ï¼šå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°è¿›è¡Œæ¨ç†\n",
    "4. **è¯„ä¼°æ¨¡å‹**ï¼šè¿è¡Œå®Œæ•´çš„è¯„ä¼°è„šæœ¬\n",
    "\n",
    "### æœ¬åœ°ä½¿ç”¨æ¨¡å‹\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# åŠ è½½CodeLlamaåŸºåº§æ¨¡å‹\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
    "model = PeftModel.from_pretrained(base_model, \"path/to/codellama-gis-lora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/codellama-gis-lora\")\n",
    "\n",
    "# æ¨ç†\n",
    "# ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**è®­ç»ƒå‚æ•°è°ƒä¼˜å»ºè®®ï¼š**\n",
    "- T4 GPU: batch_size=4, gradient_accumulation=4\n",
    "- A100 GPU: batch_size=8-12, gradient_accumulation=2-4\n",
    "- å¦‚æœæ˜¾å­˜ä¸è¶³ï¼šå‡å°batch_sizeæˆ–max_length\n",
    "- è®­ç»ƒæ—¶é—´ä¼°è®¡ï¼šT4çº¦4-6å°æ—¶ï¼ŒA100çº¦1-2å°æ—¶\n",
    "\n",
    "**ğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†GISä»£ç ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒï¼ˆCodeLlamaç‰ˆï¼‰ï¼**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
