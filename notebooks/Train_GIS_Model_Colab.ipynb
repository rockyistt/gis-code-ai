{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rockyistt/gis-code-ai/blob/main/notebooks/Train_GIS_Model_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33fe0c37",
      "metadata": {
        "id": "33fe0c37"
      },
      "source": [
        "# ğŸš€ GISä»£ç ç”Ÿæˆæ¨¡å‹è®­ç»ƒ - Google Colab (CodeLlama)\n",
        "\n",
        "æœ¬Notebookåœ¨Google Colabä¸Šè®­ç»ƒGISä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆ**æ–‡ä»¶çº§ + CodeLlama**ï¼‰\n",
        "\n",
        "**æ–‡ä»¶çº§è®­ç»ƒ** - æ¨¡å‹å­¦ä¹ ç”Ÿæˆå®Œæ•´çš„å·¥ä½œæµè€Œä¸æ˜¯å•ä¸ªæ­¥éª¤\n",
        "- è¾“å…¥ï¼šç”¨æˆ·çš„é«˜å±‚æŒ‡ä»¤ï¼ˆè‹±è¯­/è·å…°è¯­ï¼Œå¦‚ï¼š\"Create MS and HS cable objects\"ï¼‰\n",
        "- è¾“å‡ºï¼šå®Œæ•´çš„å·¥ä½œæµJSONä»£ç ï¼ˆåŒ…å«æ‰€æœ‰æ“ä½œæ­¥éª¤ï¼‰\n",
        "- ä¼˜åŠ¿ï¼šä¸€æ¬¡æ¨ç†ç”Ÿæˆæ•´ä¸ªæµ‹è¯•è„šæœ¬\n",
        "- **æ¨¡å‹**ï¼šCodeLlama-7B-Instructï¼ˆä¸“ä¸ºä»£ç ç”Ÿæˆä¼˜åŒ–ï¼‰\n",
        "\n",
        "**ä½¿ç”¨å‰å‡†å¤‡ï¼š**\n",
        "1. è¿è¡Œç¯å¢ƒï¼š`Runtime > Change runtime type > T4 GPU`ï¼ˆå…è´¹ï¼‰æˆ– `A100 GPU`ï¼ˆColab Proï¼‰\n",
        "2. æ•°æ®å‡†å¤‡ï¼šç¡®ä¿å·²ç”Ÿæˆè®­ç»ƒæ•°æ®æ–‡ä»¶\n",
        "3. é¢„è®¡æ—¶é—´ï¼š4-6å°æ—¶ï¼ˆT4ï¼‰/ 1-2å°æ—¶ï¼ˆA100ï¼‰\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "130ae856",
      "metadata": {
        "id": "130ae856"
      },
      "source": [
        "## ğŸ“‹ æ­¥éª¤1ï¼šç¯å¢ƒè®¾ç½®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "859e1522",
      "metadata": {
        "id": "859e1522",
        "outputId": "6f62b521-0be1-4271-8935-6b4b96d59f0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jan 14 09:31:47 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   66C    P8             12W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# æ£€æŸ¥GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8d621428",
      "metadata": {
        "id": "8d621428",
        "outputId": "aee8e3dc-be4d-4aeb-84ac-2ea7872fea61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“¦ Installing dependencies...\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.5/322.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m172.0/172.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hâœ… Core dependencies installed!\n",
            "\n",
            "ğŸ”§ Setting up bitsandbytes for Colab...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m881.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.5/170.5 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "datasets 2.19.0 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… Bitsandbytes configured!\n",
            "\n",
            "ğŸ”§ Fixing dependency conflicts...\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.2.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m401.2/401.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.45.0 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "datasets 2.19.0 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "diffusers 0.36.0 requires huggingface-hub<2.0,>=0.34.0, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.1 which is incompatible.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "gradio 5.50.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.45.0 requires huggingface-hub<1.0,>=0.23.2, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "datasets 2.19.0 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.3 which is incompatible.\n",
            "diffusers 0.36.0 requires huggingface-hub<2.0,>=0.34.0, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "gradio 5.50.0 requires huggingface-hub<2.0,>=0.33.5, but you have huggingface-hub 0.23.0 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.9.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mâœ… All dependencies installed and configured!\n"
          ]
        }
      ],
      "source": [
        "# å®‰è£…ä¾èµ–ï¼ˆçº¦3-5åˆ†é’Ÿï¼‰\n",
        "print(\"ğŸ“¦ Installing dependencies...\")\n",
        "\n",
        "# å®‰è£…ä¸»è¦è®­ç»ƒåº“ï¼ˆä½¿ç”¨å…¼å®¹çš„ç‰ˆæœ¬ï¼‰\n",
        "!pip install transformers -U\n",
        "!pip install -q peft==0.13.0\n",
        "!pip install -q datasets==2.19.0\n",
        "!pip install -q accelerate==0.30.0\n",
        "!pip install -q sentencepiece==0.2.0\n",
        "!pip install -q tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "id": "Tiqvq_w84TUR",
        "outputId": "870c34a6-37ca-4c20-dbb1-86606a9dcdd5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "id": "Tiqvq_w84TUR",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.45.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.5-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
            "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
            "  Downloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2026.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Downloading transformers-4.57.5-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.0\n",
            "    Uninstalling huggingface-hub-0.23.0:\n",
            "      Successfully uninstalled huggingface-hub-0.23.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.20.3\n",
            "    Uninstalling tokenizers-0.20.3:\n",
            "      Successfully uninstalled tokenizers-0.20.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.45.0\n",
            "    Uninstalling transformers-4.45.0:\n",
            "      Successfully uninstalled transformers-4.45.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 2.19.0 requires fsspec[http]<=2024.3.1,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.36.0 tokenizers-0.22.2 transformers-4.57.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "e0b0ff50e38a408084c835b3d190cbd0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62236c8b",
      "metadata": {
        "id": "62236c8b"
      },
      "source": [
        "## ğŸ’¾ æ­¥éª¤2ï¼šæŒ‚è½½Google Driveï¼ˆä¿å­˜æ¨¡å‹ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b086f8d",
      "metadata": {
        "id": "6b086f8d",
        "outputId": "144ac978-c457-4b73-9906-82b57e44df00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Google Drive mounted!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# åˆ›å»ºè¾“å‡ºç›®å½•\n",
        "!mkdir -p /content/drive/MyDrive/gis-models\n",
        "print(\"âœ… Google Drive mounted!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "752feee6",
      "metadata": {
        "id": "752feee6"
      },
      "source": [
        "## ğŸ“‚ æ­¥éª¤3ï¼šä¸Šä¼ æ•°æ®\n",
        "\n",
        "**ä¸¤ç§æ–¹å¼ä»»é€‰å…¶ä¸€ï¼š**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5240de6f",
      "metadata": {
        "id": "5240de6f"
      },
      "source": [
        "### æ–¹å¼Aï¼šä»GitHubå…‹éš†ä»“åº“ï¼ˆæ¨èï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7ff6558e",
      "metadata": {
        "id": "7ff6558e",
        "outputId": "7ccccdab-6928-418c-80ac-3a298d9098e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gis-code-ai'...\n",
            "remote: Enumerating objects: 151, done.\u001b[K\n",
            "remote: Counting objects: 100% (151/151), done.\u001b[K\n",
            "remote: Compressing objects: 100% (119/119), done.\u001b[K\n",
            "remote: Total 151 (delta 23), reused 133 (delta 13), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (151/151), 1.23 MiB | 2.48 MiB/s, done.\n",
            "Resolving deltas: 100% (23/23), done.\n",
            "/content/gis-code-ai\n",
            "âœ… data/processed/step_level_instructions_weighted_variants_marked.jsonl\n",
            "âœ… data/processed/parsed_workflows.jsonl\n"
          ]
        }
      ],
      "source": [
        "# å…‹éš†ä»“åº“\n",
        "!git clone https://github.com/rockyistt/gis-code-ai.git\n",
        "%cd gis-code-ai\n",
        "\n",
        "# æ£€æŸ¥æ•°æ®æ–‡ä»¶\n",
        "import os\n",
        "required_files = [\n",
        "    'data/processed/step_level_instructions_weighted_variants_marked.jsonl',\n",
        "    'data/processed/parsed_workflows.jsonl'\n",
        "]\n",
        "\n",
        "for f in required_files:\n",
        "    if os.path.exists(f):\n",
        "        print(f\"âœ… {f}\")\n",
        "    else:\n",
        "        print(f\"âŒ {f} - æ–‡ä»¶ä¸å­˜åœ¨ï¼\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f32dcb",
      "metadata": {
        "id": "23f32dcb"
      },
      "source": [
        "## ğŸ”„ æ­¥éª¤4ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a303fdcd",
      "metadata": {
        "id": "a303fdcd",
        "outputId": "13d9cf72-7345-4de0-94cf-df3f6465ea27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prepare_data.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile prepare_data.py\n",
        "# æ–‡ä»¶çº§æ•°æ®å‡†å¤‡è„šæœ¬\n",
        "import json\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "def prepare_file_level_training_data(instructions_file, workflows_file, output_dir, max_samples=None, split_ratio=0.9):\n",
        "    \"\"\"å‡†å¤‡æ–‡ä»¶çº§è®­ç»ƒæ•°æ®\"\"\"\n",
        "\n",
        "    print(f\"ğŸ“– Loading file-level instructions from {instructions_file}\")\n",
        "    instructions = {}\n",
        "    with open(instructions_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                item = json.loads(line)\n",
        "                file_id = item.get('file_id', '')\n",
        "                instructions[file_id] = item\n",
        "    print(f\"âœ… Loaded {len(instructions)} file-level instructions\")\n",
        "\n",
        "    print(f\"ğŸ“– Loading workflows from {workflows_file}\")\n",
        "    workflows = {}\n",
        "    with open(workflows_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                wf = json.loads(line)\n",
        "                workflows[wf.get('file_id', '')] = wf\n",
        "    print(f\"âœ… Loaded {len(workflows)} workflows\")\n",
        "\n",
        "    print(\"ğŸ”„ Converting to training format (file-level)...\")\n",
        "    training_samples = []\n",
        "\n",
        "    count = 0\n",
        "    for file_id, instr in tqdm(instructions.items()):\n",
        "        workflow = workflows.get(file_id, {})\n",
        "\n",
        "        if not workflow:\n",
        "            continue\n",
        "\n",
        "        # æ¸…ç†æŒ‡ä»¤\n",
        "        instruction_text = instr.get('instruction', '').replace('**', '').replace('*', '')\n",
        "        instruction_text = ' '.join(instruction_text.split()).strip()\n",
        "\n",
        "        # æ„å»ºä¸Šä¸‹æ–‡\n",
        "        test_app = workflow.get('test_app', '')\n",
        "        database = workflow.get('database', '')\n",
        "        total_steps = workflow.get('total_steps', 0)\n",
        "\n",
        "        context_parts = []\n",
        "        if test_app:\n",
        "            context_parts.append(f\"Application: {test_app}\")\n",
        "        if database:\n",
        "            context_parts.append(f\"Database: {database}\")\n",
        "        if total_steps > 0:\n",
        "            context_parts.append(f\"Steps: {total_steps}\")\n",
        "        input_context = \" | \".join(context_parts)\n",
        "\n",
        "        # æå–å®Œæ•´å·¥ä½œæµJSONï¼ˆæ‰€æœ‰stepsï¼‰\n",
        "        steps = workflow.get('steps', [])\n",
        "        if not steps:\n",
        "            continue\n",
        "\n",
        "        workflow_output = {\n",
        "            \"workflow\": {\n",
        "                \"metadata\": {\n",
        "                    \"test_app\": test_app,\n",
        "                    \"database\": database,\n",
        "                    \"total_steps\": len(steps)\n",
        "                },\n",
        "                \"steps\": steps\n",
        "            }\n",
        "        }\n",
        "        output_code = json.dumps(workflow_output, indent=2, ensure_ascii=False)\n",
        "\n",
        "        # è´¨é‡è¿‡æ»¤\n",
        "        if len(instruction_text.split()) < 5 or output_code == '{}':\n",
        "            continue\n",
        "\n",
        "        training_samples.append({\n",
        "            \"instruction\": instruction_text,\n",
        "            \"input\": input_context,\n",
        "            \"output\": output_code\n",
        "        })\n",
        "\n",
        "        count += 1\n",
        "        if max_samples and count >= max_samples:\n",
        "            break\n",
        "\n",
        "    print(f\"âœ… Created {len(training_samples)} training samples\")\n",
        "\n",
        "    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
        "    split_idx = int(len(training_samples) * split_ratio)\n",
        "    train_data = training_samples[:split_idx]\n",
        "    val_data = training_samples[split_idx:]\n",
        "\n",
        "    print(f\"ğŸ“Š Split: {len(train_data)} train, {len(val_data)} validation\")\n",
        "\n",
        "    # ä¿å­˜\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    train_file = output_dir / \"training_data_train.json\"\n",
        "    val_file = output_dir / \"training_data_val.json\"\n",
        "\n",
        "    with open(train_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(train_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"ğŸ’¾ Train data saved: {train_file}\")\n",
        "\n",
        "    with open(val_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(val_data, f, indent=2, ensure_ascii=False)\n",
        "    print(f\"ğŸ’¾ Validation data saved: {val_file}\")\n",
        "\n",
        "    return train_data, val_data\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import sys\n",
        "    max_samples = int(sys.argv[1]) if len(sys.argv) > 1 else None\n",
        "\n",
        "    train_data, val_data = prepare_file_level_training_data(\n",
        "        instructions_file='data/processed/file_level_instructions_weighted_variants_marked.jsonl',\n",
        "        workflows_file='data/processed/parsed_workflows.jsonl',\n",
        "        output_dir='data/training',\n",
        "        max_samples=max_samples,\n",
        "        split_ratio=0.9\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"ğŸ‰ æ•°æ®å‡†å¤‡å®Œæˆï¼\")\n",
        "    print(f\"ğŸ“Š æ•°æ®ç²’åº¦: æ–‡ä»¶çº§ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰\")\n",
        "    print(f\"ğŸ“Š è®­ç»ƒæ ·æœ¬: {len(train_data):,}\")\n",
        "    print(f\"ğŸ“Š éªŒè¯æ ·æœ¬: {len(val_data):,}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # æ˜¾ç¤ºç¤ºä¾‹\n",
        "    if train_data:\n",
        "        print(\"\\nğŸ“ è®­ç»ƒæ ·æœ¬ç¤ºä¾‹:\")\n",
        "        sample = train_data[0]\n",
        "        print(f\"Instruction: {sample['instruction'][:100]}...\")\n",
        "        print(f\"Input: {sample['input']}\")\n",
        "        print(f\"Output: {sample['output'][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1b1c77",
      "metadata": {
        "id": "3b1b1c77"
      },
      "outputs": [],
      "source": [
        "# è¿è¡Œæ•°æ®å‡†å¤‡ï¼ˆæ–‡ä»¶çº§ï¼‰\n",
        "# ä½¿ç”¨å…¨éƒ¨æ•°æ®\n",
        "!python prepare_data.py\n",
        "\n",
        "# æˆ–è€…å¿«é€Ÿæµ‹è¯•ï¼ˆåªç”¨500ä¸ªæ–‡ä»¶ï¼‰\n",
        "# !python prepare_data.py 500"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e032229f",
      "metadata": {
        "id": "e032229f"
      },
      "source": [
        "## ğŸš€ æ­¥éª¤5ï¼šå¼€å§‹è®­ç»ƒ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6a171cf7",
      "metadata": {
        "id": "6a171cf7",
        "outputId": "482f7cc6-19bf-4258-8ed1-27367d6740c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "huggingface-hub>=0.23.2,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.23.0.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-140906128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m from transformers import (\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mAutoModelForCausalLM\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Check the dependencies satisfy the minimal versions required.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_versions_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m from .utils import (\n\u001b[1;32m     28\u001b[0m     \u001b[0mOptionalDependencyNotAvailable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/dependency_versions_check.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mcontinue\u001b[0m  \u001b[0;31m# not required, check version only if installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mrequire_version_core\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpkg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"can't find {pkg} in {deps.keys()}, check dependency_versions_table.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version_core\u001b[0;34m(requirement)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;34m\"\"\"require_version wrapper which emits a core-specific hint on failure\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mhint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Try: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwanted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0m_compare_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     42\u001b[0m         )\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         )\n",
            "\u001b[0;31mImportError\u001b[0m: huggingface-hub>=0.23.2,<1.0 is required for a normal functioning of this module, but found huggingface-hub==0.23.0.\nTry: `pip install transformers -U` or `pip install -e '.[dev]'` if you're working with git main",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"ğŸ”§ Setting up training with CodeLlama...\")\n",
        "\n",
        "# ============================================================\n",
        "# é…ç½®å‚æ•° - CodeLlama (è‹±è¯­/è·å…°è¯­ä»£ç ç”Ÿæˆä¼˜åŒ–)\n",
        "# ============================================================\n",
        "\n",
        "MODEL_NAME = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/gis-models/codellama-gis-lora\"  # ä¿å­˜åˆ°Google Drive\n",
        "TRAIN_FILE = \"data/training/training_data_train.json\"\n",
        "VAL_FILE = \"data/training/training_data_val.json\"\n",
        "\n",
        "# è®­ç»ƒå‚æ•°ï¼ˆCodeLlamaä¼˜åŒ–é…ç½®ï¼‰\n",
        "NUM_EPOCHS = 3\n",
        "BATCH_SIZE = 4  # T4: 4, A100: 8-12\n",
        "GRADIENT_ACCUMULATION = 4  # æœ‰æ•ˆbatch = 16\n",
        "LEARNING_RATE = 2e-4  # CodeLlamaæ¨èå­¦ä¹ ç‡\n",
        "MAX_LENGTH = 2048  # JSONä»£ç æœ€å¤§é•¿åº¦\n",
        "\n",
        "# LoRAå‚æ•°ï¼ˆä»£ç ç”Ÿæˆä»»åŠ¡ä¼˜åŒ–ï¼‰\n",
        "LORA_R = 64  # è¾ƒå¤§çš„rå€¼é€‚åˆå¤æ‚ä»£ç ç”Ÿæˆ\n",
        "LORA_ALPHA = 16\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "print(f\"ğŸ“¦ Model: {MODEL_NAME}\")\n",
        "print(f\"ğŸ’¾ Output: {OUTPUT_DIR}\")\n",
        "print(f\"ğŸ“Š Data: FILE-LEVEL (complete workflows)\")\n",
        "print(f\"ğŸ“ˆ Epochs: {NUM_EPOCHS}, Batch: {BATCH_SIZE}, LR: {LEARNING_RATE}\")\n",
        "print(f\"ğŸ¯ Optimized for: English/Dutch â†’ JSON code generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774b429b",
      "metadata": {
        "id": "774b429b"
      },
      "outputs": [],
      "source": [
        "# åŠ è½½tokenizer (CodeLlama)\n",
        "print(\"ğŸ“– Loading CodeLlama tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"âœ… Tokenizer loaded: vocab_size={len(tokenizer)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b4deb72",
      "metadata": {
        "id": "7b4deb72"
      },
      "outputs": [],
      "source": [
        "# åŠ è½½æ¨¡å‹ï¼ˆ4-bité‡åŒ–ï¼‰\n",
        "print(\"ğŸ¤– Loading CodeLlama-7B with 4-bit quantization...\")\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "print(\"âœ… CodeLlama base model loaded\")\n",
        "\n",
        "# å‡†å¤‡æ¨¡å‹\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# åº”ç”¨LoRA\n",
        "print(\"ğŸ”§ Applying LoRA...\")\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    inference_mode=False,\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(\"âœ… LoRA applied!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3a6aeaa",
      "metadata": {
        "id": "b3a6aeaa"
      },
      "outputs": [],
      "source": [
        "# å‡†å¤‡æ•°æ®é›†\n",
        "print(\"ğŸ“Š Preparing datasets...\")\n",
        "\n",
        "train_data = load_dataset('json', data_files=TRAIN_FILE, split='train')\n",
        "eval_data = load_dataset('json', data_files=VAL_FILE, split='train')\n",
        "\n",
        "print(f\"  Train: {len(train_data)} samples\")\n",
        "print(f\"  Val: {len(eval_data)} samples\")\n",
        "\n",
        "# æ ¼å¼åŒ–prompt (CodeLlamaä¼˜åŒ–æ ¼å¼)\n",
        "def format_prompt(example):\n",
        "    instruction = example['instruction']\n",
        "    input_text = example.get('input', '')\n",
        "    output = example['output']\n",
        "\n",
        "    # CodeLlamaæ›´é€‚åˆç›´æ¥çš„ä»£ç ç”Ÿæˆæ ¼å¼\n",
        "    if input_text:\n",
        "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {instruction}\n",
        "Context: {input_text}\n",
        "\n",
        "JSON Code:\n",
        "{output}\"\"\"\n",
        "    else:\n",
        "        prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {instruction}\n",
        "\n",
        "JSON Code:\n",
        "{output}\"\"\"\n",
        "\n",
        "    return {\"text\": prompt}\n",
        "\n",
        "train_data = train_data.map(format_prompt, remove_columns=train_data.column_names)\n",
        "eval_data = eval_data.map(format_prompt, remove_columns=eval_data.column_names)\n",
        "\n",
        "# Tokenize\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples['text'],\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH,\n",
        "        padding=False,\n",
        "        return_tensors=None\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "    return tokenized\n",
        "\n",
        "print(\"ğŸ”„ Tokenizing...\")\n",
        "train_dataset = train_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_data.column_names,\n",
        "    desc=\"Tokenizing train\"\n",
        ")\n",
        "\n",
        "eval_dataset = eval_data.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=eval_data.column_names,\n",
        "    desc=\"Tokenizing val\"\n",
        ")\n",
        "\n",
        "print(\"âœ… Datasets ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "246a2b8b",
      "metadata": {
        "id": "246a2b8b"
      },
      "outputs": [],
      "source": [
        "# é…ç½®è®­ç»ƒ\n",
        "print(\"âš™ï¸ Configuring training...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    warmup_steps=100,\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    eval_steps=500,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    save_total_limit=3,\n",
        "    report_to=\"none\",\n",
        "    logging_dir=f\"{OUTPUT_DIR}/logs\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=model,\n",
        "    padding=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ready!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸš€ Starting training...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# å¼€å§‹è®­ç»ƒ\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ‰ Training completed!\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1492a771",
      "metadata": {
        "id": "1492a771"
      },
      "outputs": [],
      "source": [
        "# ä¿å­˜æ¨¡å‹\n",
        "print(\"ğŸ’¾ Saving model...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"âœ… Model saved to {OUTPUT_DIR}\")\n",
        "\n",
        "# ä¿å­˜è®­ç»ƒä¿¡æ¯\n",
        "import json\n",
        "training_info = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"learning_rate\": LEARNING_RATE,\n",
        "    \"lora_r\": LORA_R,\n",
        "    \"lora_alpha\": LORA_ALPHA,\n",
        "    \"train_samples\": len(train_dataset),\n",
        "    \"val_samples\": len(eval_dataset),\n",
        "}\n",
        "\n",
        "with open(f\"{OUTPUT_DIR}/training_info.json\", 'w') as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(\"\\nğŸ“Š Training Summary:\")\n",
        "for key, value in training_info.items():\n",
        "    print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe3514",
      "metadata": {
        "id": "66fe3514"
      },
      "source": [
        "## ğŸ§ª æ­¥éª¤6ï¼šæµ‹è¯•æ¨¡å‹"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba7233b4",
      "metadata": {
        "id": "ba7233b4"
      },
      "outputs": [],
      "source": [
        "# å¿«é€Ÿæµ‹è¯• (CodeLlama)\n",
        "print(\"ğŸ§ª Testing CodeLlama model inference...\")\n",
        "\n",
        "test_instruction = \"Create a new MS cable object at coordinates (186355533, 439556907)\"\n",
        "test_context = \"Application: PowerGrid | Database: ND | Steps: 5\"\n",
        "\n",
        "prompt = f\"\"\"You are a GIS workflow code generator. Generate complete JSON workflow code based on the instruction.\n",
        "\n",
        "Instruction: {test_instruction}\n",
        "Context: {test_context}\n",
        "\n",
        "JSON Code:\n",
        "\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"\\nğŸ”® Generating...\")\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "response = response.split(\"JSON Code:\")[-1].strip()\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ğŸ“ Test Result:\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Instruction: {test_instruction}\")\n",
        "print(f\"Context: {test_context}\")\n",
        "print(f\"\\nGenerated Output:\\n{response[:500]}...\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91162b41",
      "metadata": {
        "id": "91162b41"
      },
      "source": [
        "## ğŸ“¦ æ­¥éª¤7ï¼šä¸‹è½½æ¨¡å‹ï¼ˆå¯é€‰ï¼‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f8ae61a",
      "metadata": {
        "id": "9f8ae61a"
      },
      "outputs": [],
      "source": [
        "# æ‰“åŒ…æ¨¡å‹ç”¨äºä¸‹è½½\n",
        "!cd /content/drive/MyDrive/gis-models && zip -r codellama-gis-lora.zip codellama-gis-lora/\n",
        "print(\"âœ… Model zipped!\")\n",
        "print(f\"ğŸ“¦ Location: /content/drive/MyDrive/gis-models/codellama-gis-lora.zip\")\n",
        "print(\"ğŸ’¡ You can download it from Google Drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc7dcf9b",
      "metadata": {
        "id": "bc7dcf9b"
      },
      "source": [
        "## ğŸ¯ ä¸‹ä¸€æ­¥\n",
        "\n",
        "è®­ç»ƒå®Œæˆåï¼Œä½ å¯ä»¥ï¼š\n",
        "\n",
        "1. **åœ¨Colabä¸­ç»§ç»­æµ‹è¯•**ï¼šä½¿ç”¨ä¸Šé¢çš„æµ‹è¯•å•å…ƒæ ¼\n",
        "2. **ä¸‹è½½æ¨¡å‹**ï¼šä»Google Driveä¸‹è½½æ‰“åŒ…çš„æ¨¡å‹\n",
        "3. **æœ¬åœ°éƒ¨ç½²**ï¼šå°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°è¿›è¡Œæ¨ç†\n",
        "4. **è¯„ä¼°æ¨¡å‹**ï¼šè¿è¡Œå®Œæ•´çš„è¯„ä¼°è„šæœ¬\n",
        "\n",
        "### æœ¬åœ°ä½¿ç”¨æ¨¡å‹\n",
        "\n",
        "```python\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from peft import PeftModel\n",
        "\n",
        "# åŠ è½½CodeLlamaåŸºåº§æ¨¡å‹\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"codellama/CodeLlama-7b-Instruct-hf\")\n",
        "model = PeftModel.from_pretrained(base_model, \"path/to/codellama-gis-lora\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"path/to/codellama-gis-lora\")\n",
        "\n",
        "# æ¨ç†\n",
        "# ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**è®­ç»ƒå‚æ•°è°ƒä¼˜å»ºè®®ï¼š**\n",
        "- T4 GPU: batch_size=4, gradient_accumulation=4\n",
        "- A100 GPU: batch_size=8-12, gradient_accumulation=2-4\n",
        "- å¦‚æœæ˜¾å­˜ä¸è¶³ï¼šå‡å°batch_sizeæˆ–max_length\n",
        "- è®­ç»ƒæ—¶é—´ä¼°è®¡ï¼šT4çº¦4-6å°æ—¶ï¼ŒA100çº¦1-2å°æ—¶\n",
        "\n",
        "**ğŸ‰ æ­å–œï¼ä½ å·²ç»å®Œæˆäº†GISä»£ç ç”Ÿæˆæ¨¡å‹çš„è®­ç»ƒï¼ˆCodeLlamaç‰ˆï¼‰ï¼**"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}