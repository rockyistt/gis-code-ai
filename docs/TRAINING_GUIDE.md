# æ¨¡å‹è®­ç»ƒæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•ä½¿ç”¨å‡†å¤‡å¥½çš„æŒ‡ä»¤æ•°æ®è®­ç»ƒGISä»£ç ç”Ÿæˆæ¨¡å‹ã€‚

## ğŸ¯ è®­ç»ƒæµç¨‹

### æ–¹å¼1ï¼šå¿«é€Ÿå¼€å§‹ï¼ˆæ¨èï¼‰âš¡

ä½¿ç”¨å¿«é€Ÿè®­ç»ƒè„šæœ¬ï¼Œè‡ªåŠ¨å®Œæˆæ‰€æœ‰æ­¥éª¤ï¼š

```bash
# æµ‹è¯•æ¨¡å¼ï¼ˆå¿«é€ŸéªŒè¯æµç¨‹ï¼Œå°æ•°æ®é›†ï¼‰
python scripts/quick_train.py --test

# å®Œæ•´è®­ç»ƒ
python scripts/quick_train.py --full
```

### æ–¹å¼2ï¼šåˆ†æ­¥æ‰§è¡Œ ğŸ“

#### æ­¥éª¤1ï¼šå‡†å¤‡è®­ç»ƒæ•°æ®

```bash
python src/training/prepare_training_data.py \
  --instructions data/processed/step_level_instructions_weighted_variants_marked.jsonl \
  --workflows data/processed/parsed_workflows.jsonl \
  --output data/training/training_data.json \
  --split-ratio 0.9
```

**å‚æ•°è¯´æ˜ï¼š**
- `--instructions`: æŒ‡ä»¤æ–‡ä»¶è·¯å¾„
- `--workflows`: åŸå§‹å·¥ä½œæµæ–‡ä»¶ï¼ˆåŒ…å«JSONä»£ç ï¼‰
- `--output`: è¾“å‡ºæ–‡ä»¶è·¯å¾„
- `--split-ratio`: è®­ç»ƒ/éªŒè¯é›†åˆ’åˆ†æ¯”ä¾‹ï¼ˆé»˜è®¤0.9ï¼‰
- `--max-samples`: é™åˆ¶æ ·æœ¬æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
- `--keep-markers`: ä¿ç•™æƒé‡æ ‡è®°ï¼ˆé»˜è®¤ç§»é™¤ï¼‰

**è¾“å‡ºæ–‡ä»¶ï¼š**
- `data/training/training_data_train.json` - è®­ç»ƒé›†
- `data/training/training_data_val.json` - éªŒè¯é›†
- `data/training/training_data_stats.json` - ç»Ÿè®¡ä¿¡æ¯

#### æ­¥éª¤2ï¼šè®­ç»ƒæ¨¡å‹

```bash
python src/training/train_lora.py \
  --model-name Qwen/Qwen2.5-Coder-7B-Instruct \
  --train-file data/training/training_data_train.json \
  --val-file data/training/training_data_val.json \
  --output-dir models/qwen-gis-lora \
  --num-epochs 3 \
  --batch-size 4 \
  --learning-rate 2e-4
```

**å…³é”®å‚æ•°ï¼š**
- `--model-name`: åŸºåº§æ¨¡å‹åç§°
- `--num-epochs`: è®­ç»ƒè½®æ•°
- `--batch-size`: æ‰¹æ¬¡å¤§å°
- `--gradient-accumulation-steps`: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
- `--learning-rate`: å­¦ä¹ ç‡
- `--lora-r`: LoRAç§©ï¼ˆé»˜è®¤64ï¼‰
- `--use-4bit`: ä½¿ç”¨4-bité‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰

## ğŸ’» ç¡¬ä»¶è¦æ±‚

### æœ€ä½é…ç½®
- **GPU**: NVIDIA GPU with 16GB VRAMï¼ˆå¦‚RTX 3090, A4000ï¼‰
- **RAM**: 32GB
- **å­˜å‚¨**: 50GB

### æ¨èé…ç½®
- **GPU**: NVIDIA GPU with 24GB+ VRAMï¼ˆå¦‚RTX 4090, A5000, A100ï¼‰
- **RAM**: 64GB
- **å­˜å‚¨**: 100GB

### äº‘ç«¯è®­ç»ƒï¼ˆæ¨èï¼‰â˜ï¸
- **Google Colab Pro**: T4/A100 GPU
- **Kaggle**: P100/T4 GPUï¼ˆå…è´¹ï¼‰
- **AWS/Azure/é˜¿é‡Œäº‘**: æŒ‰éœ€ç§Ÿç”¨GPUå®ä¾‹

## ğŸ”§ é…ç½®æ–‡ä»¶

è®­ç»ƒé…ç½®ä¿å­˜åœ¨ `configs/training_config.yaml`ï¼š

```yaml
# æ¨¡å‹é…ç½®
model_name_or_path: "Qwen/Qwen2.5-Coder-7B-Instruct"
use_4bit: true

# LoRAé…ç½®
lora_r: 64
lora_alpha: 16
lora_dropout: 0.05

# è®­ç»ƒé…ç½®
num_train_epochs: 3
per_device_train_batch_size: 4
learning_rate: 2.0e-4
```

## ğŸ“Š è®­ç»ƒæ•°æ®æ ¼å¼

è®­ç»ƒæ•°æ®é‡‡ç”¨Alpacaæ ¼å¼ï¼š

```json
{
  "instruction": "Create a new MS cable object at coordinates (186355533, 439556907)",
  "input": "Application: PowerGrid | Step 1 of 5 | Database: ND",
  "output": "{\n  \"module\": \"Create\",\n  \"method\": \"Create\",\n  ...JSONä»£ç ...\n}"
}
```

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### è®­ç»ƒæ—¥å¿—

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šè¾“å‡ºï¼š
- Lossï¼ˆæŸå¤±ï¼‰
- Learning rateï¼ˆå­¦ä¹ ç‡ï¼‰
- Steps/secondï¼ˆè®­ç»ƒé€Ÿåº¦ï¼‰

### TensorBoardï¼ˆå¯é€‰ï¼‰

```bash
# ä¿®æ”¹train_lora.pyä¸­çš„report_to="tensorboard"
tensorboard --logdir models/qwen-gis-lora/runs
```

### Weights & Biasesï¼ˆå¯é€‰ï¼‰

```bash
# å®‰è£…wandb
pip install wandb

# ä¿®æ”¹train_lora.pyä¸­çš„report_to="wandb"
wandb login
```

## ğŸ¯ è®­ç»ƒæŠ€å·§

### 1. å¿«é€ŸéªŒè¯æµç¨‹

å…ˆç”¨å°æ•°æ®é›†éªŒè¯è®­ç»ƒæµç¨‹æ˜¯å¦æ­£å¸¸ï¼š

```bash
python src/training/prepare_training_data.py --max-samples 1000
python src/training/train_lora.py --num-epochs 1 --save-steps 50
```

### 2. è°ƒæ•´batch size

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥ï¼š
- å‡å° `--batch-size`ï¼ˆå¦‚æ”¹ä¸º2ï¼‰
- å¢åŠ  `--gradient-accumulation-steps`ï¼ˆå¦‚æ”¹ä¸º8ï¼‰
- ä¿æŒæœ‰æ•ˆbatch size = batch_size Ã— gradient_accumulation_steps

### 3. å­¦ä¹ ç‡è°ƒä¼˜

å»ºè®®å°è¯•çš„å­¦ä¹ ç‡èŒƒå›´ï¼š
- 1e-4ï¼ˆä¿å®ˆï¼‰
- 2e-4ï¼ˆé»˜è®¤ï¼‰
- 5e-4ï¼ˆæ¿€è¿›ï¼‰

### 4. LoRAå‚æ•°è°ƒä¼˜

- **lora_r**: 64ï¼ˆé»˜è®¤ï¼‰æˆ– 128ï¼ˆæ›´å¼ºè¡¨è¾¾èƒ½åŠ›ï¼Œä½†æ›´æ…¢ï¼‰
- **lora_alpha**: é€šå¸¸è®¾ä¸º lora_r çš„ 1/4 æˆ– 1/2

## ğŸš€ è®­ç»ƒåæ­¥éª¤

### 1. æ¨¡å‹è¯„ä¼°

```bash
python examples/evaluate_model.py \
  --model-path models/qwen-gis-lora \
  --test-file data/training/training_data_val.json
```

### 2. æ¨ç†æµ‹è¯•

```bash
python examples/demo_inference.py \
  --model-path models/qwen-gis-lora \
  --instruction "Create a new MS cable object"
```

### 3. éƒ¨ç½²æ¨¡å‹

è®­ç»ƒå¥½çš„æ¨¡å‹å¯ä»¥ï¼š
- é›†æˆåˆ°Webç•Œé¢ï¼ˆGradio/Streamlitï¼‰
- éƒ¨ç½²ä¸ºAPIæœåŠ¡ï¼ˆFastAPIï¼‰
- æ‰“åŒ…ä¸ºç¦»çº¿å·¥å…·

## â“ å¸¸è§é—®é¢˜

### Q1: CUDA out of memory
**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨ `--use-4bit` é‡åŒ–
- å‡å° `--batch-size`
- å‡å° `--max-length`
- ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚Qwen2.5-Coder-3Bï¼‰

### Q2: è®­ç»ƒå¤ªæ…¢
**è§£å†³æ–¹æ¡ˆï¼š**
- ä½¿ç”¨æ›´å¼ºçš„GPU
- å¢åŠ  `--batch-size`ï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
- å‡å°‘ `--save-steps` å’Œ `--logging-steps`

### Q3: Lossä¸ä¸‹é™
**æ£€æŸ¥ï¼š**
- å­¦ä¹ ç‡æ˜¯å¦åˆé€‚
- æ•°æ®æ˜¯å¦æœ‰é—®é¢˜
- æ˜¯å¦è¿‡æ‹Ÿåˆï¼ˆå¯¹æ¯”trainå’Œval lossï¼‰

### Q4: æ¨¡å‹ç”Ÿæˆè´¨é‡å·®
**æ”¹è¿›æ–¹æ³•ï¼š**
- å¢åŠ è®­ç»ƒæ•°æ®é‡
- è°ƒæ•´LoRAå‚æ•°ï¼ˆå¢å¤§lora_rï¼‰
- è®­ç»ƒæ›´å¤šè½®æ¬¡
- æ£€æŸ¥æ•°æ®è´¨é‡

## ğŸ“š å‚è€ƒèµ„æº

- [Qwen2.5-Coderæ¨¡å‹å¡](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)
- [LoRAè®ºæ–‡](https://arxiv.org/abs/2106.09685)
- [Hugging Face Transformersæ–‡æ¡£](https://huggingface.co/docs/transformers)
- [PEFTåº“æ–‡æ¡£](https://huggingface.co/docs/peft)

## ğŸ’¬ éœ€è¦å¸®åŠ©ï¼Ÿ

é‡åˆ°é—®é¢˜å¯ä»¥ï¼š
1. æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
2. æ£€æŸ¥GitHub Issues
3. æŸ¥é˜…ç›¸å…³æ–‡æ¡£
