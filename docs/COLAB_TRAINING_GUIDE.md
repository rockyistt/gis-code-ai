# Google Colab è®­ç»ƒæŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æ‰“å¼€Colab Notebook

ç‚¹å‡»ä¸‹é¢çš„é“¾æ¥åœ¨Google Colabä¸­æ‰“å¼€è®­ç»ƒè„šæœ¬ï¼š

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YOUR_USERNAME/gis-code-ai/blob/main/notebooks/Train_GIS_Model_Colab.ipynb)

### 2. è®¾ç½®GPUè¿è¡Œæ—¶

åœ¨Colabä¸­ï¼š
1. ç‚¹å‡» `Runtime` â†’ `Change runtime type`
2. é€‰æ‹© `Hardware accelerator` â†’ `GPU`
3. GPUç±»å‹é€‰æ‹©ï¼š
   - **T4**ï¼ˆå…è´¹ï¼‰- çº¦4-6å°æ—¶è®­ç»ƒæ—¶é—´
   - **A100**ï¼ˆColab Proï¼‰- çº¦1-2å°æ—¶è®­ç»ƒæ—¶é—´

### 3. å‡†å¤‡æ•°æ®

æœ‰ä¸‰ç§æ–¹å¼ä¸Šä¼ æ•°æ®ï¼š

#### æ–¹å¼Aï¼šä»GitHubå…‹éš†ï¼ˆæ¨èï¼‰
```bash
# åœ¨Colabä¸­è¿è¡Œ
!git clone https://github.com/YOUR_USERNAME/gis-code-ai.git
%cd gis-code-ai
```

#### æ–¹å¼Bï¼šä»Google Drive
```python
# å…ˆå°†æ•°æ®ä¸Šä¼ åˆ°Google Drive
from google.colab import drive
drive.mount('/content/drive')

# å¤åˆ¶æ•°æ®
!cp /content/drive/MyDrive/gis-data/*.jsonl data/processed/
```

#### æ–¹å¼Cï¼šæ‰‹åŠ¨ä¸Šä¼ 
```python
from google.colab import files
uploaded = files.upload()  # é€‰æ‹©æ–‡ä»¶ä¸Šä¼ 
```

### 4. è¿è¡ŒNotebook

æŒ‰é¡ºåºæ‰§è¡ŒNotebookä¸­çš„æ‰€æœ‰å•å…ƒæ ¼ï¼š

1. âœ… æ£€æŸ¥GPU
2. âœ… å®‰è£…ä¾èµ–
3. âœ… æŒ‚è½½Google Drive
4. âœ… ä¸Šä¼ /å‡†å¤‡æ•°æ®
5. âœ… å‡†å¤‡è®­ç»ƒæ•°æ®
6. âœ… è®­ç»ƒæ¨¡å‹ï¼ˆè¿™ä¸€æ­¥æœ€è€—æ—¶ï¼‰
7. âœ… æµ‹è¯•æ¨¡å‹
8. âœ… ä¿å­˜åˆ°Google Drive

## ğŸ“Š è®­ç»ƒé…ç½®

### é»˜è®¤é…ç½®ï¼ˆé€‚ç”¨äºT4 GPUï¼‰

```python
NUM_EPOCHS = 3
BATCH_SIZE = 4
GRADIENT_ACCUMULATION = 4
LEARNING_RATE = 2e-4
LORA_R = 64
```

**æœ‰æ•ˆbatch size** = BATCH_SIZE Ã— GRADIENT_ACCUMULATION = 16

### A100 GPUé…ç½®ï¼ˆæ›´å¿«ï¼‰

```python
NUM_EPOCHS = 3
BATCH_SIZE = 8
GRADIENT_ACCUMULATION = 2
LEARNING_RATE = 2e-4
```

### å¿«é€Ÿæµ‹è¯•é…ç½®

```python
NUM_EPOCHS = 1
BATCH_SIZE = 2
GRADIENT_ACCUMULATION = 2
# åœ¨å‡†å¤‡æ•°æ®æ—¶æ·»åŠ ï¼šmax_samples=1000
```

## ğŸ’¾ ä¿å­˜æ¨¡å‹

æ¨¡å‹ä¼šè‡ªåŠ¨ä¿å­˜åˆ°Google Driveï¼š
```
/content/drive/MyDrive/gis-models/qwen-gis-lora/
```

åŒ…å«ï¼š
- `adapter_config.json` - LoRAé…ç½®
- `adapter_model.bin` - LoRAæƒé‡
- `training_info.json` - è®­ç»ƒä¿¡æ¯
- `tokenizer_config.json`, `special_tokens_map.json` - Tokenizeré…ç½®

## ğŸ“¥ ä¸‹è½½æ¨¡å‹

### æ–¹æ³•1ï¼šç›´æ¥ä»Google Driveä¸‹è½½

1. è®­ç»ƒå®Œæˆåï¼Œè®¿é—®Google Drive
2. æ‰¾åˆ° `MyDrive/gis-models/qwen-gis-lora/`
3. å³é”®ä¸‹è½½æ•´ä¸ªæ–‡ä»¶å¤¹

### æ–¹æ³•2ï¼šåœ¨Colabä¸­æ‰“åŒ…ä¸‹è½½

```python
# æ‰“åŒ…æ¨¡å‹
!cd /content/drive/MyDrive/gis-models && zip -r qwen-gis-lora.zip qwen-gis-lora/

# ä¸‹è½½
from google.colab import files
files.download('/content/drive/MyDrive/gis-models/qwen-gis-lora.zip')
```

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: CUDA Out of Memory

**ç—‡çŠ¶**ï¼šè®­ç»ƒæ—¶æŠ¥é”™ `CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ¡ˆ1ï¼šå‡å°batch size
BATCH_SIZE = 2
GRADIENT_ACCUMULATION = 8

# æ–¹æ¡ˆ2ï¼šå‡å°max_length
MAX_LENGTH = 1024

# æ–¹æ¡ˆ3ï¼šå‡å°LoRAç§©
LORA_R = 32
```

### Q2: è®­ç»ƒä¸­æ–­/æ–­å¼€è¿æ¥

**é¢„é˜²æªæ–½**ï¼š
- ä½¿ç”¨Colab Proï¼ˆè¿æ¥æ›´ç¨³å®šï¼‰
- å®šæœŸä¿å­˜checkpointï¼ˆå·²è‡ªåŠ¨é…ç½® `save_steps=500`ï¼‰
- ä¿æŒæµè§ˆå™¨æ ‡ç­¾é¡µæ´»è·ƒ

**æ¢å¤è®­ç»ƒ**ï¼š
```python
# ä»checkpointæ¢å¤
trainer = Trainer(...)
trainer.train(resume_from_checkpoint=True)
```

### Q3: æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°

**æ£€æŸ¥æ¸…å•**ï¼š
```python
import os
print(os.listdir('data/processed/'))  # æŸ¥çœ‹æ–‡ä»¶
```

ç¡®ä¿å­˜åœ¨ï¼š
- `step_level_instructions_weighted_variants_marked.jsonl`
- `parsed_workflows.jsonl`

### Q4: è®­ç»ƒé€Ÿåº¦æ…¢

**ä¼˜åŒ–å»ºè®®**ï¼š
- ç¡®è®¤ä½¿ç”¨GPUï¼š`!nvidia-smi`
- ä½¿ç”¨A100 GPUï¼ˆColab Proï¼‰
- å¢å¤§batch sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
- å‡å°‘loggingé¢‘ç‡ï¼š`logging_steps=50`

### Q5: æ¨¡å‹æ•ˆæœä¸å¥½

**æ”¹è¿›æ–¹æ³•**ï¼š
1. å¢åŠ è®­ç»ƒè½®æ•°ï¼š`NUM_EPOCHS = 5`
2. ä½¿ç”¨æ›´å¤šæ•°æ®ï¼ˆå»æ‰max_samplesé™åˆ¶ï¼‰
3. è°ƒæ•´å­¦ä¹ ç‡ï¼š`LEARNING_RATE = 1e-4` æˆ– `5e-4`
4. å¢å¤§LoRAç§©ï¼š`LORA_R = 128`

## ğŸ“ˆ ç›‘æ§è®­ç»ƒ

### æŸ¥çœ‹è®­ç»ƒæ—¥å¿—

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
```
Step 10/1000 | Loss: 2.543 | LR: 0.0002 | Speed: 2.3 steps/s
Step 20/1000 | Loss: 2.134 | LR: 0.0002 | Speed: 2.4 steps/s
...
```

### ç†è§£Loss

- **åˆå§‹Loss**: é€šå¸¸åœ¨2-4ä¹‹é—´
- **è®­ç»ƒä¸­**: åº”è¯¥é€æ¸ä¸‹é™
- **æ”¶æ•›**: æœ€ç»ˆåœ¨0.5-1.5ä¹‹é—´
- **è¿‡æ‹Ÿåˆ**: å¦‚æœtrain losså¾ˆä½ä½†val losså¾ˆé«˜

### ä½¿ç”¨TensorBoardï¼ˆå¯é€‰ï¼‰

```python
# åœ¨training_argsä¸­ä¿®æ”¹
report_to="tensorboard"

# åœ¨å¦ä¸€ä¸ªcellä¸­å¯åŠ¨
%load_ext tensorboard
%tensorboard --logdir /content/drive/MyDrive/gis-models/qwen-gis-lora/logs
```

## ğŸ¯ è®­ç»ƒå®Œæˆå

### 1. æµ‹è¯•æ¨¡å‹

åœ¨Notebookçš„æµ‹è¯•å•å…ƒæ ¼ä¸­è¿è¡Œï¼š
```python
test_instruction = "Create a new MS cable object"
# æŸ¥çœ‹ç”Ÿæˆç»“æœ
```

### 2. æœ¬åœ°ä½¿ç”¨

ä¸‹è½½æ¨¡å‹åï¼Œåœ¨æœ¬åœ°ä½¿ç”¨ï¼š
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# åŠ è½½åŸºåº§æ¨¡å‹
base_model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-Coder-7B-Instruct",
    device_map="auto",
    torch_dtype=torch.float16
)

# åŠ è½½LoRA adapter
model = PeftModel.from_pretrained(base_model, "path/to/qwen-gis-lora")
tokenizer = AutoTokenizer.from_pretrained("path/to/qwen-gis-lora")

# æ¨ç†
model.eval()
# ...
```

### 3. è¯„ä¼°æ¨¡å‹

```bash
# åœ¨æœ¬åœ°è¿è¡Œ
python examples/evaluate_model.py \
  --model-path models/qwen-gis-lora \
  --test-file data/training/training_data_val.json
```

## ğŸ’° æˆæœ¬ä¼°ç®—

### Colabå…è´¹ç‰ˆ
- GPU: T4ï¼ˆ16GBï¼‰
- é™åˆ¶: 12å°æ—¶/ä¼šè¯
- æˆæœ¬: **å…è´¹**
- è®­ç»ƒæ—¶é—´: 4-6å°æ—¶
- é€‚ç”¨: æµ‹è¯•å’Œå°è§„æ¨¡è®­ç»ƒ

### Colab Pro ($9.99/æœˆ)
- GPU: T4/A100
- é™åˆ¶: 24å°æ—¶/ä¼šè¯
- è®­ç»ƒæ—¶é—´: 1-2å°æ—¶ï¼ˆA100ï¼‰
- é€‚ç”¨: å®Œæ•´è®­ç»ƒ

### Colab Pro+ ($49.99/æœˆ)
- GPU: A100ï¼ˆ40GBï¼‰
- é™åˆ¶: æ›´é•¿ä¼šè¯æ—¶é—´
- é€‚ç”¨: å¤§è§„æ¨¡è®­ç»ƒ

## ğŸ“š æ¨èé˜…è¯»

- [Google Colabä½¿ç”¨æŒ‡å—](https://colab.research.google.com/notebooks/intro.ipynb)
- [LoRAåŸç†è§£æ](https://arxiv.org/abs/2106.09685)
- [Qwen2.5-Coderæ¨¡å‹æ–‡æ¡£](https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct)

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š
1. æŸ¥çœ‹æœ¬æŒ‡å—çš„å¸¸è§é—®é¢˜éƒ¨åˆ†
2. æ£€æŸ¥Colabçš„è¾“å‡ºæ—¥å¿—
3. åœ¨GitHubä¸ŠæIssue
4. æŸ¥é˜…Transformerså’ŒPEFTæ–‡æ¡£

---

**ğŸ‰ ç¥è®­ç»ƒé¡ºåˆ©ï¼**
