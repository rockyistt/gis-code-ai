#!/usr/bin/env python3
"""
ä»Stepçº§æŒ‡ä»¤èšåˆç”ŸæˆFileçº§æŒ‡ä»¤
æ ¸å¿ƒæ€è·¯ï¼š
1. ä»æ–‡ä»¶åæ¨æ–­ä»»åŠ¡ç±»å‹ï¼ˆcreate, update, deleteç­‰ï¼‰
2. ä»stepsèšåˆæ‰€æœ‰å¯¹è±¡åï¼ˆé¿å…"multiple objects"ï¼‰
3. å¦‚æœå¯¹è±¡è¿‡å¤š(>3)ï¼Œåˆ—å‡ºå‰3ä¸ª+ç±»åˆ«
"""
import json
import re
from collections import defaultdict, Counter
from pathlib import Path
from typing import List, Dict, Any
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def infer_task_from_filename(filename: str) -> str:
    """ä»æ–‡ä»¶åæ¨æ–­ä»»åŠ¡ç±»å‹"""
    filename_lower = filename.lower()
    
    # å…³é”®è¯æ˜ å°„
    task_keywords = {
        'insert': 'create',
        'opvoeren': 'create',
        'nieuw': 'create',
        'create': 'create',
        'crud': 'manage',  # CRUDè¡¨ç¤ºå¢åˆ æ”¹æŸ¥
        'update': 'update',
        'delete': 'delete',
        'remove': 'delete',
    }
    
    for keyword, task in task_keywords.items():
        if keyword in filename_lower:
            return task
    
    return 'manage'  # é»˜è®¤


def aggregate_objects(objects: List[str], max_display: int = 3) -> Dict[str, Any]:
    """
    èšåˆå¯¹è±¡åˆ—è¡¨
    å¦‚æœå¯¹è±¡è¿‡å¤šï¼Œæ˜¾ç¤ºå‰Nä¸ª+ç±»åˆ«
    """
    unique_objects = list(dict.fromkeys(objects))  # å»é‡ä¿åº
    
    if len(unique_objects) == 0:
        return {
            'display': 'objects',
            'count': 0,
            'list': []
        }
    elif len(unique_objects) <= max_display:
        return {
            'display': ', '.join(unique_objects),
            'count': len(unique_objects),
            'list': unique_objects
        }
    else:
        # æå–ç±»åˆ«ï¼ˆé€šå¸¸æ˜¯ç¬¬ä¸€ä¸ªè¯ï¼Œå¦‚"E MS Kabel"ä¸­çš„"E"ï¼‰
        categories = [obj.split()[0] for obj in unique_objects if ' ' in obj]
        category_counts = Counter(categories)
        most_common_category = category_counts.most_common(1)[0][0] if category_counts else None
        
        # æ˜¾ç¤ºå‰Nä¸ª + ç±»åˆ«
        top_objects = unique_objects[:max_display]
        if most_common_category:
            display_str = f"{', '.join(top_objects)} and other {most_common_category} objects"
        else:
            display_str = f"{', '.join(top_objects)} and {len(unique_objects) - max_display} more objects"
        
        return {
            'display': display_str,
            'count': len(unique_objects),
            'list': unique_objects,
            'top_category': most_common_category
        }


def infer_object_category(objects: List[str]) -> str:
    """ä»å¯¹è±¡åˆ—è¡¨æ¨æ–­å¯¹è±¡ç±»åˆ«ï¼ˆé«˜å±‚æ¬¡æ¦‚æ‹¬ï¼‰"""
    if not objects:
        return "objects"
    
    # æå–å¯¹è±¡ç±»å‹å‰ç¼€ï¼ˆå¦‚ E MS, E HS, E LSï¼‰
    type_counts = Counter()
    for obj in objects:
        parts = obj.split()
        if len(parts) >= 2:
            # æå–ç±»å‹å‰ç¼€ï¼ˆå¦‚ "E MS", "E HS"ï¼‰
            prefix = ' '.join(parts[:2])
            type_counts[prefix] += 1
    
    # å¦‚æœæœ‰æ˜ç¡®çš„ä¸»è¦ç±»å‹
    if type_counts:
        most_common = type_counts.most_common(2)
        if len(most_common) == 1:
            return f"{most_common[0][0]} components"
        elif len(most_common) >= 2:
            return f"{most_common[0][0]}/{most_common[1][0]} components"
    
    # æå–å¯¹è±¡ç±»åˆ«å…³é”®è¯ï¼ˆå¦‚ Kabel, Installatie, Aardingstrafoï¼‰
    category_keywords = []
    for obj in objects:
        # å–æœ€åä¸€ä¸ªæœ‰æ„ä¹‰çš„è¯ä½œä¸ºç±»åˆ«
        words = [w for w in obj.split() if w not in ['FP', 'E', 'MS', 'HS', 'LS', 'Sec']]
        if words:
            category_keywords.append(words[-1])
    
    if category_keywords:
        category_counts = Counter(category_keywords)
        main_category = category_counts.most_common(1)[0][0]
        if category_counts[main_category] > 1:
            return f"{main_category} components"
        else:
            return f"electrical components"
    
    return "objects"


def aggregate_file_instruction(file_id: str, steps: List[Dict]) -> Dict[str, Any]:
    """ä»stepsèšåˆç”Ÿæˆfileçº§æŒ‡ä»¤ï¼ˆé«˜å±‚æ¬¡ä»»åŠ¡æè¿°ï¼‰"""
    
    # 1. ä»æ–‡ä»¶åæ¨æ–­ä¸»è¦ä»»åŠ¡
    task = infer_task_from_filename(file_id)
    
    # 2. èšåˆæ‰€æœ‰æ­¥éª¤çš„ä¿¡æ¯
    all_objects = []
    all_actions = []
    all_databases = set()
    all_keywords = []
    test_app = None
    is_high_quality = any(s.get('is_high_quality', False) for s in steps)
    
    for step in steps:
        # å¯¹è±¡
        if 'structure' in step and 'object' in step['structure']:
            obj = step['structure']['object']
            if obj and obj not in ['object', 'objects']:  # è¿‡æ»¤æ³›åŒ–è¯
                # æ¸…ç†å¯¹è±¡åï¼ˆå»æ‰"object"åç¼€ï¼‰
                obj_clean = re.sub(r'\s+object$', '', obj, flags=re.IGNORECASE)
                all_objects.append(obj_clean)
        
        # åŠ¨ä½œ
        if 'structure' in step and 'action' in step['structure']:
            action = step['structure']['action']
            if action:
                all_actions.append(action.lower())
        
        # æ•°æ®åº“/ä¸Šä¸‹æ–‡
        if 'structure' in step:
            # ä»contextå­—æ®µæå–
            if 'context' in step['structure']:
                ctx = step['structure']['context']
                if ctx:
                    all_databases.add(ctx)
            
            # ä»adverbialsæå–ï¼ˆå¦‚"in elektra dataset"ï¼‰
            if 'adverbials' in step['structure']:
                adverbs = step['structure']['adverbials']
                # æŸ¥æ‰¾"in XXX dataset/system"æ¨¡å¼
                for i, adv in enumerate(adverbs):
                    if adv == 'in' and i + 1 < len(adverbs):
                        next_word = adverbs[i + 1]
                        # è¿‡æ»¤æ‰æ³›åŒ–è¯
                        if next_word not in ['object', 'objects', 'dataset', 'system', 'database']:
                            all_databases.add(next_word)
        
        # å…³é”®è¯
        if 'keywords' in step:
            all_keywords.extend(step['keywords'])
        
        # åº”ç”¨å
        if 'instruction' in step:
            # å°è¯•ä»instructionæå–appåï¼ˆé€šå¸¸åœ¨æœ«å°¾ï¼‰
            match = re.search(r'in (NRG [^\.]+)', step['instruction'])
            if match:
                test_app = match.group(1).strip()
    
    # 3. æ¨æ–­ä¸»è¦åŠ¨ä½œï¼ˆåªå–æœ€ä¸»è¦çš„ä¸šåŠ¡åŠ¨ä½œï¼Œå¿½ç•¥è¾…åŠ©åŠ¨ä½œï¼‰
    action_counts = Counter(all_actions)
    # è¿‡æ»¤æ‰è¾…åŠ©åŠ¨ä½œï¼ˆopen, switch, navigateç­‰ï¼‰
    auxiliary_actions = {'open', 'switch', 'navigate', 'select', 'close'}
    business_actions = {action: count for action, count in action_counts.items() 
                       if action not in auxiliary_actions}
    
    if business_actions:
        # å¦‚æœæœ‰CRUDåŠ¨ä½œï¼ˆcreate/update/deleteï¼‰ï¼Œä¼˜å…ˆä½¿ç”¨
        crud_actions = ['create', 'update', 'delete']
        crud_found = [a for a in crud_actions if a in business_actions]
        
        if len(crud_found) >= 2:
            # å¤šä¸ªCRUDåŠ¨ä½œï¼Œä½¿ç”¨"manage"
            primary_action = 'manage'
        elif crud_found:
            # å•ä¸ªCRUDåŠ¨ä½œ
            primary_action = crud_found[0]
        else:
            # å–æœ€é¢‘ç¹çš„ä¸šåŠ¡åŠ¨ä½œ
            primary_action = max(business_actions, key=business_actions.get)
    else:
        # å¦‚æœæ²¡æœ‰ä¸šåŠ¡åŠ¨ä½œï¼Œä½¿ç”¨æ¨æ–­çš„ä»»åŠ¡
        primary_action = task
    
    # 4. æ¨æ–­å¯¹è±¡ç±»åˆ«ï¼ˆé«˜å±‚æ¬¡æ¦‚æ‹¬ï¼‰
    object_category = infer_object_category(all_objects)
    
    # 5. æå–æ•°æ®åº“å’Œåº”ç”¨ä¿¡æ¯
    database_str = ', '.join(all_databases) if all_databases else None
    app_str = test_app
    
    # 6. æ„å»ºé«˜å±‚æ¬¡çš„instructionæ–‡æœ¬
    # æ ¼å¼ï¼š"Manage E MS components in elektra system" æˆ– "Create cables for NRG Beheerkaart"
    instruction_parts = []
    
    # ä¸»è¦åŠ¨ä½œ + å¯¹è±¡ç±»åˆ«
    instruction_parts.append(f"{primary_action.capitalize()} {object_category}")
    
    # æ·»åŠ ä¸Šä¸‹æ–‡ï¼ˆä¼˜å…ˆdatabaseï¼Œå…¶æ¬¡appï¼‰
    if database_str:
        instruction_parts.append(f"in {database_str} system")
    elif app_str:
        instruction_parts.append(f"for {app_str}")
    
    instruction = ' '.join(instruction_parts)
    instruction = ' '.join(instruction_parts)
    
    # 7. èšåˆå…³é”®è¯ï¼ˆå»é‡å¹¶ä¿ç•™æƒé‡ï¼‰
    keyword_dict = {}
    for kw, weight in all_keywords:
        if kw in keyword_dict:
            keyword_dict[kw] = max(keyword_dict[kw], weight)  # å–æœ€é«˜æƒé‡
        else:
            keyword_dict[kw] = weight
    
    aggregated_keywords = [[kw, weight] for kw, weight in keyword_dict.items()]
    
    # 8. ä¿å­˜è¯¦ç»†çš„å¯¹è±¡åˆ—è¡¨ï¼ˆç”¨äºè®­ç»ƒæ•°æ®ï¼‰
    unique_objects = list(dict.fromkeys(all_objects))  # å»é‡ä¿åº
    
    # 9. è¿”å›ç»“æœ
    return {
        'file_id': file_id,
        'is_high_quality': is_high_quality,
        'instruction': instruction,
        'provider': 'step_aggregation_v2',
        'test_app': app_str,
        'total_steps': len(steps),
        'keywords': aggregated_keywords,
        'primary_action': primary_action,
        'object_category': object_category,
        'objects': unique_objects,  # ä¿ç•™è¯¦ç»†å¯¹è±¡åˆ—è¡¨
        'object_count': len(unique_objects),
        'databases': list(all_databases),
        'inferred_task': task
    }


def main():
    """ä¸»å‡½æ•°"""
    input_file = Path('data/processed/step_level_instructions_weighted.jsonl')
    output_file = Path('data/processed/file_level_instructions_aggregated.jsonl')
    
    logging.info(f"ğŸ“– Reading step-level instructions from {input_file}")
    
    # 1. è¯»å–stepçº§æ•°æ®å¹¶æŒ‰file_idåˆ†ç»„
    steps_by_file = defaultdict(list)
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            step = json.loads(line)
            steps_by_file[step['file_id']].append(step)
    
    logging.info(f"âœ… Loaded {len(steps_by_file)} files with {sum(len(steps) for steps in steps_by_file.values())} total steps")
    
    # 2. èšåˆç”Ÿæˆfileçº§æŒ‡ä»¤
    logging.info("ğŸ“ Aggregating file-level instructions from steps...")
    file_instructions = []
    for file_id, steps in steps_by_file.items():
        file_inst = aggregate_file_instruction(file_id, steps)
        file_instructions.append(file_inst)
    
    # 3. ä¿å­˜ç»“æœ
    logging.info(f"ğŸ’¾ Saving to {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        for inst in file_instructions:
            f.write(json.dumps(inst, ensure_ascii=False) + '\n')
    
    # 4. ç»Ÿè®¡"multiple objects"å‡ºç°æ¬¡æ•°
    multiple_count = sum(1 for inst in file_instructions if 'multiple objects' in inst['instruction'].lower())
    
    logging.info("\n" + "="*60)
    logging.info("ğŸ‰ èšåˆå®Œæˆï¼")
    logging.info(f"ğŸ“„ è¾“å‡ºæ–‡ä»¶: {output_file}")
    logging.info(f"ğŸ“Š ç»Ÿè®¡:")
    logging.info(f"   - æ€»æ–‡ä»¶æ•°: {len(file_instructions)}")
    logging.info(f"   - å«\"multiple objects\"çš„æ–‡ä»¶: {multiple_count} ({multiple_count/len(file_instructions)*100:.1f}%)")
    logging.info(f"   - å¹³å‡æ¯ä¸ªæ–‡ä»¶æ­¥éª¤æ•°: {sum(inst['total_steps'] for inst in file_instructions) / len(file_instructions):.1f}")
    logging.info("="*60)
    
    # 5. æ˜¾ç¤ºæ ·æœ¬
    logging.info("\n=== Sample aggregated instructions ===")
    for inst in file_instructions[:5]:
        logging.info(f"\nFile: {inst['file_id']}")
        logging.info(f"  Task: {inst['inferred_task']} -> Action: {inst['primary_action']}")
        logging.info(f"  Category: {inst['object_category']}")
        logging.info(f"  Instruction: {inst['instruction']}")
        logging.info(f"  Objects ({inst['object_count']}): {', '.join(inst['objects'][:3])}...")


if __name__ == '__main__':
    main()
