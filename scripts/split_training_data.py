"""
å°†å±‚æ¬¡åŒ–è®­ç»ƒæ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†

ç”¨æ³•:
    python scripts/split_training_data.py
"""

import json
import random
from pathlib import Path
from typing import List, Dict

# é…ç½®
INPUT_FILE = Path("data/processed/hierarchical_training_data.json")
OUTPUT_DIR = Path("data/training")
TRAIN_FILE = OUTPUT_DIR / "training_data_train.json"
VAL_FILE = OUTPUT_DIR / "training_data_val.json"
VAL_RATIO = 0.1  # 10%éªŒè¯é›†
RANDOM_SEED = 42

def split_by_file_id(data: List[Dict], val_ratio: float = 0.1) -> tuple:
    """
    æŒ‰file_idåˆ†å‰²æ•°æ®ï¼Œç¡®ä¿åŒä¸€ä¸ªæ–‡ä»¶çš„æ‰€æœ‰æ­¥éª¤éƒ½åœ¨åŒä¸€ä¸ªé›†åˆä¸­
    
    Args:
        data: è®­ç»ƒæ•°æ®åˆ—è¡¨
        val_ratio: éªŒè¯é›†æ¯”ä¾‹
    
    Returns:
        (train_data, val_data) å…ƒç»„
    """
    # æŒ‰file_idåˆ†ç»„
    file_groups = {}
    for sample in data:
        file_id = sample["metadata"]["file_id"]
        if file_id not in file_groups:
            file_groups[file_id] = []
        file_groups[file_id].append(sample)
    
    # è·å–æ‰€æœ‰file_idå¹¶æ‰“ä¹±
    file_ids = list(file_groups.keys())
    random.seed(RANDOM_SEED)
    random.shuffle(file_ids)
    
    # è®¡ç®—éªŒè¯é›†å¤§å°
    num_val_files = max(1, int(len(file_ids) * val_ratio))
    val_file_ids = set(file_ids[:num_val_files])
    
    # åˆ†å‰²æ•°æ®
    train_data = []
    val_data = []
    
    for file_id, samples in file_groups.items():
        if file_id in val_file_ids:
            val_data.extend(samples)
        else:
            train_data.extend(samples)
    
    return train_data, val_data


def main():
    print("=" * 60)
    print("åˆ†å‰²å±‚æ¬¡åŒ–è®­ç»ƒæ•°æ®")
    print("=" * 60)
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶
    if not INPUT_FILE.exists():
        print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {INPUT_FILE}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    
    # è¯»å–æ•°æ®
    print(f"\nğŸ“– è¯»å–æ•°æ®: {INPUT_FILE}")
    with INPUT_FILE.open("r", encoding="utf-8") as f:
        data = json.load(f)
    
    print(f"âœ… æ€»æ ·æœ¬æ•°: {len(data)}")
    
    # ç»Ÿè®¡fileæ•°é‡
    file_ids = set(sample["metadata"]["file_id"] for sample in data)
    print(f"âœ… æ€»æ–‡ä»¶æ•°: {len(file_ids)}")
    
    # åˆ†å‰²æ•°æ®
    print(f"\nğŸ”€ åˆ†å‰²æ•°æ® (éªŒè¯é›†æ¯”ä¾‹: {VAL_RATIO*100:.1f}%)")
    train_data, val_data = split_by_file_id(data, val_ratio=VAL_RATIO)
    
    print(f"âœ… è®­ç»ƒé›†: {len(train_data)} æ ·æœ¬")
    print(f"âœ… éªŒè¯é›†: {len(val_data)} æ ·æœ¬")
    
    # éªŒè¯é›†æ–‡ä»¶ç»Ÿè®¡
    train_files = set(s["metadata"]["file_id"] for s in train_data)
    val_files = set(s["metadata"]["file_id"] for s in val_data)
    print(f"âœ… è®­ç»ƒé›†æ–‡ä»¶: {len(train_files)}")
    print(f"âœ… éªŒè¯é›†æ–‡ä»¶: {len(val_files)}")
    
    # ç¡®ä¿æ²¡æœ‰é‡å 
    overlap = train_files & val_files
    if overlap:
        print(f"âš ï¸  è­¦å‘Š: {len(overlap)} ä¸ªæ–‡ä»¶åŒæ—¶å‡ºç°åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†")
    else:
        print("âœ… è®­ç»ƒé›†å’ŒéªŒè¯é›†æ— é‡å ")
    
    # ä¿å­˜æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜è®­ç»ƒé›†: {TRAIN_FILE}")
    with TRAIN_FILE.open("w", encoding="utf-8") as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    print(f"ğŸ’¾ ä¿å­˜éªŒè¯é›†: {VAL_FILE}")
    with VAL_FILE.open("w", encoding="utf-8") as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    # æ˜¾ç¤ºç¤ºä¾‹
    print("\n" + "=" * 60)
    print("è®­ç»ƒé›†ç¤ºä¾‹:")
    print("=" * 60)
    print(json.dumps(train_data[0], ensure_ascii=False, indent=2)[:500] + "...")
    
    print("\n" + "=" * 60)
    print("âœ… åˆ†å‰²å®Œæˆï¼")
    print("=" * 60)
    print(f"è®­ç»ƒé›†: {TRAIN_FILE} ({len(train_data)} æ ·æœ¬, {len(train_files)} æ–‡ä»¶)")
    print(f"éªŒè¯é›†: {VAL_FILE} ({len(val_data)} æ ·æœ¬, {len(val_files)} æ–‡ä»¶)")
    

if __name__ == "__main__":
    main()
