"""
Google Colab æ¨¡å‹ä¿å­˜ä¸åŠ è½½ - é˜²å´©æºƒç‰ˆ
é€‚ç”¨äºè®­ç»ƒåä¿å­˜åˆ°Google Driveï¼Œä»¥åŠä»DriveåŠ è½½æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
1. è®­ç»ƒåè¿è¡Œ Section 1ï¼ˆä¿å­˜ï¼‰
2. æ–°sessionä¸­è¿è¡Œ Section 2ï¼ˆåŠ è½½ï¼‰
"""

# =====================================================================
# Section 1: è®­ç»ƒåä¿å­˜æ¨¡å‹ï¼ˆé˜²å´©æºƒç‰ˆï¼‰
# =====================================================================

def save_model_safely(trainer, tokenizer, output_name="codellama-gis-lora"):
    """
    å®‰å…¨åœ°ä¿å­˜æ¨¡å‹åˆ°Google Drive
    
    ç­–ç•¥ï¼šæœ¬åœ°ä¿å­˜ â†’ éªŒè¯ â†’ å¤åˆ¶åˆ°Drive â†’ ç­‰å¾…åŒæ­¥ â†’ å†æ¬¡éªŒè¯
    """
    import shutil
    import time
    import os
    import json
    
    LOCAL_TEMP = f"/content/model_temp/{output_name}"
    DRIVE_PATH = f"/content/drive/MyDrive/gis-models/{output_name}"
    
    print("="*70)
    print("ğŸ’¾ å®‰å…¨ä¿å­˜æ¨¡å‹åˆ°Google Drive")
    print("="*70)
    
    # -------------------------------------------------------------------
    # æ­¥éª¤1: ä¿å­˜åˆ°æœ¬åœ°ï¼ˆå¿«é€Ÿä¸”å¯é ï¼‰
    # -------------------------------------------------------------------
    print("\nã€æ­¥éª¤1/5ã€‘ä¿å­˜åˆ°æœ¬åœ°ä¸´æ—¶ç›®å½•...")
    os.makedirs(LOCAL_TEMP, exist_ok=True)
    
    trainer.save_model(LOCAL_TEMP)
    tokenizer.save_pretrained(LOCAL_TEMP)
    
    # ä¿å­˜è®­ç»ƒä¿¡æ¯
    training_info = {
        "model_name": getattr(trainer.model, "name_or_path", "unknown"),
        "saved_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        "train_samples": len(trainer.train_dataset) if trainer.train_dataset else 0,
        "eval_samples": len(trainer.eval_dataset) if trainer.eval_dataset else 0,
    }
    with open(f"{LOCAL_TEMP}/training_info.json", 'w') as f:
        json.dump(training_info, f, indent=2)
    
    print(f"   âœ… å·²ä¿å­˜åˆ°: {LOCAL_TEMP}")
    
    # -------------------------------------------------------------------
    # æ­¥éª¤2: éªŒè¯æœ¬åœ°æ–‡ä»¶å®Œæ•´æ€§
    # -------------------------------------------------------------------
    print("\nã€æ­¥éª¤2/5ã€‘éªŒè¯æ–‡ä»¶å®Œæ•´æ€§...")
    
    required_files = {
        "adapter_config.json": (0.001, 0.1),  # KB èŒƒå›´
        "tokenizer_config.json": (0.001, 0.1),
        "adapter_model.safetensors": (100, 1000),  # MB èŒƒå›´
        "tokenizer.model": (0.1, 10),
    }
    
    all_ok = True
    for fname, (min_mb, max_mb) in required_files.items():
        fpath = os.path.join(LOCAL_TEMP, fname)
        
        # æ£€æŸ¥å­˜åœ¨æ€§
        if not os.path.exists(fpath):
            # safetensorså¯èƒ½ä¿å­˜ä¸ºbinæ ¼å¼
            if fname == "adapter_model.safetensors":
                fpath_alt = os.path.join(LOCAL_TEMP, "adapter_model.bin")
                if os.path.exists(fpath_alt):
                    fpath = fpath_alt
                    fname = "adapter_model.bin"
                else:
                    print(f"   âŒ {fname} ä¸å­˜åœ¨!")
                    all_ok = False
                    continue
            else:
                print(f"   âŒ {fname} ä¸å­˜åœ¨!")
                all_ok = False
                continue
        
        # æ£€æŸ¥å¤§å°
        size_mb = os.path.getsize(fpath) / (1024**2)
        status = "âœ…" if min_mb <= size_mb <= max_mb else "âš ï¸"
        print(f"   {status} {fname}: {size_mb:.2f} MB")
        
        if not (min_mb <= size_mb <= max_mb):
            print(f"      é¢„æœŸèŒƒå›´: {min_mb}-{max_mb} MB")
            all_ok = False
    
    if not all_ok:
        raise RuntimeError("âš ï¸ æ–‡ä»¶éªŒè¯å¤±è´¥ï¼è¯·æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹ã€‚")
    
    print("   âœ… æ‰€æœ‰æ–‡ä»¶éªŒè¯é€šè¿‡ï¼")
    
    # -------------------------------------------------------------------
    # æ­¥éª¤3: å¤åˆ¶åˆ°Google Drive
    # -------------------------------------------------------------------
    print(f"\nã€æ­¥éª¤3/5ã€‘å¤åˆ¶åˆ°Google Drive...")
    print(f"   ç›®æ ‡: {DRIVE_PATH}")
    
    os.makedirs(DRIVE_PATH, exist_ok=True)
    
    # é€æ–‡ä»¶å¤åˆ¶ï¼ˆæ˜¾ç¤ºè¿›åº¦ï¼‰
    file_count = 0
    for root, dirs, files in os.walk(LOCAL_TEMP):
        for file in files:
            src = os.path.join(root, file)
            rel_path = os.path.relpath(src, LOCAL_TEMP)
            dst = os.path.join(DRIVE_PATH, rel_path)
            
            os.makedirs(os.path.dirname(dst), exist_ok=True)
            
            size_mb = os.path.getsize(src) / (1024**2)
            print(f"   ğŸ“‹ {rel_path} ({size_mb:.2f} MB)")
            
            shutil.copy2(src, dst)
            file_count += 1
            
            # éªŒè¯å¤åˆ¶
            if os.path.getsize(src) != os.path.getsize(dst):
                raise RuntimeError(f"âŒ å¤åˆ¶å¤±è´¥: {rel_path}")
    
    print(f"   âœ… å·²å¤åˆ¶ {file_count} ä¸ªæ–‡ä»¶")
    
    # -------------------------------------------------------------------
    # æ­¥éª¤4: ç­‰å¾…Google DriveåŒæ­¥
    # -------------------------------------------------------------------
    print("\nã€æ­¥éª¤4/5ã€‘ç­‰å¾…Google DriveåŒæ­¥...")
    sync_time = 30
    for i in range(sync_time):
        time.sleep(1)
        if (i + 1) % 10 == 0:
            print(f"   â±ï¸  å·²ç­‰å¾… {i+1}/{sync_time} ç§’...")
    print("   âœ… åŒæ­¥ç­‰å¾…å®Œæˆ")
    
    # -------------------------------------------------------------------
    # æ­¥éª¤5: éªŒè¯Driveä¸­çš„æ–‡ä»¶
    # -------------------------------------------------------------------
    print("\nã€æ­¥éª¤5/5ã€‘éªŒè¯Google Driveä¸­çš„æ–‡ä»¶...")
    
    drive_ok = True
    for fname in required_files.keys():
        fpath = os.path.join(DRIVE_PATH, fname)
        
        # å¤„ç†safetensors/binä¸¤ç§æ ¼å¼
        if fname == "adapter_model.safetensors":
            if not os.path.exists(fpath):
                fpath = os.path.join(DRIVE_PATH, "adapter_model.bin")
                fname = "adapter_model.bin"
        
        if os.path.exists(fpath):
            size_mb = os.path.getsize(fpath) / (1024**2)
            print(f"   âœ… {fname}: {size_mb:.2f} MB")
        else:
            print(f"   âŒ {fname} åœ¨Driveä¸­ç¼ºå¤±!")
            drive_ok = False
    
    if not drive_ok:
        print("\nâš ï¸  è­¦å‘Š: Driveä¸­æŸäº›æ–‡ä»¶ç¼ºå¤±ï¼Œå¯èƒ½éœ€è¦é‡æ–°ä¿å­˜")
    
    # -------------------------------------------------------------------
    # å®Œæˆ
    # -------------------------------------------------------------------
    print("\n" + "="*70)
    print("ğŸ‰ æ¨¡å‹ä¿å­˜å®Œæˆï¼")
    print("="*70)
    print(f"ğŸ“‚ æœ¬åœ°å‰¯æœ¬: {LOCAL_TEMP}")
    print(f"â˜ï¸  Driveè·¯å¾„: {DRIVE_PATH}")
    print(f"\nğŸ’¡ å»ºè®®:")
    print(f"   1. ä»Google Driveç½‘é¡µç•Œé¢ç¡®è®¤æ–‡ä»¶å·²ä¸Šä¼ ")
    print(f"   2. å¯ä»¥å®‰å…¨åœ°æ–­å¼€Colabä¼šè¯")
    print(f"   3. ä½¿ç”¨ä¸‹é¢çš„ load_model_safely() å‡½æ•°åŠ è½½æ¨¡å‹")
    print("="*70)
    
    return LOCAL_TEMP, DRIVE_PATH


# =====================================================================
# Section 2: ä»Google DriveåŠ è½½æ¨¡å‹ï¼ˆé˜²å´©æºƒç‰ˆï¼‰
# =====================================================================

def load_model_safely(
    lora_model_name="codellama-gis-lora",
    base_model_name="codellama/CodeLlama-7b-Instruct-hf",
    use_local_cache=True
):
    """
    å®‰å…¨åœ°ä»Google DriveåŠ è½½æ¨¡å‹
    
    ç­–ç•¥ï¼šDrive â†’ æœ¬åœ°ç¼“å­˜ â†’ æ¸…ç†å†…å­˜ â†’ åˆ†æ­¥åŠ è½½
    
    Args:
        lora_model_name: LoRAæ¨¡å‹åç§°ï¼ˆåœ¨Driveä¸­çš„æ–‡ä»¶å¤¹åï¼‰
        base_model_name: åŸºç¡€æ¨¡å‹åç§°æˆ–è·¯å¾„
        use_local_cache: æ˜¯å¦å…ˆå¤åˆ¶åˆ°æœ¬åœ°ç¼“å­˜ï¼ˆå¼ºçƒˆæ¨èï¼‰
    
    Returns:
        model, tokenizer
    """
    import gc
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from peft import PeftModel
    import shutil
    import time
    from pathlib import Path
    
    DRIVE_PATH = f"/content/drive/MyDrive/gis-models/{lora_model_name}"
    LOCAL_CACHE = f"/content/model_cache/{lora_model_name}"
    
    print("="*70)
    print("ğŸ”§ ä»Google DriveåŠ è½½æ¨¡å‹ï¼ˆé˜²å´©æºƒç‰ˆï¼‰")
    print("="*70)
    print(f"ğŸ“¦ åŸºç¡€æ¨¡å‹: {base_model_name}")
    print(f"ğŸ”— LoRAæƒé‡: {lora_model_name}")
    print("="*70)
    
    # -------------------------------------------------------------------
    # é¢„å¤„ç†: æ¸…ç†å†…å­˜
    # -------------------------------------------------------------------
    print("\nã€é¢„å¤„ç†ã€‘æ¸…ç†å†…å­˜...")
    for _ in range(3):
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        total = torch.cuda.get_device_properties(0).total_memory / 1024**3
        print(f"   GPUå†…å­˜: {allocated:.2f}GB / {total:.2f}GB")
        print(f"   å¯ç”¨: {total - allocated:.2f}GB")
        
        if total - allocated < 5:
            print("   âš ï¸  è­¦å‘Š: å¯ç”¨æ˜¾å­˜ä¸è¶³5GBï¼ŒåŠ è½½å¯èƒ½å¤±è´¥")
    
    # -------------------------------------------------------------------
    # æ­¥éª¤1: å‡†å¤‡LoRAæƒé‡è·¯å¾„
    # -------------------------------------------------------------------
    if use_local_cache:
        print(f"\nã€æ­¥éª¤1/4ã€‘å¤åˆ¶åˆ°æœ¬åœ°ç¼“å­˜...")
        print(f"   è¿™å°†é¿å…Google Driveçš„I/Oç“¶é¢ˆ")
        
        if not Path(DRIVE_PATH).exists():
            raise FileNotFoundError(f"âŒ Driveè·¯å¾„ä¸å­˜åœ¨: {DRIVE_PATH}")
        
        if Path(LOCAL_CACHE).exists():
            print(f"   âœ… æœ¬åœ°ç¼“å­˜å·²å­˜åœ¨: {LOCAL_CACHE}")
        else:
            print(f"   ğŸ“‹ æ­£åœ¨å¤åˆ¶...")
            shutil.copytree(DRIVE_PATH, LOCAL_CACHE)
            time.sleep(3)  # ç»™æ–‡ä»¶ç³»ç»Ÿä¸€äº›æ—¶é—´
            print(f"   âœ… å·²å¤åˆ¶åˆ°: {LOCAL_CACHE}")
        
        lora_path = LOCAL_CACHE
    else:
        print(f"\nã€æ­¥éª¤1/4ã€‘ä½¿ç”¨Driveè·¯å¾„ï¼ˆå¯èƒ½è¾ƒæ…¢ï¼‰...")
        lora_path = DRIVE_PATH
    
    # -------------------------------------------------------------------
    # æ­¥éª¤2: åŠ è½½Tokenizer
    # -------------------------------------------------------------------
    print(f"\nã€æ­¥éª¤2/4ã€‘åŠ è½½Tokenizer...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            lora_path,
            padding_side="right",
            local_files_only=True,
            trust_remote_code=True,
        )
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"   âœ… TokenizeråŠ è½½å®Œæˆ")
        print(f"   è¯è¡¨å¤§å°: {len(tokenizer)}")
    except Exception as e:
        print(f"   âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
        print(f"   ğŸ’¡ å°è¯•ä»åŸºç¡€æ¨¡å‹åŠ è½½...")
        
        tokenizer = AutoTokenizer.from_pretrained(
            base_model_name,
            padding_side="right",
            trust_remote_code=True,
        )
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"   âœ… ä»åŸºç¡€æ¨¡å‹åŠ è½½æˆåŠŸ")
    
    # æ¸…ç†
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # -------------------------------------------------------------------
    # æ­¥éª¤3: åŠ è½½åŸºç¡€æ¨¡å‹
    # -------------------------------------------------------------------
    print(f"\nã€æ­¥éª¤3/4ã€‘åŠ è½½åŸºç¡€æ¨¡å‹...")
    print(f"   æ¨¡å‹: {base_model_name}")
    
    try:
        # å°è¯•ä½¿ç”¨int8é‡åŒ–ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
        from transformers import BitsAndBytesConfig
        
        bnb_config = BitsAndBytesConfig(
            load_in_8bit=True,
            llm_int8_threshold=6.0,
        )
        
        print(f"   ğŸ”„ ä½¿ç”¨int8é‡åŒ–åŠ è½½...")
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            quantization_config=bnb_config,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
            max_memory={0: "12GB", "cpu": "30GB"},
        )
        print(f"   âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (int8)")
    
    except Exception as e:
        print(f"   âš ï¸  int8é‡åŒ–å¤±è´¥: {str(e)[:80]}...")
        print(f"   ğŸ“Œ ä½¿ç”¨æ ‡å‡†ç²¾åº¦åŠ è½½...")
        
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            low_cpu_mem_usage=True,
            trust_remote_code=True,
        )
        print(f"   âœ… åŸºç¡€æ¨¡å‹åŠ è½½å®Œæˆ (float16)")
    
    base_model.config.use_cache = False
    
    # æ¸…ç†
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # -------------------------------------------------------------------
    # æ­¥éª¤4: åŠ è½½LoRAæƒé‡
    # -------------------------------------------------------------------
    print(f"\nã€æ­¥éª¤4/4ã€‘åŠ è½½LoRAæƒé‡...")
    print(f"   è·¯å¾„: {lora_path}")
    
    try:
        model = PeftModel.from_pretrained(
            base_model,
            lora_path,
            is_trainable=False,
            local_files_only=True,
        )
        
        # ç§»åˆ°GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if torch.cuda.is_available():
            model = model.to("cuda")
        
        model.eval()
        print(f"   âœ… LoRAæƒé‡åŠ è½½å®Œæˆ")
    
    except Exception as e:
        print(f"   âŒ LoRAæƒé‡åŠ è½½å¤±è´¥: {e}")
        print(f"\nğŸ’¡ æ£€æŸ¥æ¸…å•:")
        print(f"   1. adapter_config.json å­˜åœ¨? {Path(lora_path) / 'adapter_config.json' exists()}")
        print(f"   2. adapter_model.safetensors å­˜åœ¨? {(Path(lora_path) / 'adapter_model.safetensors').exists()}")
        print(f"   3. è·¯å¾„æ­£ç¡®? {lora_path}")
        raise
    
    # æœ€ç»ˆæ¸…ç†
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    # -------------------------------------------------------------------
    # å®Œæˆ
    # -------------------------------------------------------------------
    print("\n" + "="*70)
    print("ğŸ‰ æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("="*70)
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°: {total_params / 1e6:.1f}M")
    
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        print(f"ğŸ’¾ GPUå†…å­˜å ç”¨: {allocated:.2f}GB")
    
    print("="*70)
    print("âœ… å¯ä»¥å¼€å§‹æ¨ç†äº†ï¼")
    
    return model, tokenizer


# =====================================================================
# ä½¿ç”¨ç¤ºä¾‹
# =====================================================================

if __name__ == "__main__":
    # ---------------------------------------------------------------
    # ç¤ºä¾‹1: è®­ç»ƒåä¿å­˜ï¼ˆåœ¨è®­ç»ƒnotebookä¸­ä½¿ç”¨ï¼‰
    # ---------------------------------------------------------------
    """
    # å‡è®¾ä½ å·²ç»è®­ç»ƒå®Œæˆï¼Œæœ‰ trainer å’Œ tokenizer
    local_path, drive_path = save_model_safely(
        trainer=trainer,
        tokenizer=tokenizer,
        output_name="codellama-gis-lora"
    )
    """
    
    # ---------------------------------------------------------------
    # ç¤ºä¾‹2: æ–°sessionä¸­åŠ è½½ï¼ˆåœ¨æ¨ç†notebookä¸­ä½¿ç”¨ï¼‰
    # ---------------------------------------------------------------
    """
    # æ–¹æ³•A: ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼ˆæ¨èï¼‰
    model, tokenizer = load_model_safely(
        lora_model_name="codellama-gis-lora",
        base_model_name="codellama/CodeLlama-7b-Instruct-hf",
        use_local_cache=True  # æ¨èï¼
    )
    
    # æ–¹æ³•B: ç›´æ¥ä»DriveåŠ è½½ï¼ˆä¸æ¨èï¼Œå¯èƒ½è¾ƒæ…¢ï¼‰
    model, tokenizer = load_model_safely(
        lora_model_name="codellama-gis-lora",
        base_model_name="codellama/CodeLlama-7b-Instruct-hf",
        use_local_cache=False
    )
    
    # æµ‹è¯•æ¨ç†
    test_prompt = "Create a new cable object"
    inputs = tokenizer(test_prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=256)
    
    result = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print(f"\\nç”Ÿæˆç»“æœ:\\n{result}")
    """
    
    pass
